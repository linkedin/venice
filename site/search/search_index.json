{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Venice","text":"<p>Derived Data Platform for Planet-Scale Workloads</p> <p> </p> <p>Venice is a derived data storage platform providing:</p> <ol> <li>High throughput asynchronous ingestion from batch and streaming sources</li> <li>Low latency online reads via remote queries or in-process caching</li> <li>Active-active replication between regions with CRDT-based conflict resolution</li> <li>Multi-cluster support within each region with operator-driven cluster assignment</li> <li>Multi-tenancy, horizontal scalability and elasticity within each cluster</li> </ol> <p>ML Feature Store: Venice is particularly suitable as the stateful component backing a Feature Store, such as Feathr. AI applications feed the output of their ML training jobs into Venice and then query the data for use during online inference workloads.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>Venice bridges offline, nearline and online worlds:</p> <p></p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#gradle","title":"Gradle","text":"<pre><code>repositories {\n  mavenCentral()\n  maven {\n    name 'VeniceJFrog'\n    url 'https://linkedin.jfrog.io/artifactory/venice'\n  }\n}\n\ndependencies {\n  implementation 'com.linkedin.venice:venice-client:0.4.455'\n}\n</code></pre>"},{"location":"#maven","title":"Maven","text":"<pre><code>&lt;repositories&gt;\n  &lt;repository&gt;\n    &lt;id&gt;venice-jfrog&lt;/id&gt;\n    &lt;url&gt;https://linkedin.jfrog.io/artifactory/venice&lt;/url&gt;\n  &lt;/repository&gt;\n&lt;/repositories&gt;\n\n&lt;dependencies&gt;\n  &lt;dependency&gt;\n    &lt;groupId&gt;com.linkedin.venice&lt;/groupId&gt;\n    &lt;artifactId&gt;venice-client&lt;/artifactId&gt;\n    &lt;version&gt;0.4.455&lt;/version&gt;\n  &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"#apis","title":"APIs","text":""},{"location":"#write-apis","title":"Write APIs","text":"Write Type Batch (Hadoop) Stream (Samza) Online (HTTP) Full dataset swap \u2705 \u2705 Row insertion \u2705 \u2705 \u2705 Column updates \u2705 \u2705 \u2705 <p>Write API docs \u2192</p>"},{"location":"#read-apis","title":"Read APIs","text":"Client Type Network Hops Latency (p99) State Thin Client 2 &lt;10ms Stateless Fast Client 1 &lt;2ms Minimal Da Vinci (SSD) 0 &lt;1ms Full dataset on SSD Da Vinci (RAM) 0 &lt;10\u03bcs Full dataset in RAM <p>Read API docs \u2192</p>"},{"location":"#resources","title":"Resources","text":"<p>Getting Started - Quickstart Guide - User Guide - Operations Guide</p> <p>Community - Slack - GitHub - Blog</p> <p>Learn More - Videos &amp; Talks - Contributing Guide - API Reference</p>"},{"location":"LOCAL_DEPLOYMENT/","title":"Local Documentation Deployment","text":""},{"location":"LOCAL_DEPLOYMENT/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8+</li> <li>pip</li> <li>Git</li> </ul>"},{"location":"LOCAL_DEPLOYMENT/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Install dependencies: <pre><code>pip install -r requirements-docs.txt\n</code></pre></p> </li> <li> <p>Serve locally: <pre><code>mkdocs serve\n</code></pre></p> </li> </ol> <p>Open http://127.0.0.1:8000</p> <ol> <li>Build static site: <pre><code>mkdocs build\n</code></pre></li> </ol> <p>Output in <code>site/</code> directory.</p>"},{"location":"LOCAL_DEPLOYMENT/#options","title":"Options","text":"<p>Custom port: <pre><code>mkdocs serve --dev-addr=127.0.0.1:8080\n</code></pre></p> <p>With Javadoc: <pre><code>./gradlew aggregateJavadoc\nmkdir -p docs/javadoc\ncp -r build/javadoc/* docs/javadoc/\nmkdocs serve\n</code></pre></p> <p>Strict mode: <pre><code>mkdocs serve --strict\n</code></pre></p>"},{"location":"LOCAL_DEPLOYMENT/#troubleshooting","title":"Troubleshooting","text":"<p>Module errors: <pre><code>pip install --upgrade -r requirements-docs.txt\n</code></pre></p> <p>Port in use: <pre><code>mkdocs serve --dev-addr=127.0.0.1:8001\n</code></pre></p>"},{"location":"contributing/","title":"Contributing to Venice","text":"<p>Venice is an open source project. We welcome contributions!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ul> <li>Code of Conduct</li> <li>Contributing Guide</li> <li>Security Policy</li> </ul>"},{"location":"contributing/#development","title":"Development","text":"<ul> <li>Workspace Setup</li> <li>Development Workflow</li> <li>Testing Guide</li> <li>Style Guide</li> </ul>"},{"location":"contributing/#architecture","title":"Architecture","text":"<ul> <li>Project Navigation</li> <li>Write Path</li> <li>Java Internals</li> <li>Router API</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Writing Docs</li> <li>Design Docs</li> </ul>"},{"location":"contributing/#proposals","title":"Proposals","text":"<ul> <li>Venice Improvement Proposals (VIPs)</li> </ul>"},{"location":"contributing/code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"contributing/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"contributing/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"contributing/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"contributing/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement over Slack. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"contributing/code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"contributing/code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"contributing/code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"contributing/code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"contributing/code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"contributing/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/contributing/","title":"Contribution Agreement","text":"<p>As a contributor, you represent that the code you submit is your original work or that of your employer (in which case you represent you have the right to bind your employer).  By submitting code, you (and, if applicable, your employer) are licensing the submitted code to LinkedIn and the open source community subject to the BSD 2-Clause license.</p>"},{"location":"contributing/contributing/#responsible-disclosure-of-security-vulnerabilities","title":"Responsible Disclosure of Security Vulnerabilities","text":"<p>Please refer to our Security Policy for our policy on responsibly reporting security vulnerabilities.</p>"},{"location":"contributing/contributing/#contribution-process","title":"Contribution Process","text":"<p>The Venice community welcome everyone, and encourage a friendly and positive environment.</p> <p>Contributions of various forms are welcome!</p> <p>Please read existing GitHub issues or development work that is in progress or in the backlog to avoid duplication. If you are interested in those existing ones, you can leave a comment in the GitHub issues and the community will try to involve you. If you are not sure if it's duplicated, just create a GitHub issue and ask!</p> <p>For any PR, a GitHub issue is required.</p> <p>If you want to contribute something new, and it's not tracked in existing GitHub issues, please create a new  GitHub issue and the community will help review the idea. Please state <code>why</code> in your GitHub issue.  If you already have a short design in mind or change is not small please check the  Venice Improvement Proposal </p> <p>If you have any feature request, please create a new GitHub issue and the community will collect your feature request and work on it.</p> <p>If you have any user feedback, please create a new GitHub issue and the community will collect your feedback and work on it.</p>"},{"location":"contributing/contributing/#for-new-contributors","title":"For New Contributors","text":"<p>Please follow Venice Workspace Setup and Venice Recommended Development Workflow</p>"},{"location":"contributing/contributing/#tips-for-getting-your-pull-request-accepted","title":"Tips for Getting Your Pull Request Accepted","text":"<ol> <li>Make sure all new features are tested and the tests pass.</li> <li>Bug fixes must include a test case demonstrating the error that it    fixes.</li> <li>Open an issue first and seek advice for your change before    submitting a pull request. Large features which have never been    discussed are unlikely to be accepted.</li> </ol>"},{"location":"contributing/contributing/#contributor-sync-meeting","title":"Contributor Sync Meeting","text":"<p>Venice does Contributor sync meeting for the contributors to come togather in a virtual meeting.</p> <ol> <li>Meeting Frequency : Bi-weekly on Wednesday at 8 AM Pacific, 11 AM Eastern, 5 PM Central Europe</li> <li>Meeting Co-ordination : Please join the #contributor-sync channel in the       public Venice Slack </li> <li>Meeting Length : One hour </li> <li>Agenda : </li> <li>Discuss Venice Improvement Proposals.</li> <li>Review and address concerns for any open pull requests.</li> <li>Open discussion.</li> </ol>"},{"location":"contributing/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>This project and everyone who participates in it is governed by the Venice Code of Conduct. By participating, you are expected to uphold this code. Please report unacceptable behavior on Slack.</p>"},{"location":"contributing/security/","title":"Responsible Disclosure of Security Vulnerabilities","text":"<p>Please do not file reports on GitHub for security issues.  Please review the guidelines for reporting Security Vulnerabilities.</p> <p>Alternatively, reports can be encrypted using PGP (public key) and sent to security@linkedin.com, preferably with the title \"GitHub linkedin/venice - &lt;short summary&gt;\".</p>"},{"location":"contributing/architecture/java-internals/","title":"Java","text":"<p>{: .no_toc }</p> <p>The Venice codebase is completely in Java, and therefore it is important to have at least a basic understanding of the  language and its runtime, the Java Virtual Machine (JVM). This page is not intended to be an in-depth resource on Java,  but might be a useful starting point for Java beginners. Links to more in-depth resources will be provided along the  way, and in cases where there are many distinct ways of using Java, guidance will be provided in terms of the Venice  project's expectations.</p>"},{"location":"contributing/architecture/java-internals/#table-of-contents","title":"Table of Contents","text":"<p>{: .no_toc }</p> <ul> <li>TOC</li> </ul>"},{"location":"contributing/architecture/java-internals/#java-in-a-nutshell","title":"Java in a Nutshell","text":"<p>Java as a language is statically typed, dynamically compiled, object-oriented, and provides managed memory. Let's  briefly go over each of these buzzwords.</p>"},{"location":"contributing/architecture/java-internals/#statically-typed","title":"Statically Typed","text":"<p>Static typing, also known as strong typing, is the notion that a variable is declared to have a given type, and that the language provides guarantees that it is only possible to assign data to this variable which conforms to the declared  type. This guarantee is ensured at compilation-time, which is good in the sense that compilation happens early in the development lifecycle, therefore preventing trivial mistakes from creeping into production systems and being discovered there, at run-time, when it is already too late. Static typing therefore provides a maintenance benefit</p> <p>In addition, static typing also provides a performance benefit, because the compiled code can make certain assumptions which a dynamically typed language could not. This enables certain low-level optimizations to take place.</p>"},{"location":"contributing/architecture/java-internals/#dynamically-compiled","title":"Dynamically Compiled","text":"<p>Although Java programs need to be compiled, they are not compiled directly to machine code, but rather to an  \"intermediate representation\" called bytecode. The bytecode can then be loaded into a Java Virtual Machine (JVM), which interprets it, and dynamically compiles architecture-specific code to run it. This means that Java code is portable (\"compile once, run everywhere\"), though in practice, in the case of Venice, the code has been exercised mainly on Linux (for production usage) and macOS (as a development environment). In theory, it should work on Windows or other platforms though it has not been tested there.</p> <p>An additional benefit of dynamic compilation is, again, performance. The JVM features a JIT (Just-in-time) Compiler  which uses heuristics to analyze which code paths are most frequently called. These code paths end up being recompiled into even more optimal versions.</p>"},{"location":"contributing/architecture/java-internals/#object-oriented","title":"Object-Oriented","text":"<p>Java at its core is an object-oriented language, although over the years it has also added certain other paradigms such as functional programming. What it means concretely is that design patterns such as encapsulation, inheritance and  polymorphism are well-supported.</p>"},{"location":"contributing/architecture/java-internals/#managed-memory","title":"Managed Memory","text":"<p>An important aspect of Java is that memory management is abstracted away from the programmer. In particular, there is no need to explicitly free allocated memory, and there is no risk of accidentally accessing a memory address after it has been freed. This design improves both reliability and security, however, it comes at the cost of needing garbage  collection, which in some cases is a performance drag. These various aspects are explored more in-depth in later  sections.</p>"},{"location":"contributing/architecture/java-internals/#other-languages","title":"Other Languages","text":"<p>Before diving deeper into Java itself, it may be good to point out the role of some other languages. Although the Venice codebase is in Java, some of Venice's dependencies are not in Java.</p> <p>Furthermore, there may be other languages introduced to the Venice codebase in the future, for example Python tooling or Rust server code.</p>"},{"location":"contributing/architecture/java-internals/#c","title":"C++","text":"<p>There are two major dependencies which are written in C++, and which are accessed via the Java Native Interface (JNI):</p> <ul> <li>The RocksDB storage engine.</li> <li>The ZSTD compression algorithm.</li> </ul>"},{"location":"contributing/architecture/java-internals/#scala","title":"Scala","text":"<p>Some other Venice dependencies are written in Scala, a language which gets compiled into bytecode and runs within the JVM, alongside the bytecode compiled from Java.</p> <p>Although inter-operability between Java and Scala is supported, there are nonetheless certain challenges associated with the use of Scala due to the incompatibility across minor versions and the need to cross-compile Scala dependencies across a range of minor versions.</p> <p>For this reason, the Venice project intends to not make use of any Scala within its codebase, and to limit the use of Scala dependencies to the most minimal number of modules possible. In particular:</p> <ul> <li>The Kafka broker is written in Scala, but not the Kafka client library. The broker is only   used within integration tests, and the main code should not depend on it.</li> <li>The Spark framework is a runtime dependency of the Venice Push Job,   and therefore VPJ is the only production module which carries a (transitive) dependency to Scala.</li> </ul>"},{"location":"contributing/architecture/java-internals/#groovy","title":"Groovy","text":"<p>The build system, Gradle, functions with the Groovy language, and so the build files of each module are written in that language. Like Scala, Groovy is also a language that runs within the JVM, and is interoperable with Java. We do not intend to use Groovy outside of build files.</p>"},{"location":"contributing/architecture/java-internals/#javascript","title":"Javascript","text":"<p>Finally, there is even a tiny bit of Node.js running in some of the GitHub Actions plugins, used to generate reports! It takes a village to raise a distributed database, and Venice is no exception! Like Groovy, this is a build dependency and there is no intent to use JS anywhere else in the codebase.</p>"},{"location":"contributing/architecture/java-internals/#java-versions","title":"Java Versions","text":"<p>Currently, the Venice project intends to support Java versions 8, 11 and 17. This means that the codebase cannot use language features that have been introduced in versions later than 8. We hope to lift this restriction in the future and drop support for older Java versions. For the time being, the build ensures that compilation will fail if recent language features are used.</p> <p>The main motivation for maintaining compatibility with older Java versions is to support legacy applications which  depend on the Venice client libraries. The Venice services (controller, router and server) also work on older Java  versions though in practice we run them on Java 17 JVMs, which provide better performance. As much as possible, it is recommended to run all Venice code on the latest supported JVM version.</p>"},{"location":"contributing/architecture/java-internals/#memory","title":"Memory","text":"<p>Memory is a big subject for any programming language, and Java is no exception. It is important to understand the basic characteristics of how memory is allocated, garbage collected, laid out, measured and accessed.</p>"},{"location":"contributing/architecture/java-internals/#memory-allocation","title":"Memory Allocation","text":"<p>Memory can be allocated either on the Java stack, the Java heap or in native memory.</p> <p>Allocations to the stack include all local variables of primitive types (boolean, int, float, etc.) as well as pointers to objects. Local variables are those  defined within the scope of a function and are therefore inaccessible outside that scope.</p> <p>Allocations on the heap include all objects, as well as the fields and pointers they contain.</p> <p>Allocations to native memory are special in the sense that they require extra care (see Garbage Collection, below).  Within the Venice codebase, there is no explicit allocation of native memory, but some of Venice's dependencies do use native memory, for example Netty and RocksDB.</p>"},{"location":"contributing/architecture/java-internals/#garbage-collection","title":"Garbage Collection","text":"<p>Garbage Collection (usually referred to as GC) is an important aspect of Java. The JVM provides \"managed memory\",  meaning that memory is automatically allocated and deallocated as necessary, with minimal effort from the developer. Although this is ergonomically useful, it is still important to understand the basic principles of GC as it can have performance implications, and Venice is a performance-sensitive project. Let's take the stack, heap and native  allocations introduced previously and see how each of them behaves in terms of GC.</p> <p>Memory allocated on the stack is automatically deallocated as soon as it goes out of scope (e.g. when the function, or if block or while loop in which it was allocated finishes). This makes stack allocations very efficient since there is not \"leftover\" work to take care of (see below).</p> <p>Memory allocated on the heap, on the other hand, can (at least potentially) have a lifespan greater than the scope within which it was allocated. For example, if an allocated object is assigned to  the field of another object, then the lifecycle of the former object becomes tied to that of the latter one. These  objects on the heap eventually need to be cleaned up, which is the responsibility of the Garbage Collector and is  accomplished asynchronously via background threads, reference counting and other mechanisms. The details are outside the  scope of this wiki page, but suffice to say that GC has a cost, and so we do care about minimizing garbage generation in  performance-sensitive hot paths.</p> <p>Finally, native memory allocation requires manual memory management, as the JVM will not take care of automatically deallocating it as part of GC. This is the reason why it is important, for example, to close objects returned by RocksDB when they are no longer in use, and to invoke the ref counting APIs of Netty  when appropriate.</p>"},{"location":"contributing/architecture/java-internals/#memory-leaks","title":"Memory Leaks","text":"<p>Memory allocated on the heap or natively can leak, which causes severe operational issues.</p> <p>In the case of heap allocations, a leak can occur if references to unneeded objects are maintained. This results in GC not being capable of eliminating this garbage since the ref counting concludes that the referenced objects are  ineligible for collection. After the heap fills up, further allocations will result in OutOfMemoryError  getting thrown. In order to debug this type of issue, it is useful to configure the JVM with the  <code>-XX:HeapDumpOnOutOfMemoryError</code> config flag, and conduct heap dump analysis. See these mem leak troubleshooting tips to dig deeper.</p> <p>In the case of native allocations, a leak can occur if the Java code fails to explicitly release the objects. This can ultimately cause the process to get terminated by the OS' OOM Killer.</p>"},{"location":"contributing/architecture/java-internals/#memory-layout","title":"Memory Layout","text":"<p>It can be useful to understand how objects are laid out in memory, in order to get a sense of the overhead of the data structures we design. In general, the main design criteria is simply to adhere to object-oriented principles in order to achieve encapsulation and other characteristics of interest. But in cases where some pieces of data need to be allocated on the hot path (i.e., at high throughput), or be retained for long periods of time, it can be useful to pay attention to what the heap size of the data is going to be, with an eye towards minimizing its footprint. This section is a just a  high-level overview, and more details can be found in Java Objects Inside Out.</p>"},{"location":"contributing/architecture/java-internals/#memory-impacting-configurations","title":"Memory-Impacting Configurations","text":"<p>Note that memory layout details are dependent on which version of the JVM is used, as well as various JVM settings. In order to facilitate explanations throughout this whole section, we will provide concrete memory size numbers based on a  specific (though, arguably, fairly common) set of JVM configurations:</p> <ul> <li>64 bits JVM</li> <li>Java 17</li> <li>Max heap size of less than 32 GB</li> <li>All other memory-related settings on their default values, including:</li> <li>Compressed pointers (enabled)</li> <li>Compressed class pointers (enabled)</li> <li>Alignment size (8 bytes)</li> </ul>"},{"location":"contributing/architecture/java-internals/#object-headers","title":"Object Headers","text":"<p>All objects in Java have a header, which is made up of a mark word and a class pointer. The mark word contains various JVM internal details such as GC-related bookkeeping, the identity hashcode, and locking information, while the class  pointer is a reference to the memory address of the class definition inside the metaspace.</p> <p>Assuming the above JVM configuration details, the object layout will carry 12 bytes worth of header, and therefore look  like this:</p> <pre><code>|           mark word           | class pointer |     fields    |\n|&lt;---------- 8 bytes ----------&gt;|&lt;-- 4 bytes --&gt;|&lt;-- N bytes --&gt;|\n</code></pre> <p>In the case of arrays, there is one more header field, which is an <code>int</code> storing the length, and that is followed by  each element stored in the array (either primitives stored inline, or pointers to Objects stored elsewhere on the heap). The array, therefore, has a 16 bytes header, which looks like this:</p> <pre><code>|           mark word           | class pointer |     length    |    elements   |\n|&lt;---------- 8 bytes ----------&gt;|&lt;-- 4 bytes --&gt;|&lt;-- 4 bytes --&gt;|&lt;-- N bytes --&gt;|\n</code></pre>"},{"location":"contributing/architecture/java-internals/#alignment","title":"Alignment","text":"<p>The previous subsection taught us that (under the example JVM settings above) an Object's header takes up 12 bytes,  however, even if this Object carried no fields at all, its actual size on the heap would still be larger than 12 bytes!  That is because objects are \"aligned\" in memory, and the alignment size (under default JVM settings) is 8 bytes.  Therefore, even the basic Java <code>Object</code> with no fields in it, having just a 12 bytes header, would actually take up 16  bytes on the heap:</p> <pre><code>|           mark word           | class pointer |     wasted    |\n|&lt;---------- 8 bytes ----------&gt;|&lt;-- 4 bytes --&gt;|&lt;-- 4 bytes --&gt;|\n</code></pre> <p>From this, we can derive a rule of thumb that every time we use an Object, we are spending at least 16 bytes, and if the object carries a non-trivial number of fields, then likely more than that.</p> <p>More nuances on alignment and its downstream effects can be learned in JVM Anatomy Quark #24: Object Alignment.</p>"},{"location":"contributing/architecture/java-internals/#size-of-primitives","title":"Size of Primitives","text":"<p>Below is the size in memory of all primitive types:</p> Primitive type Size in bytes <code>boolean</code> 1 <code>byte</code> 1 <code>char</code> 2 <code>short</code> 2 <code>float</code> 4 <code>int</code> 4 <code>double</code> 8 <code>long</code> 8 <p>Note that all of the above are compact representations, meaning that every single bit carries a significant piece of information, except for <code>boolean</code> which carries only one significant bit out of the 8 bits that make up its byte.</p> <p>Even when part of a <code>boolean[]</code> primitive array, each element will still take 1 byte. This JVM design choice makes  certain operations more efficient for the CPU to perform, and protects against word tearing, at the cost of memory waste. In cases where the opposite tradeoffs are desired, it is possible to use a BitSet,  which is more space-efficient thanks to \"bit packing\", and provides support for bit-related operations such as <code>AND</code>,  <code>OR</code>, <code>XOR</code>, intersection, etc.</p>"},{"location":"contributing/architecture/java-internals/#size-comparison-between-primitives-and-objects","title":"Size Comparison Between Primitives and Objects","text":"<p>For each of the primitive types, there exists an equivalent Object type. Using the Object carries the header overhead, while the primitive type does not. Continuing with the example JVM settings above, we can see that any primitive equal or smaller to an <code>int</code> can fit \"within the shadow\" of the object alignment, and therefore takes up 16 bytes in Object  form. Whereas the <code>double</code> and <code>long</code> primitives, if packed inside of a <code>Double</code> or <code>Long</code> object, will need 20 bytes, and therefore be rounded up to 24 bytes due to alignment. And so we can see that the overhead of using an Object rather than a primitive is:</p> Object type Size in bytes Overhead compared to primitive <code>Boolean</code> 16 16x <code>Byte</code> 16 16x <code>Character</code> 16 8x <code>Short</code> 16 8x <code>Float</code> 16 4x <code>Integer</code> 16 4x <code>Double</code> 24 3x <code>Long</code> 24 3x <p>The above is a \"theoretical worst case\". If using the recommended factory methods (e.g., the various <code>valueOf</code>  overloads), then there is a possibility of leveraging singleton instances coming from a central cache. These cached instances cover both <code>Boolean</code> values, all <code>Byte</code> values, and up to 256 values (those closest to zero) of all other non-floating point types. If the cache is used, then effectively the heap size of these instances is zero (i.e.,  assuming that they are part of the \"base overhead\" of the JVM itself, and that they can be amortized across many  references), and therefore their memory cost is only that of the reference itself, which brings us to the following  point.</p> <p>An object allocated on the heap is not just floating there on its own, it has at least one pointer referencing it  (otherwise it is unreachable and therefore eligible to garbage collection). That pointer might be on the stack, or it  might be a field of some other object. Continuing with the example JVM settings above, we would be operating in the \"compressed pointers\" mode, meaning that they would take up 4 bytes each (if the max heap size is &gt;= 32 GB, then  pointers cannot be compressed and take up 8 bytes each). And so the size of the pointer needs to be added to the heap  size of the object itself, in order to calculate its full memory cost.</p> <p>If we assume the use of recommended factory methods, and we add the pointer size, then we get the following sizes:</p> Object type value Size in bytes Overhead compared to primitive <code>Boolean</code> all 4 4x <code>Byte</code> all 4 4x <code>Character</code> 0..128 4 2x <code>Character</code> others 20 10x <code>Short</code> -128..127 4 2x <code>Short</code> others 20 10x <code>Float</code> all 20 5x <code>Integer</code> -128..127 4 1x <code>Integer</code> others 20 5x <code>Double</code> all 28 3.5x <code>Long</code> -128..127 4 0.5x (interestingly!) <code>Long</code> others 28 3.5x"},{"location":"contributing/architecture/java-internals/#why-ever-use-objects-rather-than-primitives","title":"Why Ever Use Objects Rather than Primitives?","text":"<p>Given the significant overheads described above, it is fair to ask why would we ever want to use the Object version of these types, rather than the primitive version. There are a few reasons:</p> <ol> <li> <p>Objects can be null, while primitives cannot. In some cases, the full range of values provided by a primitive type     are needed for a given use case, and in addition to that, we also need to model the absence of a value. For example,    some use case may require a given field to be not only true, or false, but also absent. This could be modeled by two    primitive boolean fields, e.g., having both <code>boolean configValue</code> and <code>boolean configValueIsSet</code>, though that could    be hard to maintain. Alternatively, we could have a single <code>Boolean configValue</code> field, which has the three possible     states we care for (true, false, null). Note that <code>Optional</code> is one more way of modeling absence, though in practice    it is not terribly useful, and we choose to avoid it in the Venice project.</p> </li> <li> <p>Objects can be used as a type parameter in a class which has Generic Types,    while primitives cannot. For example, a <code>List&lt;Integer&gt;</code> is valid, while a <code>List&lt;int&gt;</code> is not. That being said, in the     case of common collections, there is an alternative, provided by one of the dependencies of the Venice project called     fastutil. In the previous example, <code>fastutil</code> could be used to provide an IntList,     which is an implementation of <code>List&lt;Integer&gt;</code> with additional methods that prevent \"boxing\" (see below).</p> </li> </ol>"},{"location":"contributing/architecture/java-internals/#boxing-and-unboxing","title":"Boxing and Unboxing","text":"<p>\"Boxing\" is the term used to designate Java's support for implicitly converting a primitive type to its Object  equivalent. \"Unboxing\" is the reverse process, where an Object is transformed into a primitive (though one must be  careful to guard against the fact that unboxing a null Object results a <code>NullPointerException</code>). For example, the  following code is valid:</p> <pre><code>class Sample {\n  Integer boxingExample(int primitiveInt) {\n    Integer objectInteger = primitiveInt;\n    return objectInteger;\n  }\n\n  int unboxingExample(Integer objectInteger) {\n    int primitiveInt = objectInteger; // Warning! This can throw a NullPointerException!\n    return primitiveInt; \n  }\n}\n</code></pre> <p>In some cases, the JIT Compiler can optimize away certain object allocations via scalar replacement, and boxing can  sometimes be eliminated in this fashion. It should not be assumed, however, that such optimization can take place (nor  that it will even if it can). More details in JVM Anatomy Quark #18: Scalar Replacement.</p> <p>In general, therefore, it is recommended to avoid implicit boxing in hot paths where it can be reasonably avoided (i.e.,  without undue maintenance burden).</p>"},{"location":"contributing/architecture/java-internals/#estimating-the-heap-size-of-objects","title":"Estimating the Heap Size of Objects","text":"<p>The above offers some \"rule of thumb\" guidance for estimating the heap size of certain kinds of objects, in the hope  that it may help developers make informed decisions about class design. But there are also some scenarios within the Venice codebase where we wish to estimate the size on heap of objects at runtime. For example, in the ingestion code, there is some buffering where we wish to deterministically enforce a cap on the size in memory of buffered objects.</p> <p>There are a few different ways in which we might assess object size in Java, including empirical and theoretical  approaches.</p>"},{"location":"contributing/architecture/java-internals/#empirical-measurement","title":"Empirical Measurement","text":"<p>Empirical approaches aim to measure the \"true size\" of objects. Broadly speaking, we could divide this solution space into two categories, presented below (though each of those could have many variants).</p> <p>One such way, which we might qualify as a \"black box approach\", is to ask the JVM how much memory it is using, then  allocating the object of interest, then asking the JVM again how much it's using, and calculating the delta between  before and after. In order to reduce background noise, the allocation step would need to produce a large number of  instances, and hang on to them, so they don't get garbage collected right away. This type of empirical approach is not  very practical within production code (due to inefficiency and occasional imprecision), but we do use it in unit tests  to validate how well the other methods are working.</p> <p>Another way is that some JVM flags or JVM agents can be configured which provide access to measurement functions. This  bucket of experimental approaches are expected to provide full precision. They are also more efficient than the black  box approach (given that there is no need to synthetically allocate a bunch of objects), though it is still not  completely efficient due to needing to rely on reflection. Moreover, the requirement to use certain JVM configurations makes these approaches not fully reliable. In Venice's case, given  that measurement was needed in some client library (the Da Vinci Client), where the JVM settings can be quite varied  across the user base, this was not considered sufficiently reliable.</p>"},{"location":"contributing/architecture/java-internals/#theoretical-prediction","title":"Theoretical Prediction","text":"<p>Rather than empirically measuring, we can use theoretical knowledge of JVM internals to \"predict\" what the size of a  given object should be. The downside of this approach is that, given that it is not empirical, it is possible the theoretical assumptions it is based on are wrong, therefore leading to imprecision. We can further break down this  bucket into two approaches:</p> <ol> <li> <p>Generic, where any object of any class can be measured. This kind of approach may be most convenient to     developers but has the downside of requiring reflection, which is not the most efficient. Moreover, achieving     precision requires taking into account whether referenced instances are \"shared\" (in which case they should not be     counted) or whether, on the contrary, they \"belong\" to the referencing object (in which case they should be counted).     Keeping track of this phenomenon is costly, both in terms of space and time. For these reasons, the generic     approach was avoided for Venice's measurement use cases.</p> </li> <li> <p>Specific, where only certain classes can be measured. This is the approach chosen in Venice, and it takes the     form of classes implementing the <code>Measurable</code> interface. These classes implement a <code>getHeapSize</code> function which can     leverage knowledge of the Venice implementation details to skip counting any shared instances, and count     everything else efficiently (i.e., without reflection on the hot path). The downside of course is that this     approach requires more maintenance cost. The utilities in the com.linkedin.venice.memory     package make it easier to implement <code>Measurable</code> correctly, though some manual work is still required.</p> </li> </ol>"},{"location":"contributing/architecture/java-internals/#concurrent-access-to-memory","title":"Concurrent Access to Memory","text":"<p>Java provides first-class support for concurrent workloads. The standard library includes many facilities in the java.util.concurrent package for writing multithreaded applications. When writing such code, it is often necessary for different threads to access shared state. In order to do this correctly, it is important to understand the Java Memory Model.</p> <p>Although a full exploration of this is beyond the scope of this page, a few basic principles can be summarized:</p> <ol> <li> <p>Code is not guaranteed to be executed in the order it is written. Due to various optimizations, certain operations     which are written sequentially in the code can be re-ordered, or executed in parallel. Occasionally, this can produce    surprising results.</p> </li> <li> <p>Memory is not a single entity, but rather many layers of cache, and caches can be incoherent. There is RAM, which     contains the \"authoritative\" value of a given piece of state, and then there are multiple levels of CPU cache which     copy some subset of the RAM, in order to make it more efficient for the CPU to access it. Threads running on     different CPUs may have access to different copies of the RAM, and therefore see inconsistent results.</p> </li> <li> <p>In order to achieve correctness when handling state which is read and written by multiple threads, we must give    explicit instructions to Java. For example:</p> </li> <li> <p>The <code>volatile</code> keyword prepended before a field indicates that reads and writes to that field are guaranteed to be       consistent. Although the way this is guaranteed is not officially specified, in practice it has been        experimentally determined that        volatile writes cause all CPU caches to get invalidated, while the volatile reads can still benefit from CPU        caches. Many of the <code>java.util.concurrent</code> classes rely on <code>volatile</code> fields to implement various functionalities        such as Compare and Swap, locking, etc.</p> </li> <li> <p>The <code>final</code> keyword prepended before a field indicates that it is immutable, and therefore safe to cache in the        CPU. Note, however, that there is a subtlety. Only a primitive final field is \"fully immutable\", whereas an Object        final field merely means that the pointer is immutable (i.e., always referencing the same instance on the heap)        while the fields within the referenced instance can themselves be mutable.</p> </li> <li> <p>The <code>synchronized</code> keyword can be used to indicate that only one thread at a time is allowed to enter the        designated critical section. This can make it so that a non-<code>volatile</code> field is accessed in a threadsafe way, but       only if all such accesses are guarded by synchronization.</p> </li> </ol>"},{"location":"contributing/architecture/java-internals/#conclusion","title":"Conclusion","text":"<p>This page is merely a small selection of Java-related topics. Ultimately, learning the language well requires reading  and writing a lot of code.</p> <p>Although most of the above content is general in nature, and only occasionally makes references to Venice-specific  details, a Venice developer should also get familiar with the rest of the project-specific recommendations found in the  Style Guide.</p>"},{"location":"contributing/architecture/navigation/","title":"Navigating the Project","text":"<p>The Venice codebase is split across these directories:</p> <ul> <li><code>clients</code>, which contains the user-facing libraries that most Venice users might be interested in. Those include:</li> <li><code>da-vinci-client</code>, which is the stateful client, providing \"eager caching\" for the Venice datasets. Learn more.</li> <li><code>venice-admin-tool</code>, which is the shell tool intended for Venice operators.</li> <li><code>venice-client</code>, which is a one-stop-shop for all clients which an online application might need, including        thin-client, fast-client, da-vinci-client, consumer.</li> <li><code>venice-producer</code>, which enables an application to perform real-time writes to Venice.</li> <li><code>venice-push-job</code>, which enables an offline job to push batch data to Venice.</li> <li><code>venice-thin-client</code>, which is the most minimal dependency one can get to issue remote reads to the Venice backend,      by delegating as much of the query logic as possible to the Venice router tier. </li> <li><code>integrations</code>, which contains additional libraries that some Venice users might be interested in, to connect Venice   with other third-party systems. The rule of thumb for including a module in this directory is that it should have   minimal Venice-specific logic, and be mostly just glue code to satisfy the contracts expected by the third-party    system. Also, these modules are intended to minimize the dependency burden of the other client libraries. Those    include:</li> <li><code>venice-beam</code>, which implements the Beam Read API, enabling a Beam job to consume the Venice changelog.  </li> <li><code>venice-pulsar</code>, which contains an implementation of a Pulsar Sink,     in order to feed data from Pulsar topics to Venice.</li> <li><code>venice-samza</code>, which contains an implementation of a Samza SystemProducer,     in order to let Samza stream processing jobs emit writes to Venice.</li> <li><code>internal</code>, which contains libraries not intended for public consumption. Those include:</li> <li><code>alpini</code>, which is a Netty-based framework used by the router service. It was forked from some code used by       LinkedIn's proprietary Espresso       document store. At this time, Venice is the only user of this library, so there should be no concern of breaking      compatibility with other dependents.</li> <li><code>venice-client-common</code>, which is a minimal set of APIs and utilities which the thin-client and other modules need       to depend on. This module used to be named <code>venice-schema-common</code>, as one can see if digging into the git history.</li> <li><code>venice-common</code>, which is a larger set of APIs and utilities used by most other modules, except the thin-client.</li> <li><code>services</code>, which contains the deployable components of Venice. Those include:</li> <li><code>venice-controller</code>, which acts as the control plane for Venice. Dataset creation, deletion and configuration,      schema evolution, dataset to cluster assignment, and other such tasks, are all handled by the controller.</li> <li><code>venice-router</code>, which is the stateless tier responsible for routing thin-client queries to the correct server      instance. It can also field certain read-only metadata queries such as cluster discovery and schema retrieval to     take pressure away from the controller.</li> <li><code>venice-server</code>, which is the stateful tier responsible for hosting data, serving requests from both routers and     fast-client library users, executing write operations and performing cross-region replication.</li> <li><code>tests</code>, which contains some modules exclusively used for testing. Note that unit tests do not belong here, and those    are instead located into each of the other modules above.</li> </ul> <p>Besides code, the repository also contains:</p> <ul> <li><code>all-modules</code>, which is merely an implementation detail of the Venice build and release pipeline. No code is expected   to go here.</li> <li><code>docker</code>, which contains our various docker files.</li> <li><code>docs</code>, which contains the wiki you are currently reading.</li> <li><code>gradle</code>, which contains various hooks and plugins used by our build system.</li> <li><code>scripts</code>, which contains a few simple operational scripts that have not yet been folded into the venice-admin-tool.</li> <li><code>specs</code>, which contains formal specifications, in TLA+ and FizzBee, for some aspects of the Venice architecture.</li> </ul> <p>If you have any questions about where some contribution belongs, do not hesitate to reach out on the community slack!</p>"},{"location":"contributing/architecture/router-api/","title":"Router REST Spec","text":"<p>This page describes how the router's REST endpoints work for reading data out of Venice with the thin client. Currently, there exists a reference implementation of the thin client in Java, but if clients in additional languages are to be  built, they could reference this spec to do so.</p> <p>Note that although this part of the architecture is RESTful, Venice in general has a bias for performance, so the implementation may not be fully up to the standards of a dogmatic RESTafarian. For example, because of URL length limitations, batch gets are implemented with a POST rather than a GET, even though they carry no side effects.</p> <p>The following sections describe the endpoints in the order they are invoked by the client during initialization, in order to prime its internal state.</p> <p>As with any client/server interaction, the developer of a new client implementation should consider details such as load balancing, connection pooling, authentication and authorization. Since these tend to be generic concerns, rather than  Venice-specific ones, they are omitted from this document.</p>"},{"location":"contributing/architecture/router-api/#cluster-discovery-endpoint","title":"Cluster Discovery Endpoint","text":"<p>The first endpoint the client invokes is to discover which cluster the store of interest belongs to:</p> <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/discover_cluster/&lt;store_name&gt;\n</code></pre> <p>This will return a JSON response in the following format:</p> <pre><code>{\n  \"cluster\": \"cluster_name\",\n  \"name\": \"store_name\",\n  \"error\": null,\n  \"errorType\": null,\n  \"d2Service\": \"D2_SERVICE_NAME_62513b39957e_af278fc8\",\n  \"serverD2Service\": \"SERVER_D2_SERVICE_NAME_62513b3d13a5_4dc8aa80\",\n  \"zkAddress\": \"localhost:63359\",\n  \"kafkaBootstrapServers\": \"localhost:1234\",\n  \"exceptionType\": null\n}\n</code></pre> <p>For all JSON responses returned by Venice, it is advisable to validate that the <code>error</code> field is <code>null</code>. If it is not <code>null</code>, then it should be a string containing some description of the error. In addition to that, the <code>errorType</code> field is a means to categorize errors programmatically according to the values in the  ErrorType  enum.</p> <p>The client then uses the information in the <code>d2Service</code> field of this response to connect to the correct cluster.</p> <p>A few important details:</p> <ol> <li>The cluster discovery mechanism currently makes certain assumptions that     D2 is used. This assumption could be loosened in the future     in order to integrate with other service discovery mechanisms.</li> <li>This step can be skipped if not leveraging multi-cluster deployments, or if the operator wishes to let clients     hardcode some cluster addresses.</li> <li>If using cluster migration, the mapping between a store and a cluster can change. In that case, there is a way for     the client to find out about it and automatically switch to the new cluster assignment. The switchover process needs     to be documented further.</li> </ol>"},{"location":"contributing/architecture/router-api/#schema-endpoints","title":"Schema Endpoints","text":"<p>After connecting to the correct cluster, the client primes its cache of  Avro schemas, by calling the endpoints below.</p>"},{"location":"contributing/architecture/router-api/#key-schema-endpoint","title":"Key Schema Endpoint","text":"<p>The key schema must be used to serialize keys to be queried. In Venice, the key schema cannot evolve, so the client can  rely on this assumption. The key schema is retrieved via: <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/key_schema/&lt;store_name&gt;\n</code></pre></p> <p>This returns a JSON response in the following format:</p> <p><pre><code>{\n  \"cluster\": \"cluster_name\",\n  \"name\": \"store_name\",\n  \"error\": null,\n  \"errorType\": null,\n  \"id\": 1,\n  \"derivedSchemaId\": -1,\n  \"schemaStr\": \"\\\"string\\\"\",\n  \"exceptionType\": null\n}\n</code></pre> The significant field here is <code>schemaStr</code>, which is the Avro schema in AVSC format, with the double quotes escaped so  that they do not interfere with the surrounding JSON envelope.</p>"},{"location":"contributing/architecture/router-api/#all-value-schemas-endpoint","title":"All Value Schemas Endpoint","text":"<p>In addition to the key schema, a Venice store is also associated with one or many value schemas, which are guaranteed to be fully compatible with one another. A Venice store can contain records encoded with various versions of the schema, so it is important for the client to know all registered schemas, so that it can perform  schema evolution. <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/value_schema/&lt;store_name&gt;\n</code></pre> This returns the following JSON response: <pre><code>{\n  \"cluster\": \"cluster_name\",\n  \"name\": \"store_name\",\n  \"error\": null,\n  \"errorType\": null,\n  \"superSetSchemaId\": 2,\n  \"schemas\": [\n    {\n      \"id\": 1,\n      \"derivedSchemaId\": -1,\n      \"rmdValueSchemaId\": -1,\n      \"schemaStr\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"ValueRecord\\\",\\\"namespace\\\":\\\"com.foo\\\",\\\"fields\\\":[{\\\"name\\\":\\\"intField\\\",\\\"type\\\":\\\"int\\\",\\\"default\\\":0}]}\"\n    }, {\n      \"id\": 2,\n      \"derivedSchemaId\": -1,\n      \"rmdValueSchemaId\": -1,\n      \"schemaStr\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"ValueRecord\\\",\\\"namespace\\\":\\\"com.foo\\\",\\\"fields\\\":[{\\\"name\\\":\\\"intField\\\",\\\"type\\\":\\\"int\\\",\\\"default\\\":0},{\\\"name\\\":\\\"stringField\\\",\\\"type\\\":\\\"string\\\",\\\"default\\\":\\\"\\\"}]}\"\n    }\n  ],\n  \"exceptionType\": null\n}\n</code></pre></p>"},{"location":"contributing/architecture/router-api/#individual-value-schema-endpoint","title":"Individual Value Schema Endpoint","text":"<p>After initialization, it is possible that the client encounters new schemas that did not exist yet at the time it was initialized. In those cases, the client can fetch those unknown schemas specifically via: <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/value_schema/&lt;store_name&gt;/&lt;id&gt;\n</code></pre> This returns a payload identical to the key schema endpoint.</p>"},{"location":"contributing/architecture/router-api/#storage-endpoints","title":"Storage Endpoints","text":"<p>After initialization, the client can begin querying the routers for data. There are three read operations supported in Venice:</p> <ul> <li>Single Get</li> <li>Batch Get</li> <li>Read Compute</li> </ul> <p>All storage endpoints should be queried with the following request header:</p> <pre><code>X-VENICE-API-VERSION = 1\n</code></pre> <p>Furthermore, the following header is optional:</p> <pre><code># Indicates that the client is capable of decompressing GZIP responses (if omitted, GZIP compressed stores will be \n# decompressed by the router, on behalf of the client).\nX-VENICE-SUPPORTED-COMPRESSION-STRATEGY = 1\n</code></pre>"},{"location":"contributing/architecture/router-api/#single-get-endpoint","title":"Single Get Endpoint","text":"<p>Querying the value for a single key can be achieved by sending an HTTP GET request to: <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/storage/&lt;store_name&gt;/&lt;key_in_base64&gt;?f=b64\n</code></pre></p> <p>Alternatively, for stores with a simple string key schema, the value can be gotten via: <pre><code>GET https://&lt;host&gt;:&lt;port&gt;/storage/&lt;store_name&gt;/&lt;string_key&gt;\n</code></pre></p> <p>The response code should be 200, and the value is encoded in Avro binary in the body of the response, with the following  response headers:</p> <pre><code># The Avro writer schema ID to lookup in the client's schema cache\nX-VENICE-SCHEMA-ID = 1\n\n# Valid values for the thin client include: 0 -&gt; NO_OP, 1 -&gt; GZIP\nX-VENICE-COMPRESSION-STRATEGY = 0\n</code></pre>"},{"location":"contributing/architecture/router-api/#batch-get","title":"Batch Get","text":"<p>Batch gets are performed as an HTTP POST request to: <pre><code>POST https://&lt;host&gt;:&lt;port&gt;/storage/&lt;store_name&gt;\n</code></pre></p> <p>The request should be accompanied by the following headers:</p> <pre><code># Not supplying the streaming header results in a legacy non-streaming mode which is less performant, and which may be \n# removed in the future\nX-VENICE-STREAMING = 1\n\n# This header makes the quota system more efficient (technically optional, but highly recommended)\nX-VENICE-KEY-COUNT = &lt;number of keys queried&gt;\n</code></pre> <p>The body of the POST request is a concatenation of all the queried keys, serialized in Avro binary. Note that the order of the keys matter (see details below).</p> <p>The response code will always be 200 since the router starts returning results incrementally before knowing if all parts of the request can be fulfilled successfully (but see the footer details below for the real status code). Furthermore, the values are laid out one after the other in the body of the response, packaged inside envelopes encoded in the  following Avro schema:</p> <pre><code>{\n  \"name\": \"MultiGetResponseRecordV1\",\n  \"namespace\": \"com.linkedin.venice.read.protocol.response\",\n  \"doc\": \"This field will store all the related info for one record\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"keyIndex\",\n      \"doc\": \"The corresponding key index for each record. Venice Client/Router is maintaining a mapping between a unique index and the corresponding key, so that Venice backend doesn't need to return the full key bytes to reduce network overhead\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"value\",\n      \"doc\": \"Avro serialized value\",\n      \"type\": \"bytes\"\n    },\n    {\n      \"name\": \"schemaId\",\n      \"doc\": \"Schema id of current store being used when serializing this record\",\n      \"type\": \"int\"\n    }\n  ]\n}\n</code></pre> <p>A few important details:</p> <ol> <li>The client needs to keep track of the order in which it wrote the keys in the request body, because the response does    not include the requested keys, but only a <code>keyIndex</code> corresponding to the key's position in the request body.</li> <li>The response envelopes are not coming in the same order as they were requested.</li> <li>When a requested key has no associated value, the envelope will contain the following sentinel values:</li> <li>The <code>keyIndex</code> is negative. </li> <li>The <code>value</code> is empty. </li> <li>The <code>schemaId</code> is <code>-1000</code>.</li> <li>If any errors occurred as part of handling this request, then a special envelope at the end of the response body is  included, enclosing a \"footer record\", with the following characteristics:</li> <li>The <code>keyIndex</code> is <code>-1000000</code>. </li> <li>The <code>value</code> is encoded using the Avro schema below.</li> <li>The <code>schemaId</code> is <code>-1001</code>.</li> </ol> <pre><code>{\n  \"name\": \"StreamingFooterRecordV1\",\n  \"namespace\": \"com.linkedin.venice.read.protocol.response.streaming\",\n  \"doc\": \"This record will store all the additional info after sending out streaming response\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"status\",\n      \"doc\": \"Final HTTP status code (non-200) after processing the streaming request completely\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"detail\",\n      \"doc\": \"Error detail\",\n      \"type\": \"bytes\"\n    },\n    {\n      \"name\": \"trailerHeaders\",\n      \"doc\": \"Additional headers after sending out response headers\",\n      \"type\": {\n        \"type\": \"map\",\n        \"values\": \"string\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"contributing/architecture/router-api/#read-compute","title":"Read Compute","text":"<p>TODO</p>"},{"location":"contributing/architecture/write-path/","title":"Introduction","text":"<p>As a high performance derived data storage platform, Venice is designed to ingest large amount of writes from a variety  of sources while providing low latency read access to the data. These characteristics are achieved by a combination of design choices and optimizations in the write path. This document describes some internal details of the Venice write path. </p> <p>Note that this page is still under construction and only provides glimpses of the Venice writing path. More details will  be added in the future.</p>"},{"location":"contributing/architecture/write-path/#terminology","title":"Terminology","text":"Name Description Store A store is a table or the dataset in Venice. Each store has multiple partitions. VT Version Topic. The version topic is a kafka topic where holds all the data for a given store. One VT per version per Store. RT Real time Topic. A real time topic is also a kafka topic but it only contains data from nearline producers. One RT per Store. ZK Apache ZooKeeper. It's used in multiple Venice components to persist Venice metadata. Helix Apache Helix. It's used in Venice to manage the cluster and assign partitions to different servers. EV Helix External View. Venice uses it to represent the state of a server in a leader/follower mode. CV Helix Customized View. Venice uses it to represent the ingestion state of a server for a given store."},{"location":"contributing/architecture/write-path/#venice-store-ingestion-on-server-side","title":"Venice Store Ingestion on Server Side","text":"<p>Helix assign leaders and followers status on each partition to different servers. When assignments are done, each server will create multiple <code>StoreIngestionTask</code>s to handle ingestion for the partitions it's assigned to. Note that there's one SIT per store per version. Multiple partitions of the same store and version are handled by the same SIT instance. The diagram below  describes the data flow of how a store ingestion task's created and ingest data on the server side.</p> <p></p>"},{"location":"contributing/architecture/write-path/#venice-store-ingestion-report","title":"Venice Store Ingestion Report","text":"<p>When ingestion is in different stages, the server reports the ingestion status. The ingestion signal is propagated to  various downstream for different business logics. The diagram below describes the signal flow of the ingestion report.</p> <p></p>"},{"location":"contributing/architecture/write-path/#quantities","title":"Quantities","text":"<p>When working on the codebase, I find it helpful to keep track of the quantities of different components in the system. Below, you'll find some visualizations of data structures and other things that are per-store, per-version, and per-partition. (Link to diagram source)</p> <p></p>"},{"location":"contributing/architecture/write-path/#venice-large-record-value-chunking","title":"Venice Large Record Value Chunking","text":"<p>Due to the ~1MB Kafka size limit, records larger than ~1MB will need to be chunked into multiple Kafka messages by <code>VeniceWriter#putLargeValue()</code>.</p> <p>In the batch push job path, the chunking in <code>VeniceWriter</code> (a class for writing data into Kafka) starts within <code>VenicePushJob</code> (more specifically, it's done in the <code>PartitionWriter</code> stage of the <code>DataWriterJob</code>). The chunks and manifest move through Kafka and the Venice Server and until they are stored unchanged in <code>RocksDB</code>. The original large record must be reconstructed at read-time by using the chunks and manifest in order to be accessed.</p> <p>In the nearline job / partial update path, the <code>Consumer</code> must first reconstruct the large record. In <code>RocksDB</code>, the original key of the record points to the <code>ChunkedValueManifest</code>, which includes a list of keys that point to the chunks. Using these components, the <code>Consumer</code> can reconstruct the large record, apply the partial update from RT, and then divide it into a new set of chunks and a new manifest. The new chunks are written to <code>RocksDB</code>, the stored manifest is updated with the new keys, and the old chunks are deleted.</p> <p>The diagram below illustrates the aforementioned chunking system. (Link to diagram source)</p> <p></p>"},{"location":"contributing/architecture/write-path/#push-job","title":"Push Job","text":"<p>More details about push jobs</p> <p></p>"},{"location":"contributing/development/dev-workflow/","title":"Venice Recommended Development Workflow","text":""},{"location":"contributing/development/dev-workflow/#create-a-design-document","title":"Create a Design Document","text":"<p>If your change is relatively minor, you can skip this step. If you are adding new major functionality, we suggest that  you add a design document and solicit comments from the community before submitting any code.</p> <p>Please follow the Design Document Guide. </p>"},{"location":"contributing/development/dev-workflow/#creating-github-issue","title":"Creating GitHub issue","text":"<p>Every PR should be preceded by a GitHub issue to explain the problem statement unless it's a trivial bug fixes or a documentation change. If your change is significant, please make sure your PR reviewers can align on the problem statement via GitHub issues first.</p> <p>The GitHub issue should contain the detailed problem statement.</p>"},{"location":"contributing/development/dev-workflow/#pull-request","title":"Pull Request","text":"<ol> <li>Fork the GitHub repository at http://github.com/linkedin/venice if you haven't already</li> <li>Clone your fork, create a new branch, push commits to the branch</li> <li>Consider whether documentation or tests need to be added or updated as part of the change, and add them as needed (doc changes should be submitted along with code change in the same PR)</li> <li>Run all tests as described in the project's Workspace setup guide.</li> <li>Open a pull request against the <code>main</code> branch of <code>linkedin/venice</code>. (Only in special cases would the PR be opened against other branches.)</li> <li>The PR title should usually be of the form <code>[component1]...[componentN]: Concise commit message</code>.</li> <li>Valid tags are: <code>[da-vinci]</code> (or <code>[dvc]</code>), <code>[server]</code>, <code>[controller]</code>, <code>[router]</code>, <code>[samza]</code>,       <code>[vpj]</code>, <code>[fast-client]</code> (or <code>[fc]</code>), <code>[thin-client]</code> (or <code>[tc]</code>), <code>[changelog]</code> (or <code>[cc]</code>),       <code>[producer]</code>, <code>[admin-tool]</code>, <code>[test]</code>, <code>[build]</code>, <code>[doc]</code>, <code>[script]</code>, <code>[compat]</code>, <code>[protocol]</code></li> <li><code>[compat]</code> tag means there are compatibility related changes in this PR, including upgrading protocol version, upgrading system store value schemas, etc. When there is a compatibility related change, it usually requires a specific deployment order, like upgrading controller before upgrading server. In this case, please explicitly call out the required deployment order in the commit message.</li> <li>If the pull request is still a work in progress, and so is not ready to be merged, but needs to be pushed to GitHub to facilitate review,     then create the PR as a draft PR</li> <li>If the PR cannot be created as a draft PR,      add <code>[WIP]</code> before the list of components.</li> <li>Please state that the contribution is your original work and that you license the work to the project under the project's open source license.</li> <li>If this PR resolves an issue be sure to include <code>Resolves #XXX</code> to correctly link and close the issue upon merge.</li> <li>The project uses Apache Jenkins for continuous testing on Linux AMD64 and ARM64 build nodes. A CI job will not be started automatically for pull request. A maintainer has to trigger the testing. Feel free to tag a maintainer and ask for a build trigger.</li> <li>Once ready, a maintainer will update the PR with the test results.</li> <li>Investigate and fix failures caused by the pull the request</li> <li>Fixes can simply be pushed to the same branch from which you opened your pull request.</li> <li>Please address feedback via additional commits instead of amending existing commits. This makes it easier for the reviewers to know what has changed since the last review. All commits will be squashed into a single one by the committer via GitHub's squash button or by a script as part of the merge process.</li> <li>Jenkins will automatically re-test when new commits are pushed.</li> <li>Despite our efforts, Venice may have flaky tests at any given point, which may cause a build to fail. You need to ping committers to trigger a new build. If the failure is unrelated to your pull request and you have been able to run the tests locally successfully, please mention it in the pull request.</li> </ol>"},{"location":"contributing/development/dev-workflow/#pr-description","title":"PR Description","text":"<p>Describe</p> <ul> <li>What changes to make and why you are making these changes.</li> <li>How are you going to achieve your goal</li> <li>Describe what testings you have done, for example, performance testing etc.</li> </ul> <p>Checklist that might be helpful to facilitate the review:</p> <ul> <li>Design one-pager, design doc, or RFC</li> <li>GitHub Issue</li> </ul>"},{"location":"contributing/development/dev-workflow/#added-new-dependencies","title":"Added new dependencies?","text":"<p>Please list the new dependencies in the PR description and answer these questions for each new dependency</p> <ul> <li>What's their license?</li> <li>Are they compatible with our license?</li> </ul>"},{"location":"contributing/development/dev-workflow/#the-review-process","title":"The Review Process","text":"<ol> <li>Other reviewers, including committers, may comment on the changes and suggest modifications. Changes can be added by simply pushing more commits to the same branch.</li> <li>Please add a comment and \"@\" the reviewer in the PR if you have addressed reviewers' comments. Even though GitHub sends notifications when new commits are pushed, it is helpful to know that the PR is ready for review once again.</li> <li>Lively, polite, rapid technical debate is encouraged from everyone in the community. The outcome may be a rejection of the entire change.</li> <li>Reviewers can indicate that a change looks suitable for merging by approving it via GitHub's review interface. This indicates the strongest level of technical sign-off on a patch and it means: \"I've looked at this thoroughly and take as much ownership as if I wrote the patch myself\". If you approve a pull request, you will be expected to help with bugs or follow-up issues on the patch. Consistent, judicious use of pull request approvals is a great way to gain credibility as a reviewer with the broader community. Venice reviewers will typically include the acronym LGTM in their approval comment. This was the convention used to approve pull requests before the \"approve\" functionality was introduced by GitHub.</li> <li>Sometimes, other changes will be merged which conflict with your pull request's changes. The PR can't be merged until the conflict is resolved. This can be resolved with <code>\"git fetch upstream\"</code> followed by <code>\"git rebase upstream/main\"</code> and resolving the conflicts by hand, then pushing the result to your branch.</li> <li>Try to be responsive to the discussion rather than let days pass between replies.</li> </ol>"},{"location":"contributing/development/dev-workflow/#closing-your-pull-request-issue","title":"Closing Your Pull Request / Issue","text":"<ol> <li>If a change is accepted, it will be merged and the pull request will automatically be closed, along with the associated Issue if the PR description contains <code>Resolves #XXX</code></li> <li>Note that in the rare case you are asked to open a pull request against a branch besides <code>main</code>, that you will actually have to close the pull request manually</li> <li>If your pull request is ultimately rejected, please close it.</li> <li>If a pull request has gotten little or no attention, consider improving the description or the change itself and ping likely reviewers after a few days. Consider proposing a change that's easier to include, like a smaller and/or less invasive change.</li> <li>If a pull request is closed because it is deemed not the right approach to resolve an Issue, then leave the Issue open. However, if the review makes it clear that the problem identified in the Issue is not going to be resolved by any pull request (not a problem, won't fix) then also resolve the Issue.</li> </ol>"},{"location":"contributing/development/dev-workflow/#attribution-acknowledgements","title":"Attribution &amp; Acknowledgements","text":"<p>This guide is based on the Contributing Code Changes guide from the Apache Kafka project.</p>"},{"location":"contributing/development/style-guide/","title":"Style Guide","text":"<p>{: .no_toc }</p> <p>This page describes some stylistic concerns which the development team cares about. Some of them are enforced by  automation, while others are guidelines of a more informal, or even philosophical, nature. More generally, we believe in  acquiring a deep understanding of the principles behind these guidelines, and being thoughtful about which situation  they apply or don't apply to. We don't buy into mainstream ideas such as \"all coupling is bad\", \"all optimizations are premature\", etc. We take this common wisdom and incorporate it into our reflections, without blindly taking it at face value.</p> <p>We also want to acknowledge that our conclusions change over time, either due to hindsight or because of the evolving  context that the project needs to navigate. As such, this should be thought of as a living document, and it is therefore  natural that not all parts of the code base adhere to it perfectly. When such deviations are discovered, it is  encouraged to try to rectify them \"in passing\", even if the objective of the code change is unrelated. This, too, is a  judgment call, and needs to be balanced by concerns like managing the risk of unintended regressions.</p> <p>For those who are new to the Java language, it may be useful to read this introduction to Java.</p>"},{"location":"contributing/development/style-guide/#table-of-contents","title":"Table of Contents","text":"<p>{: .no_toc }</p> <ul> <li>TOC</li> </ul>"},{"location":"contributing/development/style-guide/#automation","title":"Automation","text":"<p>Regarding code style, we use the Eclipse Java Formatter variant of Spotless, which automatically reformats the code as part of git commit hooks. Make sure to run <code>./gradlew assemble</code> to get the git hooks set up.</p> <p>We also use Spotbugs, with some of the rules  resulting in build failures if violated. Over time, we intend to pick up more of these rules, fix the code to comply  with them, and add them to the list. If you would like to contribute to this effort, feel free to open an issue and  suggest which rules you are interested in fixing. Note that there are a few rules that we intend to ignore  as they seem to be imprecise or not sufficiently useful.</p> <p>In the future, we might add other forms of automation to increase code quality along other dimensions. Feel free to suggest ideas in this space.</p>"},{"location":"contributing/development/style-guide/#guidelines","title":"Guidelines","text":"<p>Below are a set of guidelines to take into consideration when developing in the Venice project. They are not absolute rules and there may be good reasons to deviate from them occasionally. When deviating, it is useful to leave comments explaining why we deviated, whether it was intentional, or due to the need for expediency. This helps future maintainers  understand what is actually worth cleaning up and how careful they need to be when doing it.</p>"},{"location":"contributing/development/style-guide/#compatibility","title":"Compatibility","text":"<p>We care about compatibility across versions of the software. This is a bidirectional statement. Old clients need to be able to talk with new servers, and new clients with old servers as well. It also includes interactions across lifetimes of the same process, for example, state persisted by an old version of the server code should be usable by a newer  version of the server code, and vice versa. If compatibility is impossible, then we should look for ways to achieve correct behavior anyway (e.g. potentially at the cost of efficiency, such as the server needing to throw away and regenerate the state).</p> <p>For enums which are going to be communicated across processes or across lifetimes of the same process, consider using  VeniceEnumValue, EnumUtils and related unit test classes, which provide a structure to minimize the chance that we mistakenly change the mapping of numeric ID -&gt; enum value.</p>"},{"location":"contributing/development/style-guide/#javadoc","title":"JavaDoc","text":"<p>Speaking of comments, we ideally want JavaDoc at the top of all classes. The top of class JavaDoc should indicate the set of responsibilities of the class. If the list of responsibilities grows long and/or lacks a common theme, it may be an indicator that the class ought to be split up.</p> <p>JavaDoc for functions is desired for public client APIs intended to be leveraged directly by end users. For internal functions, JavaDoc is desired only if there is something important to call out. Sometimes, a lengthy function JavaDoc may be an indication that the function's name is not sufficiently clear, or that the complex function should be split  into multiple simpler (and well-named!) functions.</p> <p>Use a single-line JavaDoc if it fits, e.g., <code>/** A very short description */</code></p> <p>Note that we also use JavaDoc everywhere we feel like, not just at the top of functions and classes. This is atypical, but intentional. It allows us to use the <code>{@link ClassName}</code> syntax in order to make the code more easily navigable and refactorable. If your IDE complains about dangling JavaDocs, that hint should be disabled. If using IntelliJ, this will be configured automatically when calling: <code>./gradlew cleanIdea idea</code></p>"},{"location":"contributing/development/style-guide/#logging","title":"Logging","text":"<p>We use log4j2 and want to use interpolation, rather than manual string concatenation, everywhere for efficiency reasons.  Note that Spotless may break up fragments of strings by concatenating over multiple lines, but that doesn't matter as it gets optimized away by the compiler. Only concatenations with variables end up carrying an overhead.</p> <p>Hot path logging should not be above debug level. Keep in mind that exception paths could occasionally turn into hot  paths. In those cases, we may want to use the <code>RedundantExceptionFilter</code>. More generally, hot path logging typically benefits from being converted into a metric instead.</p> <p>Do your best to make log messages meaningful. Avoid scary yet imprecise wording. If the situation is dire, let the log spell out precisely what happened, along with enough context to make debugging easier.</p> <p>If there is a known solution to remediate the issue, consider why isn't this solution reactively activated so the system  fixes itself, rather than just logging? If the solution exists but cannot be wired in reactively, then it may be  desirable for the log to indicate what that solution is, to give the user or operator a clue about what to do next  (i.e. the Ghost in the Shell design pattern).</p>"},{"location":"contributing/development/style-guide/#encapsulation","title":"Encapsulation","text":"<p>As much as possible, try to maintain tight encapsulation in all classes. Internal state should be exposed as little as feasible, and possibly not at all. Consider providing only getters, and not setters, if there is no need for the latter. Always be careful when returning objects (as opposed to primitives) as these may contain state that can then be mutated from outside the class they originated from. For example, instead of returning a map, it may be preferable to expose only a getter for retrieving entries from this map. Alternatively, the map could be placed in a read-only wrapper (but do consider this option carefully if it is going to happen on the hot path, in which case perhaps the read-only wrapper could be pre-allocated, especially if the wrapped map is final).</p> <p>If the API of a class is such that another class needs to call multiple functions in a row to achieve a desired outcome, then ask yourself whether the calling class is hand holding the internal state of the called class. Would the internal state of the called class be left in an inconsistent or incoherent state if the calling class stopped halfway through its sequence of function calls, or if it called those same functions in a different order? If the answer is yes, then perhaps the multiple functions should be presented as a single public function, which then internally delegates to many private functions, in the correct order.</p> <p>Avoid passing <code>this</code> into classes as this may make the flow of the code difficult to understand. Also, more generally, consider whether a class actually needs a handle of an instance of an entire other class (and thus have the ability to  call any of its functions), or whether it could make do with an instance of a more constrained interface, which the  other class implements, or perhaps even just a handle to a specific function of the other class (thus limiting the  surface area of their interaction).</p>"},{"location":"contributing/development/style-guide/#avoid-wildcard-imports","title":"Avoid Wildcard Imports","text":"<p>We avoid wildcard imports since they may lead to bugs due to pulling in unintended classes or functions. If using  IntelliJ, the auto-conversion of imports into wildcards can be disabled by following these  instructions. This would be a good candidate for automation, perhaps via a new Spotbugs plugin; contributions welcome!</p>"},{"location":"contributing/development/style-guide/#avoid-optionals","title":"Avoid Optionals","text":"<p>We are aligned with the philosophy of the original creators of the Optional API, which is that it is a useful construct in the context of the Java 8 stream APIs, but should generally not be used beyond that. Null is a perfectly appropriate way to denote emptiness, and is not more or less likely to cause a <code>NullPointerException</code>. Sentinel values in primitive  types (e.g., <code>-1</code> for a numeric value that is otherwise expected to be positive) are also perfectly appropriate ways to denote emptiness. For more info, here are a good video and  post on this subject.</p>"},{"location":"contributing/development/style-guide/#avoid-java-stream-api-in-hot-paths","title":"Avoid Java Stream API in Hot Paths","text":"<p>The Java Stream API (e.g., <code>list.stream().map(...)</code>) is quite expensive both in terms of CPU and memory allocation, and not necessarily more readable than traditional for loops. Use it sparingly, and only in code which is for sure not a hot path (e.g., it is fine for control plane logic, but not for read and write hot paths).</p>"},{"location":"contributing/development/style-guide/#avoid-double-brace-initialization","title":"Avoid Double Brace Initialization","text":"<p>Java supports anonymous classes, which are fine in some contexts. For example:</p> <pre><code>interface MyInterface {\n  void foo();\n}\n\nclass Sample {\n  void sample() {\n    MyInterface anonymousClassInstance = new MyInterface() {\n      @Override\n      public void foo() {\n        System.out.println(\"bar\");\n      }\n    };\n  }\n}\n</code></pre> <p>Java also supports static blocks, which are executed once the first time a class is loaded. For example:</p> <pre><code>class Sample2 {\n  static {\n    System.out.println(\"This will always be printed first, and only once.\");\n  }\n\n  void sample() {\n    System.out.println(\"This will be printed everytime the function is called, but always after the static block.\");\n  }\n}\n</code></pre> <p>There is an antipattern which consists of initializing a collection (such as a map) with \"double braces\" in order to add elements into it. This is a combination of the above two techniques, with the first (outer) set of braces denoting the anonymous class, and the second (inner) set of braces denoting a static block. For example:</p> <pre><code>class Sample3 {\n  void sample() {\n    // antipattern:\n    Map&lt;String, String&gt; map1 = new HashMap&lt;&gt;() {\n      {\n        put(\"k1\", \"v1\");\n        put(\"k2\", \"v2\");\n      }\n    };\n\n    // correct way:\n    Map&lt;String, String&gt; map2 = new HashMap&lt;&gt;();\n    map2.put(\"k1\", \"v1\");\n    map2.put(\"k2\", \"v2\");\n\n    // other correct way:\n    Map&lt;String, String&gt; map3 = CollectionUtil.&lt;String, String&gt;mapBuilder()\n        .put(\"k1\", \"v1\")\n        .put(\"k2\", \"v2\")\n        .build();\n  }\n}\n</code></pre> <p>The reason to avoid the above antipattern is that anonymous classes take up memory in the JVM's metaspace, and we only wish to pay this overhead for cases where we really need a separate class. The double brace style is not the only way of populating a map, nor is it even the least verbose way to do it, so there is no point in doing it this way.</p>"},{"location":"contributing/development/style-guide/#look-for-ways-to-mitigate-failures","title":"Look for Ways to Mitigate Failures","text":"<p>In a system with lots of moving parts, it should be expected that things fail all the time. We should look for design patterns that help us mitigate failures, wherever possible.</p> <p>An example of this is the way that dataset versions work in Venice. A dataset version has a unique name, based on the  dataset name concatenated with a monotonically increasing version number. A given dataset version name is immutable, in the sense that it will forever point to one and only dataset version, and cannot be reused. Even if a dataset is deleted and then re-created under the same name, we don't restart the version number sequence, so there cannot be a clash in the names of dataset versions coming from before and after the dataset re-creation. A dataset version is associated with various resources including a Kafka topic, a Helix resource, persistent state, and in-memory state. When purging an old dataset version, if any of the resources that constitute it fail to get cleaned up properly, it doesn't prevent future dataset versions from continuing to get created, since they should never clash. In this case, therefore, a failure to delete a resource results not in immediate and widespread systemic failure, but merely in a resource leak, which can be  monitored, alerted on, and remediated if it creeps beyond a certain threshold.</p>"},{"location":"contributing/development/style-guide/#be-a-benevolent-tyrant","title":"Be a Benevolent Tyrant","text":"<p>The CPU will do whatever we tell it, day in day out, without complaints, but it does not mean we ought to abuse it. Although there is undoubtedly a kernel of truth in the saying that \"premature optimization is the root of all evil\", it is important to consider that the reverse is not equally true. In other words, non-optimized code is not the root of  all clean code. This picture from one of the talks by Java performance expert Aleksey Shipil\u00ebv describes the idea in an  easy to grasp manner:</p> <p></p> <p>For example, if a class contains some final string property, and the code in this class repeatedly performs a lookup by that property, then it implies that the result of this lookup may change over time. If that is true, then the code is fine, but if it is not true that the result of the lookup would change over time, then it is simply useless code. Doing the lookup just once, and caching the result in another final property, makes the code not only faster and more  efficient, but also easier to reason about, since it indicates the immutability of this looked up property.</p> <p>Another example is interrogating a map to see if it contains a key, and if true, then getting that key out of the map. This requires 2 lookups, whereas in fact only 1 lookup would suffice, as we can get a value from the map and then check whether it's null. Moreover, doing it in 1 lookup is actually cleaner, since it eliminates the race condition where the lookup may exist during the <code>containsKey</code> check but then be removed prior to the subsequent <code>get</code> call. Again, the  faster code is cleaner.</p> <p>Yet another example is using the optimal data structure for a given use case. A frequent use case within Venice is to look something up by partition number (which are in a tight range, between zero and some low number), and thus it is possible do the job with either an int-keyed map or an array. If both work equally well from a functional standpoint,  then let us use an array, as it is more efficient to perform an index lookup within an array than a map lookup. For  collections of primitives, it is advised to consider using fastutil. If using a more  efficient data structure requires significant acrobatics, then we may still prefer to opt for the less efficient one,  for the sake of maintainability (e.g., if it falls within the red zone of the above diagram). That being said, we should  consider whether we can build a new data structure which achieves both convenience and efficiency for a given use case (e.g., yellow zone). This kind of low-level work is not considered off-limits within Venice, and we welcome it if there  is a good rationale for it.</p> <p>More generally, always keep in mind that the hot path in Venice may be invoked hundreds of thousands of times per second  per server, and it is therefore important to minimize overhead in these paths. By being benevolent tyrants, our CPUs  serve us better, and will hopefully care for us when AGI takes over the world.</p>"},{"location":"contributing/development/testing/","title":"Code Coverage Guide","text":"<p>The Venice repository has two code coverage check on GitHub Action, submodule-level coverage verification and new-commit coverage verification, which aim to improve the testability and code quality overall. The GitHub Action fails the Pull Request if the coverage check doesn't meet the requirements. This guide provides the details on how these reports are generated and how to debug them.</p>"},{"location":"contributing/development/testing/#module-structure","title":"Module structure","text":"<p>Venice has 4 modules, <code>all-modules</code>, <code>clients</code>, <code>internal</code> and <code>services</code>. Each module has its own submodules, except <code>all-modules</code>.</p>"},{"location":"contributing/development/testing/#clients","title":"Clients","text":"<p><code>da-vinci-client</code>, <code>venice-admin-tool</code>, <code>venice-client</code>, <code>venice-push-job</code>, <code>venice-samza</code>, <code>venice-thin-client</code></p>"},{"location":"contributing/development/testing/#internal","title":"Internal","text":"<p><code>alpini</code>, <code>venice-avro-compatibility-test</code>, <code>venice-client-common</code>, <code>venice-common</code>, <code>venice-consumer</code>,  <code>venice-jdk-compatibility-test</code>, <code>venice-test-common</code></p>"},{"location":"contributing/development/testing/#services","title":"Services","text":"<p><code>venice-router</code>, <code>venice-controller</code>, <code>venice-server</code></p>"},{"location":"contributing/development/testing/#submodule-level-coverage-verification","title":"Submodule-level coverage verification","text":""},{"location":"contributing/development/testing/#description","title":"Description","text":"<p>This will check the overall code coverage at the submodule level. Jacoco generates the report based off the current submodule, performs the coverage verification and fails the build if the coverage is below the threshold. The threshold, targets at the branch coverage, is defined in the <code>build.gradle</code> of each  submodule independently.</p>"},{"location":"contributing/development/testing/#example-commands","title":"Example commands","text":"<pre><code># Template\n./gradlew :&lt;module name&gt;:&lt;submodule name&gt;:jacocoTestCoverageVerification \n\n./gradlew :clients:venice-push-job:jacocoTestCoverageVerification \n</code></pre> <p>The report will be located at <code>&lt;module name&gt;/&lt;submodule name&gt;/build/reports/jacoco/test/index.html</code>.</p>"},{"location":"contributing/development/testing/#debugging-notes","title":"Debugging notes","text":""},{"location":"contributing/development/testing/#run-commands-against-module-doesnt-generate-the-report","title":"Run commands against module doesn't generate the report","text":"<p>Due to the current project setup, running the commands, e.g. <code>./gradlew :clients:jacocoTestCoverageVerification</code>, doesn't execute the unit tests thus no jacoco report will be generated. Please be sure to run the commands against the submodule.</p>"},{"location":"contributing/development/testing/#new-commit-coverage-verification","title":"New-commit coverage verification","text":""},{"location":"contributing/development/testing/#description_1","title":"Description","text":"<p>This will check the overall code coverage at the commit-level. DiffCoverage, which is an extension of Jacoco, gathers the diff files by comparing the local branch and remote upstream  main branch, and leverages the Jacoco report, to re-generate a new report only for these newly added lines/files. Similarly, it performs the coverage verification and fails the build if the coverage is below the threshold. The threshold is defined at 60% for branch coverage.</p>"},{"location":"contributing/development/testing/#example-commands_1","title":"Example commands","text":"<pre><code># Template\n./gradlew :&lt;module name&gt;:&lt;submodule name&gt;:jacocoTestCoverageVerification diffCoverage --continue\n\n./gradlew :clients:venice-push-job:jacocoTestCoverageVerification diffCoverage --continue\n</code></pre> <p>The report will be located at <code>&lt;module name&gt;/build/reports/jacoco/diffCoverage/html/index.html</code>.</p>"},{"location":"contributing/development/testing/#debugging-notes_1","title":"Debugging notes","text":""},{"location":"contributing/development/testing/#integration-tests-are-added-but-diffcoverage-doesnt-identify-the-code-coverage-of-it","title":"Integration tests are added but DiffCoverage doesn't identify the code coverage of it","text":"<p>Though integration tests are strongly encouraged, both Jacoco and DiffCoverage only work with unit tests so please consider writing unit tests.</p>"},{"location":"contributing/development/testing/#unit-tests-are-added-but-the-diffcoverage-report-doesnt-reflect-my-changes","title":"Unit tests are added but the DiffCoverage report doesn't reflect my changes","text":"<p>There are two possible reasons. 1. Jacoco report isn't up-to-date. DiffCoverage report relies on the Jacoco Report to reflect the correct coverage.  If the Jacoco report isn't up-to-date, for example, executing the wrong command <code>./gradlew :clients:venice-push-job:diffCoverage</code> which misses the step of re-running unit tests and updating the Jacoco report, can cause this issue.     1. Please be sure to run test and Jacoco report first. 2. Units tests are placed in a different module. Jacoco can only analyze and generate reports based off the current  submodule. So, if you write new source codes in <code>submodule A</code> and <code>submodule B</code>, and you only have written unit tests in <code>submodule B</code> which cover changes in <code>submodule A</code>, this cannot be recognized by Jacoco thus the DiffCoverage doesn't think there's coverage too.    1. Please move/re-organize some unit tests to the right submodule such that the coverage can be detected and reported.</p>"},{"location":"contributing/development/testing/#the-diffcoverage-report-shows-some-files-dont-belong-to-my-local-changes","title":"The DiffCoverage report shows some files don't belong to my local changes","text":"<p>That's usually due to your local branch and upstream is not up-to-date so when it runs <code>git diff</code>, newly merged codes in <code>linkedin/venice</code> are mistakenly treated as your changes.</p> <p>Please do the followings: 1. Go to your Github fork(https://github.com//venice) and sync the <code>main</code> branch with upstream <code>linkedin:main</code> 2. Run <code>git fetch upstream</code> locally and pull the latest changes to your <code>main</code> branch 3. Merge <code>main</code> branch to your feature branch. 4. Confirm the diff are only your changes by running <code>git diff upstream/main</code>."},{"location":"contributing/development/workspace-setup/","title":"Venice Workspace Setup","text":"<p>We recommend using a Unix-based environment for development, such as Linux or macOS. If you're on Windows, we recommend using WSL2.</p>"},{"location":"contributing/development/workspace-setup/#fork-the-venice-repository","title":"Fork the Venice Repository","text":"<p>Fork the Venice repo at https://github.com/linkedin/venice.</p>"},{"location":"contributing/development/workspace-setup/#setting-up-the-repository-locally","title":"Setting up the repository locally","text":"<pre><code>git clone git@github.com:${githubUsername}/venice.git\ncd venice\ngit remote add upstream git@github.com:linkedin/venice.git\ngit fetch upstream\n</code></pre>"},{"location":"contributing/development/workspace-setup/#setting-up-java","title":"Setting up Java","text":"<p>We use Java 17 for development. You can download it here.</p> <p>Make sure to set the <code>JAVA_HOME</code> environment variable to the location of your JDK installation. How to do this will be dependent on your OS.</p>"},{"location":"contributing/development/workspace-setup/#setting-up-the-ide","title":"Setting up the IDE","text":"<p>We recommend using IntelliJ IDEA for development to take advantage of the debugger, and provide instructions for it. However, any IDE of your choice should work.</p> <p>To get the free version of IntelliJ IDEA visit the JetBrains website, and download the Community Edition version (not Ultimate). It should be the second download button on the page.</p> <p>To set up IntelliJ, run: <pre><code>./gradlew idea\n</code></pre></p>"},{"location":"contributing/development/workspace-setup/#disable-wildcard-imports","title":"Disable wildcard imports","text":"<ol> <li>In IntelliJ, click the gear icon in the top right and select settings</li> <li>Go to Editor -&gt; Code Style -&gt; Java and select Imports from the tabs</li> <li>Next to Scheme, select the gear icon and hit duplicate</li> <li>Only check <code>Use single class import</code></li> <li>Set <code>Class count to use import with *</code> to 1000</li> <li>Set <code>Names count to use static import with *</code> to 1000</li> </ol>"},{"location":"contributing/development/workspace-setup/#setting-up-your-system","title":"Setting up your system","text":""},{"location":"contributing/development/workspace-setup/#set-the-maximum-number-of-open-files-limit","title":"Set the maximum number of open files limit","text":"<p>There are many resources on the web for updating the limits of maximum number of open files for each operating system. We recommend setting a soft limit of at least <code>64000</code> and a hard limit of at least <code>524288</code>. Feel free to experiment with various values and find ones that work for you.</p>"},{"location":"contributing/development/workspace-setup/#build-the-project","title":"Build the project","text":"<pre><code>./gradlew clean assemble\n</code></pre>"},{"location":"contributing/development/workspace-setup/#run-the-test-suite","title":"Run the test suite","text":"<pre><code>./gradlew check --continue\n</code></pre>"},{"location":"contributing/development/workspace-setup/#gradle-cheatsheet","title":"Gradle Cheatsheet","text":"<pre><code># Build project classes and artifacts\n./gradlew assemble\n\n# Run all checks (spotbugs, code coverage, tests etc)\n./gradlew check --continue\n\n# Run only enabled checks that may fail the build\n./gradlew spotbugs\n\n# Run all checks, including ones that are in review/incubating stage, and do not fail the build\n./gradlew spotbugs -Pspotallbugs -Pspotbugs.ignoreFailures\n\n# Run jacoco code coverage check, which will also generate jacoco report\n./gradlew jacocoTestCoverageVerification\n\n# Run jacoco code coverage check along with jacoco report and diff coverage check\n./gradlew jacocoTestCoverageVerification diffCoverage --continue\n\n# Run enabled checks only for main code and in da-vinci-client subproject\n./gradlew :clients:da-vinci-client:spotbugsMain\n\n # Run jacoco code coverage check for a specific module along with jacoco report\n./gradlew :clients:da-vinci-client:jacocoTestCoverageVerification\n\n# Run jacoco code coverage check for a specific module along with jacoco report and diff coverage check\n./gradlew :clients:da-vinci-client:jacocoTestCoverageVerification diffCoverage\n\n# Run a specific test in any module\n./gradlew :sub-module:testType --tests \"fully.qualified.name.of.the.test\"\n\n# To run a specific integration test\n./gradlew :internal:venice-test-common:integrationTest --tests \"fully.qualified.name.of.the.test\"\n</code></pre>"},{"location":"contributing/documentation/design-docs/","title":"Design Documents","text":""},{"location":"contributing/documentation/design-docs/#venice-improvement-proposal-vip","title":"Venice Improvement Proposal (VIP)","text":"<p>Venice follow the Venice Improvement Proposal (VIP). VIP is the mechanism used to propose changes to the  Venice Codebase. </p> <p>Not all the changes require a VIP treatment, Please use your commonsense to check if writing this VIP help yourself  and the reviewers time instead of just doing it in the pull request itself. Generally, this applies to * Large code refactors * New functionality  * Public API changes</p> <p>In practical terms, the VIP defines a process in which developers can submit a design doc,  receive feedback and get the \"go ahead\" to execute.</p>"},{"location":"contributing/documentation/design-docs/#who-can-create-a-vip","title":"Who can create a VIP?","text":"<p>Any person willing to contribute to the Venice project is welcome to create a VIP.</p>"},{"location":"contributing/documentation/design-docs/#how-does-the-vip-process-work","title":"How does the VIP process work?","text":"<p>A VIP proposal can be in these states: 1. DRAFT: (Optional) This might be used for contributors to collaborate and    to seek feedback on an incomplete version of the proposal.</p> <ol> <li> <p>DISCUSSION: The proposal has been submitted to the community for    feedback and approval.</p> </li> <li> <p>ACCEPTED: The proposal has been accepted by the Venice project.</p> </li> <li> <p>REJECTED: The proposal has not been accepted by the Venice project.</p> </li> <li> <p>IMPLEMENTED: The implementation of the proposed changes have been    completed and everything has been merged.</p> </li> <li> <p>RELEASED: The proposed changes have been included in an official    Venice release.</p> </li> </ol> <p>The process works in the following way:</p> <ol> <li>The author(s) of the proposal will create a file named \"VIP-xxx.md\" in proposal folder     cloning from the template for VIP proposals. The \"xxx\" number should      be chosen to be the next number from the existing VIP issues, listed here</li> <li>The author(s) submit this file as a PR named \"VIP-xxx: {short description}\" in DRAFT/DISCUSSION stage.</li> <li>People discuss using PR comments, each is its own threaded comment.     General comments can be made as general comment in the PR. There are two other ways for an interactive    discussion. </li> <li>Venice Community Slack Channel, Create a slack channel with #VIP-xxx</li> <li>Venice Contributor Sync Meeting, see details here at Contributor Sync Meeting</li> <li>Depending on the outcome of the discussion, the status could move to ACCEPTED or REJECTED, or it could stay     in DISCUSSION stage (e.g. if we agree tentatively on the broad strokes, but there are still action items to     refine certain aspects). At the end of this, the PR gets merged, and at that point the VIP will appear in the     directory of all historical VIPs.</li> <li>If the merged VIP is still in DISCUSSION stage, then further PRs will be submitted to address     the remaining action items.</li> <li>If the VIP is ACCEPTED, then next stages are implementation, with eventually some code change PR also     carrying an alteration of the VIP page to move the status to IMPLEMENTED.</li> <li>All Pull Requests for this functionality, should prefix this \"VIP-XXX\" number in the title for quick access. </li> </ol>"},{"location":"contributing/documentation/design-docs/#acknowledgements","title":"Acknowledgements","text":"<p>This guide is inspired from the  Apache Pulsar project proposal plan. </p>"},{"location":"contributing/documentation/writing-docs/","title":"Documentation Guideline","text":"<p>We use GitHub Pages to host Venice documentation with MkDocs Material theme. Documentation is built automatically by GitHub Actions when changes are pushed to the <code>main</code> branch.</p>"},{"location":"contributing/documentation/writing-docs/#general","title":"General","text":"<p>It is strongly encouraged that any code change which affects the validity of information in the docs also include  updates to the docs, so that both are kept in sync atomically.</p> <p>Experimental functionalities and future plans are also worth documenting, though they must be clearly marked as such, so that users and operators reading those docs can make informed decisions about the level of risk they are willing to take on if trying out a given functionality. If the level of maturity of a given functionality is not called out, then it implicitly means that the functionality is considered mature and its API is unlikely to change. Undocumented configs and APIs may or may not be considered mature and stable, and if in doubt, it is appropriate to open an Issue to request that it be explicitly documented.</p> <p>In general, it is recommended to get familiar with the docs before writing more docs, to try to keep the style and structure coherent. That being said, even if unsure where some documentation belongs, do err on the side of including it (anywhere), and reviewers may suggest placing it elsewhere.</p>"},{"location":"contributing/documentation/writing-docs/#hierarchy","title":"Hierarchy","text":"<p>Documentation hierarchy is configured in <code>mkdocs.yml</code> in the <code>nav:</code> section. Pages do not require front matter. The navigation structure is defined centrally:</p> <pre><code>nav:\n  - Home: index.md\n  - User Guide:\n    - user-guide/index.md\n    - Write APIs:\n      - user-guide/write-apis/batch-push.md\n</code></pre> <p>To add a new page: 1. Create the markdown file in the appropriate directory 2. Add an entry to the <code>nav:</code> section in <code>mkdocs.yml</code> 3. The page title is taken from the first <code># Heading</code> in the markdown file</p> <p>For more information, consult MkDocs Material Documentation.</p>"},{"location":"contributing/documentation/writing-docs/#pictures-and-diagrams","title":"Pictures and Diagrams","text":"<p>It is encouraged to use diagrams within the documentation, but there are some guidelines to standardize the way it is done, and to avoid certain anti-patterns.</p>"},{"location":"contributing/documentation/writing-docs/#text-based-assets","title":"Text-based Assets","text":"<p>For text-based assets (which are preferred whenever feasible), we wish to check them into source control. This should  include both the displayable asset (e.g. in SVG format) and the source file from which the displayable asset was  generated (e.g. in XML format). This makes the docs self-contained, and enables contributors to edit the assets over  time.</p> <p>Diagrams conforming to these guidelines can be placed under the <code>/docs/assets/images</code> path of the repo, and then  embedded in the docs with a relative link like this:</p> <pre><code>![](../../assets/images/vip_3_read_path.drawio.svg)\n</code></pre> <p>The draw.io service makes it easy to generate such assets. If using PlantUML,  it's recommended to generate diagrams into svg format by following this guide.</p>"},{"location":"contributing/documentation/writing-docs/#binary-assets","title":"Binary Assets","text":"<p>For binary assets (e.g. PNG, BMP, JPG, etc.), we do NOT wish to check them into source control. Instead, they should be  linked from an external source. This can be done in GitHub itself. Within the Pull Request that proposes the doc change,  the contributor can insert images in the PR's description or comments, and then take the URL GitHub generated for it.  Then the modified files included in the PR can be edited to link to that image, and the PR updated. Externally hosted  images can be embedded with an absolute link like this:</p> <pre><code>![](https://user-images.githubusercontent.com/1248632/195111861-518f81c4-f226-4942-b88a-a34337da79e3.png)\n</code></pre>"},{"location":"contributing/documentation/writing-docs/#emojis","title":"Emojis","text":"<p>Here's a link to all the emojis available in README files: Emoji Cheat Sheet. </p>"},{"location":"contributing/documentation/writing-docs/#testing-doc-changes","title":"Testing Doc Changes","text":"<p>There are two ways to test doc changes, locally and on the public web. Local testing is convenient to iterate quickly, while public web testing is useful to make sure that nothing breaks (e.g., especially if changing styles, Ruby  dependencies, or Jekyll configs) and to share more significant documentation changes with PR reviewers.</p>"},{"location":"contributing/documentation/writing-docs/#testing-locally","title":"Testing Locally","text":"<p>The docs are rendered and served by MkDocs with the Material theme. Install dependencies:</p> <pre><code>pip install -r requirements-docs.txt\n</code></pre> <p>Then from the repository root, run:</p> <pre><code>python3 -m mkdocs serve\n</code></pre> <p>Then navigate to <code>http://127.0.0.1:8000</code> and view the docs in your browser. MkDocs hot reloads changes to markdown files and configuration automatically. If you modify <code>mkdocs.yml</code>, the server will restart automatically.</p> <p>For more options, see <code>docs/LOCAL_DEPLOYMENT.md</code>.</p>"},{"location":"contributing/documentation/writing-docs/#testing-on-the-public-web","title":"Testing on the Public Web","text":"<p>A GitHub fork can have its own documentation. This can be setup by:</p> <ol> <li>Navigating to the fork's Settings &gt; Pages, i.e.: <code>https://github.com/&lt;username&gt;/venice/settings/pages</code></li> <li>Selecting which branch to publish the docs from</li> <li>Selecting \"GitHub Actions\" as the source (not \"Deploy from a branch\")</li> <li>Push changes to trigger the <code>deploy-docs.yml</code> workflow</li> <li>Navigate to your fork's docs at: <code>https://&lt;username&gt;.github.io/venice</code></li> </ol> <p>For significant doc changes, please follow this process and add a link inside the PR to the docs hosted in the PR author's own fork.</p>"},{"location":"contributing/proposals/","title":"Venice Improvement Proposals (VIPs)","text":"<p>VIPs document significant design decisions and architectural changes.</p> <p>VIP Process \u2192</p>"},{"location":"contributing/proposals/#active-vips","title":"Active VIPs","text":"VIP Title Status VIP-1 Authentication Service API Accepted VIP-2 Removing Per Record Offset Metadata Under Discussion VIP-3 Rust Server Read Path Under Discussion VIP-4 Store Lifecycle Hooks Accepted VIP-5 Facet Counting Under Discussion VIP-6 Venice on Kubernetes Under Discussion <p>VIP Template \u2192</p>"},{"location":"contributing/proposals/proposals/","title":"Developer Guide","text":"<p>This folder includes all the Venice Improvement Proposals. Read further details about the VIP process. </p> VIP-# Proposal Status VIP-1 Authentication Service API Accepted VIP-2 Removing Per Record Offset Metadata From Venice-Server Storage With Heartbeats Under Discussion VIP-3 Rust Server Read Path Under Discussion VIP-4 Store Lifecycle Hooks Accepted VIP-5 Facet Counting Under Discussion VIP-6 Venice on Kubernetes Under Discussion"},{"location":"contributing/proposals/vip-1/","title":"VIP-1: Authentication Service API","text":"<ul> <li>Status: Accepted</li> <li>Author(s): Enrico Olivelli</li> <li>Pull Request: PR 471</li> <li>Release: N/A</li> </ul>"},{"location":"contributing/proposals/vip-1/#introduction","title":"Introduction","text":"<p>Currently, Venice doesn't provide a well-defined extensible way to authenticate clients, but it only supports TLS based authentication, and it is hard coded. This VIP proposes a new API to write plugins to authenticate clients. The first follow-up work will be to implement a plugin to support JWT token based authentication.</p>"},{"location":"contributing/proposals/vip-1/#problem-statement","title":"Problem Statement","text":"<p>Venice's services are based on HTTP/REST APIs, and we need a way to authenticate clients using standard mechanisms like JWT tokens and OAuth2. It means that all the services must perform authentication (and authorization) checks on each request.</p> <p>Therefore, we need a way to write plugins to perform authentication checks, this way it will be easy to  add more and more mechanisms in the future.</p> <p>Authentication mechanisms vary a lot from each other, and they are not only based on single steps; we need to support at least only mechanisms that work well on HTTP and that can be performed in a single step,  like basic HTTP Authentication, that needs only to use an HTTP Header or TLS based Authentication, that is based on the client certificate exchanged during the TLS handshake.</p>"},{"location":"contributing/proposals/vip-1/#scope","title":"Scope","text":"<ol> <li> <p>What is in scope?</p> </li> <li> <p>Define an API to write plugins to perform authentication checks on Venice components (controller, server and router).</p> </li> <li>The API must support single step authentication mechanisms (JWT, OAuth2, TLS client authentication).</li> <li>Support retrofitting the existing TLS mechanism as a plugin (without introducing the implementation).</li> <li>Ensure that the Authentication API is used by all the services and applied to every request.</li> <li> <p>Ensure that the AuthorizerService is able to use the Authentication API to perform authorization checks.</p> </li> <li> <p>What is out of scope?</p> <ul> <li>Remove legacy DynamicAccessController</li> <li>Implement a JWT plugin to support JWT token based authentication (this will be a follow-up work) </li> <li>Implement the TLS client certificate plugin</li> <li>Refactor the AuthorizerService APIs (even if some changes will be needed) </li> <li>Implement other authentication mechanisms (Kerberos, SAML, etc.)</li> <li>Modify the Admin Tools to support authentication (this will be a follow-up work)</li> <li>Deal with authentication against the PubSub broker (Kafka, Pulsar, etc.)</li> </ul> </li> </ol>"},{"location":"contributing/proposals/vip-1/#project-justification","title":"Project Justification","text":"<p>This VIP is required in order to allow Venice user to use standard authentication mechanisms like JWT and OAuth2/OpenID Connect. </p> <p>For instance JWT, OAuth2 and OpenId connect are widely used by Apache Pulsar users, and introducing these features will help the adoption of Venice in the Pulsar community.</p>"},{"location":"contributing/proposals/vip-1/#functional-specification","title":"Functional specification","text":"<p>The core of the VIP is the AuthenticationService API, that is used by all the services to perform authentication checks.</p> <p>The AuthenticationService API is a Java interface that mandates the contract for the authentication plugins.</p> <p>The goal of the AuthenticationService is to map an HTTP request to a user identity, a Principal, that can be used by the AuthorizerService.</p> <p>As the AuthenticationService must work on all the VeniceComponents it won't have any hard dependency on the HTTP layer, as in Venice we are using multiple technologies, depending on the Component.</p> <pre><code>public interface AuthenticationService extends Closeable {\n\n   /**\n    * Maps an HTTP request to a Principal.\n    * Any unchecked exception thrown by this method will be logged and the request will be rejected.\n    * @param requestAccessor access the HTTP Request fields\n    * @return the Principal or null if the request is not authenticated\n    */\n   default Principal getPrincipalFromHttpRequest(HttpRequestAccessor requestAccessor);\n\n   /**\n    * Generic Wrapper over an HTTP Request.\n    */\n   interface HttpRequestAccessor {\n       String getHeader(String headerName);\n      X509Certificate getCertificate();\n   }\n\n   /**\n    * Lifecycle method, called when the Venice component is initialized.\n    * @param veniceProperties the configuration of the component being initialized\n    * @throws Exception\n    */\n   default void initialise(VeniceProperties veniceProperties) throws Exception;\n\n   /**\n    * Lifecycle method, called when the Venice component is closed.\n    */\n   default void close();\n\n}\n</code></pre> <p>You configure the classname of the AuthenticationService with an entry <code>authentication.service.class</code>. The AuthenticationService is initialised by passing the VeniceProperties read from the configuration file.  </p>"},{"location":"contributing/proposals/vip-1/#proposed-design","title":"Proposed Design","text":"<p>Most the work is about introducing the new API and refactoring the existing code to use it. The legacy Authentication mechanism will initially be left untouched, but when you configure the new AuthenticationService, the legacy mechanism will be disabled.</p> <p>AuthenticationService and AuthorizerService will kick-in in spite of the legacy DynamicAccessController. And then the AuthenticationService the ACL checks will be performed by the AuthorizerService, and not by the DynamicAccessController.</p> <p>In Venice clusters in which you configure AuthenticationService and AuthorizerService it is not expected that all the current admin tools work, especially the ones that are based on the legacy DynamicAccessController (ACLs...). It is out of the scope of this VIP to modify the admin tools.</p> <p>Multi-region support is not taken into account because we are only introducing a new API about Authentication, it depends on every specific mechanism how to implement intra and inter region authentication.</p> <p>New Authentication mechanism may have an impact on performance depending on the technology user and the implementation. It is out of the scope of this VIP to enter the details of the performance impact. It is possible that in the future in order to support some authentication mechanisms we will need to introduce an asynchronous API to perform authentication checks.</p>"},{"location":"contributing/proposals/vip-1/#development-milestones","title":"Development Milestones","text":"<p>The implementation for this VIP introduces: - the Java API - the Controller implementation (loading the plugin and calling the API) - the Router implementation (loading the plugin and calling the API) - the Server implementation (loading the plugin and calling the API) - some dummy plugins to test the API</p>"},{"location":"contributing/proposals/vip-1/#test-plan","title":"Test Plan","text":"<p>The implementation will be tested with unit tests and integration tests, main topics: - AuthenticationService plugin lifecycle (boostrap, initialization, close) - Verifying that the plugin is invoked by the Controller, Router and Server</p>"},{"location":"contributing/proposals/vip-1/#references","title":"References","text":"<ul> <li>AuthenticationProvider API in Apache Pulsar</li> <li>Authentication Service Docs in Apache Pulsar</li> <li>OAuth2</li> <li>JWT</li> </ul>"},{"location":"contributing/proposals/vip-2/","title":"VIP-2: Removing Per Record Offset Metadata From Venice-Server Storage With Heartbeats","text":"<ul> <li>Status: Under Discussion</li> <li>Author(s): Zac Policzer</li> <li>Pull Request: PR 513</li> <li>Release: N/A</li> </ul> <p>Not yet merged. Discussed in PR 513.</p>"},{"location":"contributing/proposals/vip-3/","title":"VIP-3: Rust Server Read Path","text":"<ul> <li>Status: Under Discussion</li> <li>Author(s): Felix GV</li> <li>Pull Request: PR 604</li> <li>Release: N/A</li> </ul>"},{"location":"contributing/proposals/vip-3/#introduction","title":"Introduction","text":"<p>As Venice users need ever-increasing performance, in terms of throughput, average latency and tail latency, it is necessary to revisit past design choices and implementation details to consider what would it take to reach the next level. This proposal is submitted in that spirit.</p> <p>The Fast Client is the most significant change to the read path since the inception of Venice, and it is progressing well. It helps latency by eliminating the router hop and most importantly by eliminating an entire JVM from the hot  path. After this, the only JVM left in the backend is that of the server, which this proposal aims to eliminate as well.</p> <p>The goal is to replace this Java code with another lower-level language which would be free of garbage collection, thus  making tail latency much more predictable. In addition, this would likely open up the door to increased throughput  capacity as well.</p> <p>The specific language proposed to replace Java is Rust, which has become mature and well  accepted in the industry over the past years. It promises performance similar to C++, without some of its pitfalls,  thanks to its borrow checker.</p>"},{"location":"contributing/proposals/vip-3/#problem-statement","title":"Problem Statement","text":"<p>The Venice read path should be completely free of garbage collection. Venice should be able to serve single get read requests, end-to-end (i.e. as measured by the client), at a p99.99 latency under 1 millisecond.</p>"},{"location":"contributing/proposals/vip-3/#scope","title":"Scope","text":"<p>This project can be broken up in several milestones, but we will focus on the most minimal scope which can be shipped independently of the rest and serve as the MVP version of this initiative.</p> <p>At a high level, the MVP is to support batch gets, compatible with the Fast Client.</p> <p>In more details, achieving the MVP requires:</p> <ul> <li>Bringing Rust/Cargo inside the Venice build.</li> <li>Selecting all required crates (probably RocksDB, gRPC and JNI at a minimum).</li> <li>Building and publishing a jar which includes the DLL of the Rust server, compiled for Mac and Linux.</li> <li>Writing the Rust code which can listen for gRPC requests, query RocksDB and return the response.</li> </ul> <p>Out of scope of the MVP is:</p> <ul> <li>Read compute support.</li> <li>Thin client support.</li> <li>Auxiliary endpoints (e.g. metadata, schema, dictionary, etc).</li> <li>Everything else that is not part of the read path.</li> </ul>"},{"location":"contributing/proposals/vip-3/#project-justification","title":"Project Justification","text":"<p>The demand for AI use cases powered by Venice is growing on a steep trajectory along all dimensions, including the number of use cases, the queries per second, the keys per query, the number of records and the size of records. While Venice is holding up for now and we can keep growing clusters and building new ones in the short-term, it is important  to also take a longer-term view and to kick off initiatives that will improve the foundation of the platform, even if  those initiatives will only land in production in the next 6-12 months. We need Venice to not only continue to further  scale horizontally, but to better scale vertically as well. As the footprint of the Venice backend continues to grow,  the efficiency of each node in the system becomes more important to optimize.</p>"},{"location":"contributing/proposals/vip-3/#functional-specification","title":"Functional specification","text":"<p>This is operator-facing work, so by design it should be as transparent as possible to the user. For the operator, the  new APIs and configs include:</p> <ul> <li>New server config:</li> <li>To enable the Rust server</li> <li>To specify the port which the Rust server will listen on</li> <li>Other configs of the Rust server implementation (e.g. worker thread count, etc; TBD)</li> <li>Other config or store setting (TBD):</li> <li>To determine if cluster discovery will return the Rust server port or the regular port for a given store/cluster     (or possibly a mix, where only some servers enable the Rust implementation).</li> </ul> <p>From the user's perspective, as long as they are using the Fast Client with the gRPC mode enabled, they should be eligible to benefiting from the Rust server, if it happens to be enabled by the operator.</p>"},{"location":"contributing/proposals/vip-3/#proposed-design","title":"Proposed Design","text":"<p>In a nutshell, the idea is to create a Rust program which wraps RocksDB, and presents to our server JVM the same JNI APIs which RocksJava has today (or at least the subset we use). In this way, the server continues to use RocksDB as normal, for both the ingestion path and the legacy read path. However, this Rust program would also open up a port and listen for gRPC requests, which it would handle and respond to. The diagram below shows the current and proposed architectures side-by-side.</p> <p></p>"},{"location":"contributing/proposals/vip-3/#why-rust","title":"Why Rust?","text":"<p>In terms of a GC-free language, the main contender would be C++. We can certainly consider that option as well, but it seems that nowadays Rust is broadly accepted as a solid alternative, and the momentum behind it is growing. Furthermore,  Rust's performance already rivals that of C++ in many workloads, and where there are differences one way or the other  they seem to be quite minor. The benefits of Rust's borrow checker are expected to reduce risk in terms of executing  this project and operating the end result safely.</p> <p>In order to write idiomatic, higher quality, Rust code, the project will adopt rustfmt  and rust-clippy.</p>"},{"location":"contributing/proposals/vip-3/#why-grpc","title":"Why gRPC?","text":"<p>Because it seems easier than to re-implement Venice's bespoke REST spec. Also, although gRPC is probably not perfect, it is likely more efficient than the current REST protocol, and will make it easier to support clients in other languages. Also, we already have gRPC embedded in the stack following the Summer 2023 internship project, so it might as well be leveraged.</p> <p>There is more than one crate option for integrating with gRPC, but the most mature one seems to be Tonic.</p>"},{"location":"contributing/proposals/vip-3/#why-jni","title":"Why JNI?","text":"<p>Because it is supported in Rust, and that should make the integration easier. An alternative would be to have the Java  server communicate with the Rust server via some other mechanism (loopback socket, pipes, etc) but then that would mean adding lots of forks in the code, and might also be less efficient. Using a non-JNI mechanism would also mean that just enabling the Rust server (even if it's not used yet) would already alter the behavior of the main code, thus creating more risk. In order to work in an incremental manner, where the Rust code can be developed in small chunks and hidden behind a config flag until ready, it is preferable to keep this interface the same.</p> <p>In the end state, JNI would only be used by the Java code responsible for the ingestion, and the read path would be fully JNI-free. Also, it should be noted that the Rust server will not be a separate child process spawned by the Java process, but rather fully embedded in the Java process. The Rust server will be activated by a JNI call initiated by the Java server, which spins up the gRPC Handler on the Rust side, along with any port and threads it needs to function.</p> <p>Doing JNI properly requires packaging a jar with code compiled for multiple architectures (at least Mac and Linux for the time being). There is some complexity there, but it can be done.</p> <p>Although there are many tutorials on how use JNI with Rust, we can consider using a higher-level framework to facilitate the work, such as duchess.</p>"},{"location":"contributing/proposals/vip-3/#quota","title":"Quota","text":"<p>The read quota will need to be re-implemented in Rust. There are essentially three approaches we can consider:</p> <ol> <li>Let the Rust server and Java server share a quota amount between them, so that no matter if the traffic comes in via    the Java port or the Rust port, it will be accounted for and the configured limit will be respected. This requires    having some kind of synchronization between the two sides (either at the granularity of each request, which is likely    too expensive, or by reserving some allotment periodically, which is complicated).</li> <li>Let the Rust server and Java server each have the full quota amount and not need any interaction between each other    for sharing it. This is much simpler to implement, but the downside is that if clients were to perfectly balance    their traffic across the two ports, they could get up to twice their configured quota.</li> <li>If the Rust server is enabled, then completely disable the ability of the Java side to answer requests. This would    uphold the quota limit strongly, without the complexity of quota sharing described in approach 1. The downside is    that if the Rust server supports only a subset of operations (e.g. lack of Read Compute support), then unsupported    operations would become completely unavailable on the cluster(s) where the Rust read path is enabled.</li> </ol> <p>Since the migration is expected to be controlled by the operator, and should only be activated for users of the Fast Client (i.e. not for users that run a mix of Thin Clients and Fast Clients), the edge case solved by the more complicated solution 1 does not seem likely to come up. Therefore, the proposal is to pick either solution 2 or 3.</p> <p>Also, it can be considered whether quota should be part of the MVP at all. If the Rust server mode will be tried in  dedicated clusters at first, then quota may not even be a must-have (though of course it will be needed eventually).</p>"},{"location":"contributing/proposals/vip-3/#cluster-discovery","title":"Cluster Discovery","text":"<p>The general idea is that when the Rust mode is activated in a server, that server would announce that it has a gRPC  endpoint at the port which the Rust server listens to. The announcement can be done by the Java server, to minimize complexity on the Rust side. Beyond that, there are many variants on how to make things work end-to-end.</p> <p>A few alternatives:</p> <ol> <li>The Java and Rust servers are all announced to the same resource name. This means there is some gRPC resource which    a Fast Client can hit, and that could be served either by Java or Rust, and possibly both. Changing which path the    clients go through would require turning on or off the announcement on the Java and/or Rust side, which could be    done either via reconfiguring and bouncing the server, or via some admin tooling.</li> <li>The Java and Rust servers are announced to different resource names. When cluster discovery happens, the client asks    where to connect, the backend could decide to return one or the other of these resources, thus directing the traffic    fully (at least for that one client instance) into one path or the other. Changing which path the clients go through    would require updating the mapping returned by cluster discovery, which would incur the same delay as cluster     migration.</li> <li>Instead of announcing per-cluster resources, we could announce per-store resources, and then have some admin tooling    to enable and disable the announcement for Rust and Java servers on a per-store basis. This could give the most    flexibility, since the client would not remain stuck with whatever resource name was returned by the cluster     discovery call done at startup time (and periodically thereafter), but rather could adapt rapidly.</li> </ol> <p>These options provide different levels of granularity of control and speed of change, with finer granularity and faster change carrying greater complexity cost.</p>"},{"location":"contributing/proposals/vip-3/#authauth","title":"Auth/Auth","text":"<p>The Rust server will open a port and receive connections from clients, so it will need to authenticate and authorize them. Currently, the auth/auth logic is all in Java, so there needs to be a solution to doing it on the Rust side. There are two main alternatives:</p> <ol> <li>Design an abstraction which can be implemented by a custom Rust module, so that the Rust server can interrogate an    API and find out if a given principal/token/etc is allowed to access a given resource.</li> <li>Implement a mechanism by which the Rust server can call into the Java server, ask whether a given principal/token/etc    is allowed to access a given resource, let the Java server answer that with the current abstraction that exists over    there, return that response to Rust, and finally let Rust cache this decision so that known principals need not do    this again. In this scheme, there needs to also be a mechanism to periodically refresh the authorization decision    (e.g. once a minute), in case it may have changed.</li> </ol> <p>Given that Venice already requires the operator to provide a Java implementation of the authorization implementation, it is probably easier to leverage this one and only implementation in Rust as well (via approach 2) rather than require the operator to implement their authorization in two places. Furthermore, approach 2 has the potential to be more efficient, since the authorization logic need not run within the hot path of every single request. For these reasons, approach 2 is preferred.</p>"},{"location":"contributing/proposals/vip-3/#metrics","title":"Metrics","text":"<p>Similarly to auth/auth, there is already an abstraction for taking metrics out of Venice on the Java side, so for the metrics that will be gathered on the Rust side, two analogous approaches exist: build a Rust abstraction that can be implemented so that the Rust server gains the ability to emit metrics directly, or build a mechanism by which metrics held in Rust's memory can be copied over to the Java side, and emitted alongside the other Java side metrics. The latter approach is analogous to what is being done in Ingestion Isolation, where the child process periodically sends all its  metrics to the parent process. For the same reason as in auth/auth that it would be simpler for the operator, the second  approach is preferred here as well.</p>"},{"location":"contributing/proposals/vip-3/#development-milestones","title":"Development Milestones","text":"<p>The milestones for the MVP would be the following:</p> <ol> <li>Build a simple RocksDB wrapper which gets packaged into a jar and exposes the same JNI APIs as RocksJava. This can be    built with a single architecture at first. Print \"hello world\" when JNI is called successfully.</li> <li>Swap RocksJava for the wrapper inside Venice. We can do a micro-benchmark at this stage to validate the assumption     that JNI via Rust is no worse than JNI via C++.</li> <li>Integrate the chosen crate for adding gRPC support, and do a \"hello world\" of the RPC.</li> <li>Add server configs to enable the Rust gRPC module (including basic service disco announcement).</li> <li>Implement the batch get support.</li> <li>Implement support for auth/auth.</li> <li>Implement support for metrics.</li> <li>Multi-architecture (Linux + Mac) support.</li> <li>(Possibly optional) implement quota.</li> <li>(Possibly optional) more advanced/granular service disco control.</li> </ol> <p>Beyond the MVP, the following scope expansions are natural followups:</p> <ol> <li>Quota (if excluded from the MVP).</li> <li>Bidirectional streaming for batch get (if not implemented in the MVP).</li> <li>Read compute support.</li> <li>Support auxiliary endpoints (metadata, schema, etc).</li> </ol> <p>Other ideas for leveraging Rust, which are less likely to happen, but could eventually be considered:</p> <ol> <li>Rust Fast Client which Java (and possibly other languages) could call via JNI (or similar), so that as much of the    end-to-end transmission of data can be GC-free, with only the very last mile being done in the user app's language.</li> <li>Rust Router. This is an alternative to the idea 1 above, in case we would want to keep the Thin Client as a part of     the architecture in the long-term, perhaps as a way to implement support for clients in other languages more easily.</li> <li>Rust ingestion. This would be quite complicated, since the Venice ingestion is very complex and sophisticated. It    could possibly be broken up into increments, e.g. we might start off with Write Compute being done in Rust, and     exposed as a single JNI call to Java (as opposed to the multiple calls it currently needs).</li> <li>Rust controller. This is no doubt the least attractive milestone, since the controller is fairly complex, and not    performance-sensitive. Almost certainly wouldn't happen.</li> </ol>"},{"location":"contributing/proposals/vip-3/#test-plan","title":"Test Plan","text":"<p>We of course need Rust unit tests for the Rust code.</p> <p>All Fast Client integration tests written in Java should get an extra permutation for enabling the Rust server.</p> <p>Besides that, there will need to be extensive pre-prod certification to minimize the risk that enabling the Rust server  could destabilize the Venice server.</p>"},{"location":"contributing/proposals/vip-3/#references","title":"References","text":"<p>Material for learning Rust:</p> <ul> <li>The Book</li> <li>Rust Atomics and Locks</li> <li>The Rustonomicon (probably unnecessary for this project, but just in case...)</li> </ul>"},{"location":"contributing/proposals/vip-4/","title":"VIP-4: Store Lifecycle Hooks","text":"<ul> <li>Status: Accepted</li> <li>Author(s): Felix GV</li> <li>Pull Request: PR 881</li> <li>Release: N/A</li> </ul>"},{"location":"contributing/proposals/vip-4/#introduction","title":"Introduction","text":"<p>The Venice Push Job takes data from a grid and pushes it to a store in all  regions. This works fine in many cases, but there are some use cases where we would like to have greater control over  the steps of the process. This proposal is to add new configs and hooks which can be used both to monitor and control  the push job in a finer-grained manner than is possible today. In particular, this proposal focuses on the way that each  individual region is handled.</p> <p>Currently, the sequence of steps happening within a push job is as follows:</p> <p></p> <p>A few notes on the above diagram:</p> <ul> <li> <p>A new schema will be registered only if the data being pushed does not conform to any already known schema, and    the schema auto-registration config is enabled. If that config is disabled, then an unknown schema leads to job    failure.</p> </li> <li> <p>The compression job is optional, and whether it runs or not depends on configurations.</p> </li> <li> <p>The data push job writes to a store-version pub sub topic in one of the regions (typically the one which is local to   where the push job is running), but all regions start ingesting right away, as soon as the data push job begins    writing. For regions that are remote from the topic the data push job is writing to, the leader servers are performing    replication, while in the region which contains that topic, the replication is a no-op.</p> </li> <li> <p>The <code>SERVING</code> step is also called \"version swap\". If there are Da Vinci Clients, they will ingest and swap on their   own, and the child controller of that region will wait until all DVC instances have swapped (started serving) before   enacting the region-level swap, after which the servers will also start serving the new store-version to clients which   perform remote queries. That is why the \"DVC Read Traffic SERVING\" step is a dependency for the \"Server Read Traffic   SERVING\" step.</p> </li> </ul>"},{"location":"contributing/proposals/vip-4/#problem-statement","title":"Problem Statement","text":"<p>Pushing to all regions in parallel makes the push faster, but it also means that if the push causes an issue, the impact is going to be global (affecting all regions). It would be desirable to have certain checks and balances that reduce the blast radius in cases where the content of the push causes issues. Examples of such issues include:</p> <ul> <li>Data quality issues:</li> <li>Incomplete data (due to issues in data generation logic, or in upstream datasets).</li> <li>Change in semantics (e.g. some embeddings trained by a new ML model / weights / params / etc. are incompatible with      that used at inference-time).</li> <li>Schema-related issues:</li> <li>Some optional fields which used to always be populated get deleted (or get populated with null) and the reading app      fails due to lack of null checking. This kind of app-side bug can happen even though a schema evolution is fully      compatible.</li> <li>Infra issues:</li> <li>Larger payloads take more resources, resulting in lack of capacity and thus latency degradation.</li> <li>Certain types of yet-unknown infra bugs are somehow triggered by a data push.</li> </ul> <p>Tighter control of how the new version of the dataset is deployed to each region could allow us to catch issues while only one region is affected, and abort deploying to other regions. See Scope, below, for specific examples of flow  control strategies.</p> <p>In addition, we would like to make it easier to integrate the push job into proprietary monitoring systems such that  each region getting data deployed to it results in events getting emitted or other observability actions.</p>"},{"location":"contributing/proposals/vip-4/#scope","title":"Scope","text":"<p>This proposal is about full push jobs. Incremental pushes and nearline writes are out of scope. At the time of  submitting this proposal, it is undetermined whether this work will apply to stream reprocessing jobs. In terms of  priority, we care mostly about supporting full pushes from offline grids, and it may be fine to leave stream  reprocessing out of scope, although depending on the design details we choose, we may be able to support stream  reprocessing \"for free\" as well (i.e. if the hooks are executed in the controller). Incremental Push is out of scope of this proposal.</p> <p>The goal is for lifecycle hooks to achieve the following use cases:</p> <ul> <li>Orchestrate how data is served in each region, including:</li> <li>Ensuring a minimum delay (e.g. 1 hour) between each region beginning to serve the new store-version.</li> <li>Delaying the swapping of a new store-version to be within some time of day (e.g. during \"business hours\").</li> <li>Performing custom health checks on the client applications to ensure that their key metrics are still healthy within     a region where a new store-version was swapped, before proceeding to more regions. Based on the outcome of this      check:<ul> <li>Having the ability to abort the swapping of a store-version to further regions.</li> <li>Having the ability to rollback to the previous store-version in regions that already swapped.</li> </ul> </li> <li>Trigger informational events in proprietary monitoring systems after important lifecycle milestones are completed.</li> </ul> <p>The above use cases all are operator-centric, and so (at least for now) there is no concern of making it very ergonomic for Venice users to register new hooks or evolve old hooks dynamically. The general expectation is that there would be a small number of hooks maintained by the Venice operators and that it's ok for hooks to be bundled and upgraded  alongside the Venice components. In cases where Venice users need to customize hook behaviors, that could be achieved via store-level configs passed into hooks, and there is no need to provide the flexibility of letting users register whole new hook implementations.</p>"},{"location":"contributing/proposals/vip-4/#project-justification","title":"Project Justification","text":"<p>The cost of having global impact in the case of issues mentioned above is too high, and we would like to provide  first-class options to reduce the blast radius. Building this within Venice itself will make it easier to automate these methodologies, thus reducing toil for users.</p>"},{"location":"contributing/proposals/vip-4/#functional-specification","title":"Functional specification","text":"<p>The proposed API for this functionality is described in code here:</p> <ul> <li>StoreLifecycleHooks, which is the main part of this    proposal.</li> <li>StoreLifecycleEventOutcome, which is the signal   returned by some hooks to indicate that a given step should proceed or abort.</li> <li>StoreVersionLifecycleEventOutcome, which   is the signal returned by some other hooks which need more fine-grained control over the workflow. In addition to    proceeding and aborting, this also provides the option to wait, which tells the hooks framework to try invoking the    hook again later, and rollback, which tells the framework to rollback to the previous store-version in all regions.</li> <li>JobStatusQueryResponse, which is the payload   returned by the <code>/job</code> controller endpoint, is extended to include status update timestamps. This will be populated by   the child controller to indicate the time when its own individual status last changed, and the parent controller will   aggregate these into a map keyed by region. All hooks which return the <code>StoreVersionLifecycleEventOutcome</code> will have   access to this payload in their input, so that they can make decisions based on the status of each region. The time   when the status was last updated for a given region is useful in order to achieve the use case of a hook which injects   a delay between each region swap. The code change to support these extra timestamps is included in this VIP, to    demonstrate feasibility and present the proposed algorithm (see <code>OfflinePushStatus::getStatusUpdateTimestamp</code>).</li> </ul>"},{"location":"contributing/proposals/vip-4/#proposed-design","title":"Proposed Design","text":"<p>The main design consideration is where to execute the hooks. At a high level, there are three options:</p> <ol> <li>Within the push job.</li> <li>Within the parent controller (chosen option).</li> <li>Within the child controllers.</li> </ol> <p>It is also possible to consider invoking some hooks in one of these location while other hooks would be executed  elsewhere. The table below summarizes the feasibility and tradeoffs for each of the proposed hooks:</p> Hook function name Actions CC PC VPJ <code>validateHookConfig</code> \u27a1\ufe0f \u2620\ufe0f \u274c \u2705 \u274c <code>preStartOfPushJob</code> \u27a1\ufe0f \u2620\ufe0f \u274c \u2705 \u2705 <code>postStartOfPushJob</code> None \u274c \u2705 \u2705 <code>preSchemaRegistration</code> \u27a1\ufe0f \u2620\ufe0f \u274c \u2705 1\ufe0f\u20e3 <code>postSchemaRegistration</code> None \u2705 \u2705 1\ufe0f\u20e3 <code>preStoreVersionCreation</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f \u2705 \u2705 \u2705 <code>postStoreVersionCreation</code> None \u2705 \u2705 \u2705 <code>preStartOfStoreVersionIngestionForDaVinci</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f 2\ufe0f\u20e3 2\ufe0f\u20e3 2\ufe0f\u20e3 <code>postStartOfStoreVersionIngestionForDaVinci</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f 2\ufe0f\u20e3 2\ufe0f\u20e3 2\ufe0f\u20e3 <code>postStoreVersionLeaderReplication</code> None \u2705 \u274c \u274c <code>preStoreVersionSwap</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f \u2705 \u2705 \u2705 <code>postStoreVersionSwap</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f \u2705 \u2705 \u2705 <code>preEndOfPushJob</code> \u27a1\ufe0f \u2620\ufe0f \u270b \u21a9\ufe0f \u2705 \u2705 \u2705 <code>postEndOfPushJob</code> None \u2705 \u2705 \u2705 <p>Legend:</p> <ul> <li>The Actions column represent which control mechanism is available to each hook:</li> <li>\u27a1\ufe0f Proceed: Move forward with this step.</li> <li>\u2620\ufe0f Abort: Cancel this step (and as a consequence, short-circuit any future step that would come after this one).</li> <li>\u270b Wait: Let the hooks framework re-run this step later (i.e. 1 minute later, by default).</li> <li> <p>\u21a9\ufe0f Rollback: Let the store go back to the store-version it had prior to beginning the push job (in all regions). </p> </li> <li> <p>The last three columns are the feasibility of implementing this hook in a given component:</p> </li> <li>CC Child Controller.</li> <li>PC Parent Controller.</li> <li>VPJ Venice Push Job.</li> <li>\u2705 It is feasible to implement this hook within this component (without unreasonable complexity).</li> <li>\u274c It is NOT feasible to implement this hook within this component (without unreasonable complexity).</li> <li>1\ufe0f\u20e3 Schema registration hooks in push jobs could only be invoked in cases where auto-registration is enabled and    the new schema originates from the push job itself, whereas schema registrations which are performed directly on the    controller could not trigger the hook.</li> <li>2\ufe0f\u20e3 The hook for the start of ingestion for Da Vinci Clients is tricky for a few reasons. The start of ingestion is    controlled by updating the Meta Store, which is a non-replicated system store updated by child controllers, so those   must be involved (either by running the hook there in the first place, or by having some mechanism that enables the   parent or VPJ to inform the child controllers of when the system store is eligible for getting updated, such as by   adding a new field to the <code>AddVersion</code> admin channel command). However, see Rollback Support below for why running    hooks in the child controller may be insufficient.</li> </ul>"},{"location":"contributing/proposals/vip-4/#rollback-support","title":"Rollback Support","text":"<p>In order to support the ability for the hooks which return the <code>StoreVersionLifecycleEventOutcome</code> to rollback, the most natural way to achieve this is likely to involve the parent controller, either by having those hooks run there in the  first place, or by having a propagation mechanism to it:</p> <ul> <li> <p>If the hooks are executed in the child controller, the propagation mechanism might be to extend the job status check    which the parent controller does periodically in order for the child to inform the parent of the need to rollback, or    else build a new mechanism for the child to directly interact with the parent (or even with other child controllers    directly...).</p> </li> <li> <p>If the hooks are executed in VPJ, then it would need to interact with the parent via the controller client to trigger    the rollback.</p> </li> </ul>"},{"location":"contributing/proposals/vip-4/#configs","title":"Configs","text":"<p>There needs to be new configs:</p> <ul> <li> <p><code>venice.store.lifecycle.hooks</code>: Comma-separated list of FQCN of the hook implementations to load.</p> </li> <li> <p><code>venice.store.lifecycle.hooks.threads</code>: Number of threads used by the hooks framework to execute all hooks.</p> </li> <li> <p><code>venice.store.lifecycle.hooks.timeout.seconds</code>: Max duration allowed for a hook before the hooks framework interrupts   it.</p> </li> <li> <p><code>venice.store.lifecycle.hooks.wait.interval.seconds</code>: The time to wait before re-invoking a hook which returned <code>WAIT</code>    (default: 60 seconds).</p> </li> <li> <p><code>venice.store.lifecycle.hooks.configs.&lt;arbitrary&gt;</code>: Any number of configs to be passed (after clipping everything   before the <code>&lt;arbitrary&gt;</code> part) into the hooks constructor.</p> </li> </ul> <p>In addition, the store config will get a new <code>Map&lt;String, String&gt;</code> of store-level config overrides. Those configs are \"stringly-typed\", rather than strongly-typed, since we are not aware of the  configs needed by each hook at compile-time, and we therefore cannot shape the definition of the store config schema  accordingly. This issue is mitigated via the <code>validateHookConfig</code>, which can be used to prevent invalid configs from entering the system.</p>"},{"location":"contributing/proposals/vip-4/#metrics","title":"Metrics","text":"<p>There needs to be new metrics to monitor hook health. Each new metric will be per function and per registered hooks  class (i.e. 13 functions per class if we implement all of them, or less if we cut the scope). For each class/function hook, there will be:</p> <ul> <li>the occurrence rate of:</li> <li>hook invocations</li> <li>each hook return signal (for the functions that return something other than <code>void</code>)</li> <li>failures (exceptions)</li> <li>timeouts</li> <li>the time spend waiting in queue before being executed (which will be useful to determine if the thread pool count is    under-provisioned)</li> </ul> <p>N.B.: Implementing metrics from within VPJ is a bit more complicated, whereas controllers already have the ability to emit metrics.</p>"},{"location":"contributing/proposals/vip-4/#design-recommendation","title":"Design Recommendation","text":"<p>Running hooks in the parent controller is probably most straightforward. The only issue is the inability to support the <code>postStoreVersionLeaderReplication</code> hook, but that one is not critical and could be left out of scope.</p> <p>Concretely, choosing the parent controller path would work like this:</p> <ol> <li> <p>The parent controller would invoke the hooks for <code>preStartOfStoreVersionIngestionForDaVinci</code> and for    <code>preStoreVersionCreation</code> for each of the regions. If all hooks respond with <code>PROCEED</code> then it's essentially a normal     push, otherwise it would configure the <code>AddVersion</code> command sent to the admin channel with the appropriate inclusions    in <code>targetedRegions</code> (depending on the result of <code>preStoreVersionCreation</code>) and in a new field for controlling the    DVC ingestion (depending on the result of <code>preStartOfStoreVersionIngestionForDaVinci</code>) which the child controller     would honor by holding off on writing to the Meta Store.</p> </li> <li> <p>The parent controller would create the new store-version with <code>versionSwapDeferred = true</code>.</p> </li> <li> <p>When the parent controller sees that a region has completed ingestion, it would invoke the <code>preStoreVersionSwap</code>    hook for that region, and if the hook responds with <code>PROCEED</code>, then it would send an admin command targeted for that    region to swap the current version.</p> </li> <li> <p>If at any stage the hooks respond with <code>ROLLBACK</code> then the parent controller would send more admin channel commands    to do the swap in the reverse direction.</p> </li> </ol> <p>This recommendation has been accepted, after design reviews, hence we will implement hooks in the parent controller.</p>"},{"location":"contributing/proposals/vip-4/#development-milestones","title":"Development Milestones","text":"<p>At a high-level:</p> <ol> <li>Evolve the admin channel protocol.</li> <li>Implement the child controller changes for dealing with the admin channel changes.</li> <li>Implement the parent controller changes to support the hooks framework and the orchestration described above.</li> </ol> <p>More fine-grained plan TBD after finalizing the design.</p>"},{"location":"contributing/proposals/vip-4/#test-plan","title":"Test Plan","text":"<p>The first hooks will be built such that they are no-op by default, and require a hook config to enable them. That hook config will be left disabled in the global config, and will be tested at small scale via the store-level overrides.</p> <p>After store-level testing and stabilization is satisfactory, we will begin enabling them globally.</p>"},{"location":"contributing/proposals/vip-4/#references","title":"References","text":"<p>N/A.</p>"},{"location":"contributing/proposals/vip-5/","title":"VIP-5: Facet Counting","text":"<ul> <li>Status: Accepted</li> <li>Author(s): Felix GV</li> <li>Pull Request: PR 1612</li> <li>Release: N/A</li> </ul>"},{"location":"contributing/proposals/vip-5/#introduction","title":"Introduction","text":"<p>Facet Counting is a type of aggregation query popular in the search domain. It provides information about the  cardinality of values for the fields of documents of interest.</p> <p>Given that search is one of the types of derived data workloads in which Venice already participates (i.e., the Da Vinci Client can serve as a search solution's forward index component), there is demand from Venice users to increase the  scope of capabilities such that Facet Counting can also be performed natively by Venice.</p> <p>This document introduces the domain, and suggests a roadmap for implementing this capability in various phases,  including on the client-side and server-side.</p>"},{"location":"contributing/proposals/vip-5/#problem-statement","title":"Problem Statement","text":"<p>This section dives deeper in defining what Facet Counting is, and how to integrate it into Venice.</p> <p>Before diving into specifics, it may be useful to provide an analogy in order to make a general observation: the  proposal in this VIP has similarities and differences compared to Read Compute. Read Compute, as it exists today, is a record-wise (or we could say: row-wise) operation, meaning that for a given input record there is exactly one output record. This type of workload is very natural to push down into the server-side, in such way that the work is split  across many servers and the client can retrieve the various output records individually. The use cases presented here also have a portion of work which is executable on a per-record basis, and therefore has the potential of being pushed down to the server-side, however, given that they are \"aggregation queries\", there is also a final processing step which must be performed in some central location (e.g., in the client). The work can therefore only be partially pushed down.</p>"},{"location":"contributing/proposals/vip-5/#facet-counting-use-cases","title":"Facet Counting Use Cases","text":"<p>Let's define what Facet Counting is, with a series of examples from simple to more complex, using SQL to explain it  (although SQL is just a convenient way to express query semantics, but is not part of this proposal).</p> <p>These are functioning SQL queries that have been run on a DuckDB database populated by Venice's own Push Job Details  system store. This system store's Avro schema can be seen here: key, value. The SQL schema for the table is also included below, though only a subset of these columns are used in the examples:</p> <pre><code>SELECT column_name, column_type FROM (DESCRIBE current_version);\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              column_name              \u2502 column_type \u2502\n\u2502                varchar                \u2502   varchar   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 storeName                             \u2502 VARCHAR     \u2502\n\u2502 versionNumber                         \u2502 INTEGER     \u2502\n\u2502 clusterName                           \u2502 VARCHAR     \u2502\n\u2502 reportTimestamp                       \u2502 BIGINT      \u2502\n\u2502 pushId                                \u2502 VARCHAR     \u2502\n\u2502 partitionCount                        \u2502 INTEGER     \u2502\n\u2502 valueCompressionStrategy              \u2502 INTEGER     \u2502\n\u2502 chunkingEnabled                       \u2502 BOOLEAN     \u2502\n\u2502 jobDurationInMs                       \u2502 BIGINT      \u2502\n\u2502 totalNumberOfRecords                  \u2502 BIGINT      \u2502\n\u2502 totalKeyBytes                         \u2502 BIGINT      \u2502\n\u2502 totalRawValueBytes                    \u2502 BIGINT      \u2502\n\u2502 totalCompressedValueBytes             \u2502 BIGINT      \u2502\n\u2502 totalGzipCompressedValueBytes         \u2502 BIGINT      \u2502\n\u2502 totalZstdWithDictCompressedValueBytes \u2502 BIGINT      \u2502\n\u2502 pushJobLatestCheckpoint               \u2502 INTEGER     \u2502\n\u2502 failureDetails                        \u2502 VARCHAR     \u2502\n\u2502 sendLivenessHeartbeatFailureDetails   \u2502 VARCHAR     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 18 rows                                   2 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributing/proposals/vip-5/#facet-counting-by-values-of-a-single-column","title":"Facet Counting by Values of a Single Column","text":"<p>The simplest example of Facet Counting would be to count how many times each distinct value appears in a given column, and we would typically want to limit the amount of results returned, as there could be quite a few:</p> <pre><code>SELECT   storeName,\n         COUNT(storeName) AS cnt\nFROM     current_version\nGROUP BY storeName\nORDER BY cnt DESC\nLIMIT    5;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          storeName          \u2502  cnt  \u2502\n\u2502           varchar           \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 HB_VPJtarget_prod-venice-0  \u2502  2539 \u2502\n\u2502 HB_VPJtarget_prod-venice-13 \u2502  2340 \u2502\n\u2502 HB_VPJtarget_prod-venice-8  \u2502  2337 \u2502\n\u2502 HB_VPJtarget_prod-venice-7  \u2502  2335 \u2502\n\u2502 HB_VPJtarget_prod-venice-3  \u2502  2334 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributing/proposals/vip-5/#facet-counting-by-buckets-within-a-column","title":"Facet Counting by Buckets Within a Column","text":"<p>Another example would be to group not by values, but by buckets. An example of this would be to have buckets for  \"last 24h\", \"last week\", \"last 30 days\".</p> <pre><code>SET VARIABLE most_recent_job_time = (SELECT MAX(reportTimestamp) FROM current_version);\nSET VARIABLE ms_per_day = (SELECT 24 * 60 * 60 * 1000);\nSET VARIABLE ms_per_week = (SELECT GETVARIABLE('ms_per_day') * 7);\nSET VARIABLE ms_per_30_days (SELECT CAST(GETVARIABLE('ms_per_day') AS BIGINT) * 30);\n\nSELECT * FROM (\n    SELECT   'last_24h' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_day'))\nUNION\nSELECT * FROM (\n    SELECT   'last_week' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_week'))\nUNION\nSELECT * FROM (\n    SELECT   'last_30_days' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_30_days'))\nORDER BY cnt;\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 value_or_bucket \u2502  cnt  \u2502\n\u2502     varchar     \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 last_24h        \u2502  2150 \u2502\n\u2502 last_week       \u2502 14811 \u2502\n\u2502 last_30_days    \u2502 61991 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributing/proposals/vip-5/#facet-counting-on-multiple-columns","title":"Facet Counting on Multiple Columns","text":"<p>Finally, a common example of Facet Counting would be to perform the same as above, but for multiple columns, all at  once. In real scenarios, there could be hundreds of columns included, but to keep things simple we will do only two  columns counted by values and one column counted by bucket:</p> <pre><code>SELECT * FROM (\n    SELECT   'clusterName' AS col,\n             clusterName AS value_or_bucket, \n             COUNT(clusterName) AS cnt \n    FROM     current_version \n    GROUP BY clusterName \n    ORDER BY cnt \n    DESC     LIMIT 5) \nUNION\nSELECT * FROM (\n    SELECT   'storeName' AS col, \n             storeName AS value_or_bucket, \n             COUNT(storeName) AS cnt \n    FROM     current_version \n    GROUP BY storeName \n    ORDER BY cnt \n    DESC     LIMIT 5)\nUNION\nSELECT * FROM (\n    SELECT   'reportTimestamp' AS col,\n             'last_24h' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_day'))\nUNION\nSELECT * FROM (\n    SELECT   'reportTimestamp' AS col,\n             'last_week' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_week'))\nUNION\nSELECT * FROM (\n    SELECT   'reportTimestamp' AS col,\n             'last_30_days' as value_or_bucket,\n             COUNT(reportTimestamp) as cnt\n    FROM     current_version\n    WHERE    reportTimestamp &gt; GETVARIABLE('most_recent_job_time') - GETVARIABLE('ms_per_30_days'))\nORDER BY col, cnt DESC;\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       col       \u2502       value_or_bucket       \u2502  cnt  \u2502\n\u2502     varchar     \u2502           varchar           \u2502 int64 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 clusterName     \u2502 venice-1                    \u2502 51060 \u2502\n\u2502 clusterName     \u2502 venice-3                    \u2502 33422 \u2502\n\u2502 clusterName     \u2502 venice-5                    \u2502 27961 \u2502\n\u2502 clusterName     \u2502 venice-0                    \u2502 18471 \u2502\n\u2502 clusterName     \u2502 venice-4                    \u2502 18350 \u2502\n\u2502 reportTimestamp \u2502 last_30_days                \u2502 61991 \u2502\n\u2502 reportTimestamp \u2502 last_week                   \u2502 14811 \u2502\n\u2502 reportTimestamp \u2502 last_24h                    \u2502  2150 \u2502\n\u2502 storeName       \u2502 HB_VPJtarget_prod-venice-0  \u2502  2539 \u2502\n\u2502 storeName       \u2502 HB_VPJtarget_prod-venice-13 \u2502  2340 \u2502\n\u2502 storeName       \u2502 HB_VPJtarget_prod-venice-8  \u2502  2337 \u2502\n\u2502 storeName       \u2502 HB_VPJtarget_prod-venice-7  \u2502  2335 \u2502\n\u2502 storeName       \u2502 HB_VPJtarget_prod-venice-3  \u2502  2334 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 13 rows                                     3 columns \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"contributing/proposals/vip-5/#filtering","title":"Filtering","text":"<p>The above examples all demonstrate Facet Counting being performed on a full table. And while that is feasible, it can be a costly proposition. In practice, a more common scenario is to perform Facet Counting on a subset of rows of the  dataset. In the case of this proposal, the goal is to provide the ability to perform Facet Counting on some pre-defined keys within the Venice dataset.</p>"},{"location":"contributing/proposals/vip-5/#where-to-perform-the-counting","title":"Where to Perform the Counting?","text":"<p>There are a variety of locations where the counting could be performed: client, router, server. Ultimately, it is  probably ideal to have the ability to perform it in all three locations, and to decide by configuration what mode the system will operate in, so that we have the most operational flexibility. That, however, does not mean that we need to implement all of these in order to start getting value from the project (see Development Milestones).</p>"},{"location":"contributing/proposals/vip-5/#client-side-computation","title":"Client-side Computation","text":"<p>There is already support for performing Read Compute on the client-side, which is useful as a fallback in cases where server-side Read Compute is disabled for the queried store. Similarly, the ability to perform server-side Facet Counting  should be enabled via a store config, and the client should be capable of gracefully degrading to client-side compute if the server-side is disabled. This is important so that users can make use of the API no matter what the server-side settings are, and configs then become a lever for shifting work across components, rather than a functional blocker.</p>"},{"location":"contributing/proposals/vip-5/#server-side-computation","title":"Server-side Computation","text":"<p>The appeal of supporting server-side computations for Facet Counting is the same as for Read Compute:</p> <ol> <li>The response sizes should be much smaller, thus saving on network bandwidth and improving the overall end-to-end     performance.</li> <li>Moreover, the work can be partitioned across many servers, each of which would need to do just a fraction of the     total.</li> </ol> <p>The difference with currently supported Read Compute queries is that, given that Facet Counting is an aggregation query, there still needs to be some amount of work performed in the component which receives the subset of results from each server. In the case of the Fast Client, this final step must be performed on the FC itself. In the case of the Thin  Client, it could be performed either on the TC or on the Router.</p>"},{"location":"contributing/proposals/vip-5/#router-side-computation","title":"Router-side Computation","text":"<p>As mentioned above, for Thin Clients performing Facet Counting queries on a store where server-side computation is  enabled, the final step of the query processing could be done either on the client-side or router-side. From a  functional standpoint, either way works, but from an efficiency standpoint, it may be better to do it on the router-side to further reduce network bandwidth (between router and client). That being said, given that latency-sensitive  applications ought to onboard to the Fast Client anyway, there may be diminishing returns in expending effort to support router-side computation. Therefore, this milestone can be scheduled last, and it may be acceptable to not implement it  at all.</p>"},{"location":"contributing/proposals/vip-5/#scope","title":"Scope","text":"<p>The following is in-scope:</p> <ol> <li>Count by value and count by bucket, both of which would apply to specified keys only.</li> <li>The ability to perform counting on multiple columns as part of the same query, including the ability to specify the    same column for the sake of both count by value and count by bucket.</li> <li>Client-side (TC, FC, DVC) and server-side computation.</li> </ol> <p>The following is out of scope:</p> <ol> <li>Full table scans.</li> <li>Router-side computation.</li> <li>Perform both Read Compute and Facet Counting on the same keys as part of the same query.</li> </ol>"},{"location":"contributing/proposals/vip-5/#project-justification","title":"Project Justification","text":"<p>The goal of this project is to make Venice more amenable to leverage in a variety of derived data use cases. This can help users minimize the complexity of juggling multiple independent systems, leading to productivity and operability  improvements for them.</p> <p>While it is already possible for users to manually implement Facet Counting on data retrieved via Batch Get (which would be equivalent to the client-side support described above, from a performance standpoint), this approach does not scale well for larger workloads. Having the ability to code against a Venice-provided API and let the infrastructure decide if/when to shift the workload to the server-side provides better cost and scalability options.</p>"},{"location":"contributing/proposals/vip-5/#functional-specification","title":"Functional Specification","text":"<p>The Facet Counting API would be a DSL similar to, but distinct from, the Read Compute API. Queries could be specified using the following builder pattern:</p> <pre><code>public interface ComputeAggregationRequestBuilder&lt;K&gt; extends ExecutableRequestBuilder&lt;K, ComputeAggregationResponse&gt; {\n  /**\n   * Aggregation query where the content of specified fields are going to be grouped by their value, then the occurrence\n   * of each distinct value counted, after which the top K highest counts along with their associated values are going\n   * to be returned.\n   *\n   * @param topK The max number of distinct values for which to return a count.\n   * @param fieldNames The names of fields for which to perform the facet counting.\n   * @return The same builder instance, to chain additional operations onto.\n   */\n  ComputeAggregationRequestBuilder&lt;K&gt; countGroupByValue(int topK, String... fieldNames);\n\n  /**\n   * Aggregation query where the content of specified fields are going to be grouped by buckets, with each bucket being\n   * defined by some {@link Predicate}, and each matching occurrence incrementing a count for its associated bucket,\n   * after which the bucket names to counts are going to be returned.\n   *\n   * @param bucketNameToPredicate A map of predicate name -&gt; {@link Predicate} to define the buckets to group values by.\n   * @param fieldNames The names of fields for which to perform the facet counting.\n   * @return The same builder instance, to chain additional operations onto.\n   * @param &lt;T&gt; The type of the fields to apply the bucket predicates to.\n   */\n  &lt;T&gt; ComputeAggregationRequestBuilder&lt;K&gt; countGroupByBucket(\n          Map&lt;String, Predicate&lt;T&gt;&gt; bucketNameToPredicate,\n          String... fieldNames);\n}\n</code></pre> <p>Responses could be accessed using the following container:</p> <pre><code>public interface ComputeAggregationResponse {\n  /**\n   * @param fieldName for which to get the value -&gt; count map\n   * @return the value counts as defined by {@link ComputeAggregationRequestBuilder#countGroupByValue(int, String...)}\n   */\n  &lt;T&gt; Map&lt;T, Integer&gt; getValueToCount(String fieldName);\n\n  /**\n   * @param fieldName for which to get the bucket -&gt; count map\n   * @return the bucket counts as defined by {@link ComputeAggregationRequestBuilder#countGroupByBucket(Map, String...)}\n   */\n  Map&lt;String, Integer&gt; getBucketNameToCount(String fieldName);\n}\n</code></pre> <p>Using the proposed API to achieve the <code>Facet Counting on Multiple Columns</code> use case above would look like this:</p> <pre><code>public class VIP5Example {\n  public void vip(AvroGenericReadComputeStoreClient&lt;GenericRecord, GenericRecord&gt; client, Set&lt;GenericRecord&gt; keySet) {\n    // Note that the computeAggregation() API is not added in this PR, and will be added once initial support is built\n     ComputeAggregationRequestBuilder&lt;GenericRecord&gt; requestBuilder = client.computeAggregation();\n\n    // Using the Predicate API to define the filtering criteria of each bucket\n    long currentTime = System.currentTimeMillis();\n    Map&lt;String, Predicate&lt;Long&gt;&gt; bucketByTimeRanges = new HashMap&lt;&gt;();\n    bucketByTimeRanges.put(\"last_24h\", LongPredicate.greaterThan(currentTime - 1 * Time.MS_PER_DAY));\n    bucketByTimeRanges.put(\"last_week\", LongPredicate.greaterThan(currentTime - 7 * Time.MS_PER_DAY));\n    bucketByTimeRanges.put(\"last_30_days\", LongPredicate.greaterThan(currentTime - 30 * Time.MS_PER_DAY));\n\n    // Facet count for multiple columns, including both grouped by value and grouped by buckets\n     ComputeAggregationRequestBuilder&lt;GenericRecord&gt; requestBuilder = requestBuilder\n            .countGroupByValue(5, \"clusterName\", \"storeName\")\n            .countGroupByBucket(bucketByTimeRanges, \"reportTimestamp\");\n\n    // Specify filter to specific keys in order to execute the query\n    CompletableFuture&lt;ComputeAggregationResponse&gt; facetCountResponse = requestBuilder.execute(keySet);\n  }\n}\n</code></pre> <p>As we can see, the proposed DSL is significantly more succinct than the equivalent SQL. That is because although SQL can  be made to do nearly anything, Facet Counting of multiple columns in a single query is not a common use case.</p>"},{"location":"contributing/proposals/vip-5/#proposed-design","title":"Proposed Design","text":"<p>The client-side computation is very similar to Read Compute and therefore fairly straightforward. Essentially, the  client would retrieve the values for the keys of interest via Batch Get, then perform the computation locally.</p> <p>Regarding server-side computation, it could be achieved either by extending the current <code>/compute</code> endpoint on servers, or as a new endpoint. Given that there are significant differences between the two, it may be tedious to cram the two of them into the same endpoint. Moreover, since there is no need for the time being to combine Read Compute and Facet  Counting for the same set of keys as part of a single query, there is no incentive for now to incur this complexity  cost. If, in the future, it becomes necessary to combine these types of queries together, then the architecture can be further evolved at that point.</p> <p>The Facet Counting endpoint would need to have its own wire protocol, similar to the Read Compute wire protocol:</p> <ol> <li>Start by listing the computation details (encoded from what was specified in <code>ComputeAggregationRequestBuilder</code>).</li> <li>Then list all the keys to query and compute on.</li> </ol> <p>It is necessary to support protocol evolution, but being separate from Read Compute, these two can evolve on separate  tracks.</p>"},{"location":"contributing/proposals/vip-5/#development-milestones","title":"Development Milestones","text":"<ol> <li>Start with client-side support, so that users can start using the API as soon as possible, and that it works     regardless of store and server settings.</li> <li>Then implement the wire protocol and server-side support.</li> </ol>"},{"location":"contributing/proposals/vip-5/#future-work","title":"Future Work","text":"<p>Although this is out of scope from the current proposal, it is interesting to note that having the scaffolding in place  to perform aggregation queries opens up the door to performing other kinds of aggregations besides counting, e.g., min, max, average, sum, or other functions...</p>"},{"location":"contributing/proposals/vip-5/#test-plan","title":"Test Plan","text":"<p>This functionality requires the full scope of test methodologies: unit and integration tests, followed by collaboration with early adopter users to integrate and gradually ramp.</p>"},{"location":"contributing/proposals/vip-5/#references","title":"References","text":"<ol> <li>Facet Counting in Lucene</li> </ol>"},{"location":"contributing/proposals/vip-5/#appendix","title":"Appendix","text":"<p>See the code attached to PR 1612.</p>"},{"location":"contributing/proposals/vip-6/","title":"VIP-6: Run Venice on Kubernetes","text":"<ul> <li>Status: Under Discussion</li> <li>Author(s): Nisarg Thakkar</li> <li>Pull Request: PR 2064</li> <li>Release: N/A</li> </ul> <p>Please make a copy of this page - DO NOT EDIT this design document directly, unless you are making  changes to the template.</p> <p>Remove the instructions (in italics) before publishing.</p>"},{"location":"contributing/proposals/vip-6/#introduction-200-words-at-most","title":"Introduction  (200 words at most)","text":"<p>This section should provide a concise and comprehensive overview of this document. After reading the introduction,  the reader should have a high level understanding of the problem being solved (the \"what\"), why the problem is  important (the \"why\"), and how this design intends to solve it (the \"how\").  When the draft design document is circulated, readers may use the introduction to determine if it is  applicable to their area of work and if they are interested in reading further.</p>"},{"location":"contributing/proposals/vip-6/#problem-statement","title":"Problem Statement","text":"<p>This section should frame the problem clearly and state the non-negotiable requirements for a solution.  There is some overlap with the Introduction section but it is here for emphasis since it forms the basis  for evaluating any proposed design. There should be no disagreement on the problem statement,  otherwise a design or implementation is unlikely to be successful. </p>"},{"location":"contributing/proposals/vip-6/#scope","title":"Scope","text":"<p>Call out explicitly:</p> <ol> <li>What is in scope?</li> </ol> <p>For example, this functionality will only apply to batch-only stores.</p> <ol> <li> <p>What is out of scope?</p> <p>For example, this functionality won\u2019t apply to DaVinci Clients.</p> </li> </ol>"},{"location":"contributing/proposals/vip-6/#project-justification","title":"Project Justification","text":"<p>This section should justify the necessity to address the problem mentioned above, in another way,  we could think about what will happen if we don\u2019t solve the problem, and we could consider the following dimensions. 1. New use cases. 2. Customer experience 3. Productivity improvement. 4. Resilience. 5. Operability improvement. 6. Performance improvement. 7. Maintainability improvement. 8. Scalability improvement. 9. Cost to Serve reduction. 10. Reduce Toil. 11. Craftsmanship improvement. 12. Security improvement. 13. Etc.</p>"},{"location":"contributing/proposals/vip-6/#functional-specification","title":"Functional Specification","text":"<p>If this is a development of a functionality/product for users, specify what it should look like from the user's point of view. It may include: 1. Public API. 2. Expected public behavior. 3. Public behavior of error handling, etc.</p>"},{"location":"contributing/proposals/vip-6/#proposed-design","title":"Proposed Design","text":"<p>This section must describe the proposed design and implementation in detail, and how it  addresses the problems outlined in the problem statement. It must provide details on alternative solutions  that were considered, stating clearly why they lost out to the final design. This latter point is particularly  important or else we tend to vacillate on old decisions because we cannot remember the original rationale.  Phrase this section in terms of trade offs since it is rare that one approach is better in all ways; there is generally a tradeoff and making that explicit makes it much easier to understand.</p> <p>The following aspects of the design must be covered when applicable:</p> <ol> <li>Changes to APIs or protocols or data format highlighting any compatibility issues and how they will be addressed.</li> <li>Major components and sequence diagrams between them.</li> <li>Multi-region implications including Parent/Child Controller communication.</li> <li>Alternative designs considered, and brief explanation of tradeoffs and decision versus the chosen design,  and important aspects to consider:</li> <li>Security.</li> <li>Performance.</li> <li>Maintainability.</li> <li>Operability.</li> <li>Scalability.</li> <li>Compatibility.</li> <li>Testability.</li> <li>Risks/Mitigations.</li> <li>Overall effort.</li> <li>Leverageable.</li> <li>Align with the long-term vision.</li> <li>Preferred option by the designer/author.</li> <li>Conclusions/Decisions made in the design review sessions. Explanation why it\u2019s selected</li> </ol>"},{"location":"contributing/proposals/vip-6/#development-milestones","title":"Development Milestones","text":"<p>This section described milestones for rather large projects so that we can set up intermediate goals for  better progress management. For small projects, this may not be necessary.</p>"},{"location":"contributing/proposals/vip-6/#test-plan","title":"Test Plan","text":"<p>Describe in few sentences how the functionality will be tested.  How will we know that the implementation works as expected? How will we know nothing broke?</p> <ol> <li>What unit tests would be added to cover the critical logic?</li> <li>What integration tests would be added to cover major components, if they cannot be tested by unit tests?</li> <li>Any tests to cover performance or Security concerns.</li> </ol>"},{"location":"contributing/proposals/vip-6/#references","title":"References","text":"<p>List any references here.</p>"},{"location":"contributing/proposals/vip-6/#appendix","title":"Appendix","text":"<p>Add supporting information here, such as the results of performance studies and back-of-the-envelope calculations.  Organize it however you want but make sure it is comprehensible to your readers.</p>"},{"location":"contributing/proposals/vip-template/","title":"VIP-<code>$VIP_Number</code>: <code>$VIP_Title</code>","text":"<ul> <li>Status: \"one of ['Under Discussion', 'Accepted', 'Implemented', 'Rejected']\"</li> <li>Author(s): (Names)</li> <li>Pull Request: (Link to the main pull request to resolve this VIP)</li> <li>Release: (Which release include this VIP)</li> </ul> <p>Please make a copy of this page - DO NOT EDIT this design document directly, unless you are making  changes to the template.</p> <p>Remove the instructions (in italics) before publishing.</p>"},{"location":"contributing/proposals/vip-template/#introduction-200-words-at-most","title":"Introduction  (200 words at most)","text":"<p>This section should provide a concise and comprehensive overview of this document. After reading the introduction,  the reader should have a high level understanding of the problem being solved (the \"what\"), why the problem is  important (the \"why\"), and how this design intends to solve it (the \"how\").  When the draft design document is circulated, readers may use the introduction to determine if it is  applicable to their area of work and if they are interested in reading further.</p>"},{"location":"contributing/proposals/vip-template/#problem-statement","title":"Problem Statement","text":"<p>This section should frame the problem clearly and state the non-negotiable requirements for a solution.  There is some overlap with the Introduction section but it is here for emphasis since it forms the basis  for evaluating any proposed design. There should be no disagreement on the problem statement,  otherwise a design or implementation is unlikely to be successful. </p>"},{"location":"contributing/proposals/vip-template/#scope","title":"Scope","text":"<p>Call out explicitly:</p> <ol> <li>What is in scope?</li> </ol> <p>For example, this functionality will only apply to batch-only stores.</p> <ol> <li> <p>What is out of scope?</p> <p>For example, this functionality won\u2019t apply to DaVinci Clients.</p> </li> </ol>"},{"location":"contributing/proposals/vip-template/#project-justification","title":"Project Justification","text":"<p>This section should justify the necessity to address the problem mentioned above, in another way,  we could think about what will happen if we don\u2019t solve the problem, and we could consider the following dimensions. 1. New use cases. 2. Customer experience 3. Productivity improvement. 4. Resilience. 5. Operability improvement. 6. Performance improvement. 7. Maintainability improvement. 8. Scalability improvement. 9. Cost to Serve reduction. 10. Reduce Toil. 11. Craftsmanship improvement. 12. Security improvement. 13. Etc.</p>"},{"location":"contributing/proposals/vip-template/#functional-specification","title":"Functional Specification","text":"<p>If this is a development of a functionality/product for users, specify what it should look like from the user's point of view. It may include: 1. Public API. 2. Expected public behavior. 3. Public behavior of error handling, etc.</p>"},{"location":"contributing/proposals/vip-template/#proposed-design","title":"Proposed Design","text":"<p>This section must describe the proposed design and implementation in detail, and how it  addresses the problems outlined in the problem statement. It must provide details on alternative solutions  that were considered, stating clearly why they lost out to the final design. This latter point is particularly  important or else we tend to vacillate on old decisions because we cannot remember the original rationale.  Phrase this section in terms of trade offs since it is rare that one approach is better in all ways; there is generally a tradeoff and making that explicit makes it much easier to understand.</p> <p>The following aspects of the design must be covered when applicable:</p> <ol> <li>Changes to APIs or protocols or data format highlighting any compatibility issues and how they will be addressed.</li> <li>Major components and sequence diagrams between them.</li> <li>Multi-region implications including Parent/Child Controller communication.</li> <li>Alternative designs considered, and brief explanation of tradeoffs and decision versus the chosen design,  and important aspects to consider:</li> <li>Security.</li> <li>Performance.</li> <li>Maintainability.</li> <li>Operability.</li> <li>Scalability.</li> <li>Compatibility.</li> <li>Testability.</li> <li>Risks/Mitigations.</li> <li>Overall effort.</li> <li>Leverageable.</li> <li>Align with the long-term vision.</li> <li>Preferred option by the designer/author.</li> <li>Conclusions/Decisions made in the design review sessions. Explanation why it\u2019s selected</li> </ol>"},{"location":"contributing/proposals/vip-template/#development-milestones","title":"Development Milestones","text":"<p>This section described milestones for rather large projects so that we can set up intermediate goals for  better progress management. For small projects, this may not be necessary.</p>"},{"location":"contributing/proposals/vip-template/#test-plan","title":"Test Plan","text":"<p>Describe in few sentences how the functionality will be tested.  How will we know that the implementation works as expected? How will we know nothing broke?</p> <ol> <li>What unit tests would be added to cover the critical logic?</li> <li>What integration tests would be added to cover major components, if they cannot be tested by unit tests?</li> <li>Any tests to cover performance or Security concerns.</li> </ol>"},{"location":"contributing/proposals/vip-template/#references","title":"References","text":"<p>List any references here.</p>"},{"location":"contributing/proposals/vip-template/#appendix","title":"Appendix","text":"<p>Add supporting information here, such as the results of performance studies and back-of-the-envelope calculations.  Organize it however you want but make sure it is comprehensible to your readers.</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Quick guides to set up and run Venice locally.</p> <ul> <li>Single Datacenter Quickstart</li> <li>Multi Datacenter Quickstart</li> </ul>"},{"location":"getting-started/quickstart-multi-dc/","title":"Venice Multi-Datacenter Docker Quickstart","text":"<p>Follow this guide to set up a multi-datacenter venice cluster using docker images provided by Venice team.</p>"},{"location":"getting-started/quickstart-multi-dc/#step-1-install-and-set-up-docker-engine-and-docker-compose","title":"Step 1: Install and set up Docker Engine and docker-compose","text":"<pre><code>Follow https://docs.docker.com/engine/install/ to install docker and start docker engine\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-2-download-docker-compose-multi-dc-setupyaml-file","title":"Step 2: Download docker-compose-multi-dc-setup.yaml file","text":"<pre><code>wget https://raw.githubusercontent.com/linkedin/venice/main/docker/docker-compose-multi-dc-setup.yaml\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-3-run-docker-compose-to-bring-up-venice-multi-colo-setup","title":"Step 3: Run docker compose to bring up Venice multi-colo setup","text":"<pre><code>docker-compose -f docker-compose-multi-dc-setup.yaml up -d\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-4-access-venice-client-containers-bash-shell","title":"Step 4: Access <code>venice-client</code> container's bash shell","text":"<pre><code>docker exec -it venice-client /bin/bash\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-5-make-sure-that-youre-in-optvenice-directory","title":"Step 5: Make sure that you're in /opt/venice directory","text":"<pre><code># pwd\n/opt/venice\n# if not change current working directory to /opt/venice\ncd /opt/venice\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-6-create-a-store","title":"Step 6: Create a store","text":"<p>Note: If you change the store name from <code>test-store</code> to something else, you will have to modify <code>/opt/venice/sample-data/multi-dc-configs/batch-push-job.properties</code> and <code>/opt/venice/sample-data/multi-dc-configs/inc-push-job.properties</code> to use the provided store name. </p> <pre><code>./create-store.sh http://venice-controller.dc-parent.venicedb.io:5555 venice-cluster0 test-store sample-data/schema/keySchema.avsc sample-data/schema/valueSchema.avsc \n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-7-lets-add-a-dataset-to-the-store-using-batch-push","title":"Step 7: Let's add a dataset to the store using batch push","text":""},{"location":"getting-started/quickstart-multi-dc/#print-dataset","title":"Print dataset","text":"<pre><code>./avro-to-json.sh sample-data/batch-push-data/kv_records.avro \n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#run-a-push-job","title":"Run a push job","text":"<pre><code>./run-vpj.sh sample-data/multi-dc-configs/batch-push-job.properties \n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#fetch-data-from-dc-0","title":"Fetch data from dc-0","text":"<pre><code>./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 90 # should return a value\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 100 # should return a value\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 110 # should return null\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 120 # should return null\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#fetch-data-from-dc-1","title":"Fetch data from dc-1","text":"<pre><code>./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 90 # should return a value\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 100 # should return a value\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 110 # should return null\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 120 # should return null\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-8-lets-update-some-existing-records-in-the-dataset-and-add-few-new-records-using-incremental-push","title":"Step 8: Let's update some existing records in the dataset and add few new records using incremental push","text":""},{"location":"getting-started/quickstart-multi-dc/#print-records-to-be-updated-and-added-to-the-existing-dataset-in-the-store","title":"Print records to be updated and added to the existing dataset in the store","text":"<pre><code>./avro-to-json.sh sample-data/inc-push-data/kv_records_v1.avro \n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#run-incremental-push-job","title":"Run incremental push job","text":"<pre><code>./run-vpj.sh sample-data/multi-dc-configs/inc-push-job.properties \n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#fetch-data-from-dc-0_1","title":"Fetch data from dc-0","text":"<pre><code>./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 90 # should return an unchanged value\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 100 # should return an updated value\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 110 # should return inserted value\n./fetch.sh http://venice-router.dc-0.venicedb.io:7777 test-store 120 # should return null\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#fetch-data-from-dc-1_1","title":"Fetch data from dc-1","text":"<pre><code>./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 90 # should return an unchanged value\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 100 # should return an updated value\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 110 # should return inserted value\n./fetch.sh http://venice-router.dc-1.venicedb.io:7777 test-store 120 # should return null\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-9-exit-from-the-venice-client-container","title":"Step 9: Exit from the venice-client container","text":"<pre><code># type exit command on the terminal or use cntrl + c\nexit\n</code></pre>"},{"location":"getting-started/quickstart-multi-dc/#step-10-teardown-the-cluster","title":"Step 10: Teardown the cluster","text":"<pre><code>docker-compose -f docker-compose-multi-dc-setup.yaml down\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/","title":"Venice Single-Datacenter Docker Quickstart","text":"<p>Follow this guide to set up a simple venice cluster using docker images provided by Venice team.</p>"},{"location":"getting-started/quickstart-single-dc/#step-1-install-and-set-up-docker-engine-and-docker-compose","title":"Step 1: Install and set up Docker Engine and docker-compose","text":"<pre><code>Follow https://docs.docker.com/engine/install/ to install docker and start docker engine\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#step-2-download-venice-quickstart-docker-compose-file","title":"Step 2: Download Venice quickstart Docker compose file","text":"<pre><code>wget https://raw.githubusercontent.com/linkedin/venice/main/docker/docker-compose-single-dc-setup.yaml\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#step-3-run-docker-compose","title":"Step 3: Run docker compose","text":"<p>This will download and start containers for kafka, zookeeper, venice-controller, venice-router, venice-server, and venice-client. Once containers are up and running, it will create a test cluster, namely, <code>venice-cluster</code>.</p> <p>Note: Make sure the <code>docker-compose-single-dc-setup.yaml</code> downloaded in step 2 is in the same directory from which you will run the following command. <pre><code>docker-compose -f docker-compose-single-dc-setup.yaml up -d\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#step-4-access-venice-client-containers-bash-shell","title":"Step 4: Access <code>venice-client</code> container's bash shell","text":"<p>From this container, we will create a store in <code>venice-cluster</code>, which was created in step 3, push data to it and run queries against it. <pre><code>docker exec -it venice-client bash\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#step-5-create-a-venice-store","title":"Step 5: Create a venice store","text":"<p>The below script uses <code>venice-admin-tool</code> to create a new store: <code>venice-store</code>. We will use the following key and value schema for store creation.</p> <p>key schema: <pre><code>{\n    \"name\": \"key\",\n    \"type\": \"string\"\n}\n</code></pre> value schema: <pre><code>{\n   \"name\": \"value\",\n   \"type\": \"string\"\n}\n</code></pre></p> <p>Let's create a venice store: <pre><code>./create-store.sh http://venice-controller:5555 venice-cluster0 test-store sample-data/schema/keySchema.avsc sample-data/schema/valueSchema.avsc\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#step-6-push-data-to-the-store","title":"Step 6: Push data to the store","text":"<p>Venice supports multiple ways to write data to the store. For more details, please refer to Write APIs. In this example, we will use batch push mode and push 100 records. <pre><code>key: 1 to 100\nvalue: val1 to val100\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#print-dataset","title":"Print dataset","text":"<pre><code>./avro-to-json.sh sample-data/batch-push-data/kv_records.avro \n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#run-a-push-job","title":"Run a push job","text":"<p>Let's push the data: <pre><code>./run-vpj.sh sample-data/single-dc-configs/batch-push-job.properties\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#step-7-read-data-from-the-store","title":"Step 7: Read data from the store","text":"<p>Let's query some data from <code>venice-store</code>, using <code>venice-thin-client</code> which is included in venice container images. (key: <code>1 to 100</code>) <pre><code>./fetch.sh &lt;router&gt; &lt;store&gt; &lt;key&gt;\n</code></pre> For example: <pre><code>$ ./fetch.sh http://venice-router:7777 test-store 1 \nkey=1\nvalue=val1\n\n$ ./fetch.sh http://venice-router:7777 test-store 100\nkey=100\nvalue=val100\n\n# Now if we do get on non-existing key, venice will return `null`\n$ ./fetch.sh http://venice-router:7777 test-store 101\nkey=101\nvalue=null\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#step-8-update-and-add-some-new-records-using-incremental-push","title":"Step 8: Update and add some new records using Incremental Push","text":"<p>Venice supports incremental push which allows us to update values of existing rows or to add new rows in an existing store. In this example, we will 1. update values for keys from <code>91-100</code>. For example, the new value of <code>100</code> will be <code>val100_v1</code> 2. add new rows (key: <code>101-110</code>)</p>"},{"location":"getting-started/quickstart-single-dc/#print-records-to-be-updated-and-added-to-the-existing-dataset-in-the-store","title":"Print records to be updated and added to the existing dataset in the store","text":"<pre><code>./avro-to-json.sh sample-data/inc-push-data/kv_records_v1.avro \n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#run-incremental-push-job","title":"Run incremental push job","text":"<pre><code>./run-vpj.sh sample-data/single-dc-configs/inc-push-job.properties\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#step-9-read-data-from-the-store-after-incremental-push","title":"Step 9: Read data from the store after Incremental Push","text":"<p>Incremental Push updated the values of keys 91-100 and added new rows 101-110. Let's read the data once again.</p> <pre><code># Value of 1 changed remains unchanged\n$ ./fetch.sh http://venice-router:7777 test-store 1\nkey=1\nvalue=val1\n\n# Value of 100 changed from test_name_100 to test_name_100_v1\n$ ./fetch.sh http://venice-router:7777 test-store 100\nkey=100\nvalue=val100_v1\n\n# Incremental Push added value for previously non-existing key 101\n$ ./fetch.sh http://venice-router:7777 test-store 101\nkey=101\nvalue=val101\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#step-10-exit-venice-client","title":"Step 10: Exit <code>venice-client</code>","text":"<pre><code># type exit command on the terminal or use ctrl + c\nexit\n</code></pre>"},{"location":"getting-started/quickstart-single-dc/#step-11-stop-docker","title":"Step 11: Stop docker","text":"<p>Tear down the venice cluster <pre><code>docker-compose -f docker-compose-single-dc-setup.yaml down\n</code></pre></p>"},{"location":"getting-started/quickstart-single-dc/#next-steps","title":"Next steps","text":"<p>Venice is a feature rich derived data store. It offers features such as write-compute, read-compute, streaming ingestion, multi data center active-active replication, deterministic conflict resolution, etc. To know more about such features please refer to the User Guide and reach out to the Venice team.</p>"},{"location":"operations/","title":"Operations Guide","text":"<p>Documentation for Venice operators.</p>"},{"location":"operations/#sections","title":"Sections","text":"<ul> <li>Data Management - Repush and system stores</li> <li>Advanced - P2P bootstrapping and data integrity</li> </ul>"},{"location":"operations/advanced/data-integrity/","title":"Venice Data Integrity Validation","text":""},{"location":"operations/advanced/data-integrity/#what-is-data-integrity-validation-div","title":"What is Data Integrity Validation (DIV)?","text":"<ul> <li>Validate and detect errors in transmitted (Kafka) data.</li> <li>The sender (VeniceWriter) calculates and includes the DIV metadata.</li> <li>The receiver recalculates and verifies the DIV metadata on arrival.</li> </ul>"},{"location":"operations/advanced/data-integrity/#what-does-div-metadata-look-like","title":"What does DIV metadata look like?","text":"<ul> <li>Sender transmits data in segments.</li> <li>Sender decides when to close the current segment and start a new one.</li> <li>A segment contains multiple Kafka records</li> <li>Start of Segment (SOS)</li> <li>End of Segment (EOS)</li> <li>Control or data messages between</li> <li>GUID</li> <li>One unique ID per-sender (VeniceWriter).</li> <li>GUID changes if VeniceWriter or the server restarts.</li> <li>Sequence number</li> <li>Within a segment, the sequence number starts at 0.</li> </ul>"},{"location":"operations/advanced/data-integrity/#data-integrity-issues-in-venice","title":"Data integrity issues in Venice:","text":"<ul> <li>A segment starts from a non-zero sequence number (UNREGISTERED_PRODUCER).</li> <li>A gap exists between or within segments (MISSING).</li> <li>Data within a segment is corrupted (CORRUPT).</li> <li>Producers have produced duplicate messages, which is expected due to retries (DUPLICATE).</li> </ul>"},{"location":"operations/advanced/data-integrity/#different-behaviors-before-and-after-end-of-push-eop","title":"Different behaviors before and after End Of Push (EOP)","text":"<ul> <li>End Of Push (EOP) is a control message in the Kafka topic sent once per partition at bulk load end, after all data producers come online.</li> <li>When pushing a new store version, if DIV detects fatal data validation issues (including UNREGISTERED_PRODUCER, MISSING, and CORRUPT) while consuming batch push data before EOP, the ingestion task errors and aborts.</li> <li>If issues appear after receiving EOP, the ingestion task continues, but DIV logs a warning about data integrity issues.</li> </ul>"},{"location":"operations/advanced/data-integrity/#data-structures","title":"Data structures","text":"<ul> <li>Sender - VeniceWriter/Segment</li> <li>Data - ProducerRecord/KafkaMessageEnvelope/ProducerMetadata</li> <li>Receiver - ConsumptionTask/StoreIngestionTask</li> </ul>"},{"location":"operations/advanced/data-integrity/#persistence","title":"Persistence","text":"<ul> <li>DIV needs to be checkpointed for several reasons:</li> <li>DIV state would be lost on server restarts leading to potential data integrity issues undetected (inaccuracy)</li> <li>Otherwise, without checkpointing, DIV needs to rebuild its state from the beginning of the topic on every restart (inefficiency)</li> <li>In case of a crash or restart, we need to know where to resume validation from (last checkpoint).</li> </ul>"},{"location":"operations/advanced/data-integrity/#why-two-separate-div-validators","title":"Why Two Separate DIV validators?","text":"<ul> <li>Two state pipeline</li> <li>Stage 1: Consumer Thread</li> <li>Kafka \u2192 Consumer Thread \u2192 Validation \u2192 Queue \u2192 ...</li> <li>Reads messages from Kafka topics</li> <li>Performs validation using Consumer DIV (RT or remote VT)</li> <li> <p>Queues messages to StoreBufferService</p> </li> <li> <p>Stage 2: Drainer Thread</p> </li> <li>... \u2192 Queue \u2192 Drainer Thread \u2192 Validation \u2192 Storage Engine</li> <li>Dequeues messages from StoreBufferService</li> <li>Performs validation using drainer DIV (all topics)</li> <li>Persists data to storage engine</li> <li> <p>Checkpoints offsets to disk.</p> </li> <li> <p>Consumer is always ahead of drainer</p> </li> <li>The consumer thread reads from Kafka faster than the drainer can persist to disk. There's buffering in between.</li> <li>Kafka producer buffers (for leaders)</li> <li>StoreBufferService queue etc.</li> <li>Time T1: Consumer validates message at offset 1000 \u2192 Consumer DIV state = offset 1000</li> <li>Time T2: Consumer validates message at offset 2000 \u2192 Consumer DIV state = offset 2000</li> <li>Time T3: Drainer persists message at offset 1000 \u2192 Drainer DIV state = offset 1000</li> <li> <p>If we only had one consumer DIV, we couldn't checkpoint the correct state to disk.</p> </li> <li> <p>DIV State Must Match Persisted Data</p> </li> <li> <p>The DIV checkpoint saved to disk must exactly match what's actually persisted in the storage engine. Since the drainer is responsible for persistence, only the Drainer DIV state can be safely checkpointed.</p> </li> <li> <p>Leader Double Validation</p> </li> <li>Consumer DIV (in consumer thread): Validates RT messages before producing to VT</li> <li>Drainer DIV (in drainer thread): Validates same messages again before persisting (unnecessary)</li> </ul>"},{"location":"operations/advanced/data-integrity/#div-in-state-transitions-per-partition","title":"DIV in State Transitions (per-partition)","text":"<ul> <li>OFFLINE -&gt; STANDBY: DIV state restoration.   The DIV state is restored from the persisted OffsetRecord (drainerDiv). In STANDBY mode, the DIV continues to validate incoming messages against the restored state and keeps it updated.</li> <li>STANDBY -&gt; OFFLINE: The DIV state is cleared immediately without being persisted to disk.   Any DIV state accumulated since the last checkpoint is lost The system relies on periodic checkpoints during consumption, not on-demand checkpoints during state transitions. This design choice prioritizes fast un-subscription over preserving the absolute latest DIV state.</li> <li>STANDBY -&gt; LEADER: Wipes all DIV state for the partition in the consumer DIV.   Copies the producer states from the drainer's DIV validator to the consumer DIV.</li> <li>LEADER -&gt; STANDBY: The drainer DIV maintains producer states that have been validated and persisted.   If this replica becomes leader again, it will clone the drainer DIV state.</li> </ul>"},{"location":"operations/advanced/data-integrity/#kafka-log-compaction-impacts-div-in-venice","title":"Kafka Log Compaction impacts DIV in Venice","text":"<ul> <li>When Kafka log compaction runs, it:</li> <li>Deletes duplicate records (identified by the key) and keeps only the latest record.</li> <li>Creates gaps in sequence numbers that DIV would normally flag as data loss.</li> <li>Venice uses a log compaction delay threshold to distinguish between:<ul> <li>Real data loss (missing messages within the compaction window)</li> <li>Expected gaps (missing messages beyond the compaction window)</li> </ul> </li> <li>Example:<ul> <li>T0: Producer sends messages with seq# 1, 2, 3, 4, 5</li> <li>T1: Kafka stores all messages</li> <li>T2: min.compaction.lag.ms = 24 hours passes</li> <li>T3: Kafka compaction runs, deletes messages 2, 3 (duplicate keys)</li> <li>T4: Consumer reads: seq# 1, 4, 5 (gap detected!)</li> </ul> </li> <li>DIV Check:<ul> <li>Last message timestamp: T0</li> <li>Current time: T4 (&gt; 24 hours later)</li> <li>elapsedTime &gt;= min.compaction.lag.ms? YES</li> </ul> </li> <li>Result: TOLERATE the gap (expected compaction)<ul> <li>T4: Consumer reads: seq# 1, 4, 5 (gap detected!)</li> <li>elapsedTime &lt; min.compaction.lag.ms? YES</li> </ul> </li> <li>Result: THROW EXCEPTION (data loss!)</li> </ul>"},{"location":"operations/advanced/data-integrity/#div-cleanup-mechanism","title":"DIV cleanup mechanism","text":"<ul> <li>Prevents DIV from accumulating unbounded producer state by automatically expiring and removing old producer tracking information.</li> <li>Without cleanup, this state grows indefinitely</li> <li>Large memory footprint - State accumulates in heap</li> <li>Checkpoint overhead - All producer states are persisted to disk on every offset sync</li> <li>Solution: Time-Based Expiration</li> <li>MaxAge allows Venice to automatically expire old producer state that's no longer relevant.</li> <li>Only applied to drainer DIV (not consumer DIV, which is transient)</li> <li>For hybrid stores, maxAge is &gt;= rewind time.</li> <li>Can be disabled by setting to -1.</li> <li>How it works:</li> <li>When checkpointing offsets, DIV clears expired state.</li> <li>When loading state from disk (e.g., after restart), maxAge is also applied. This prevents loading stale state after a restart.</li> <li>Why MaxAge \u2265 Rewind Time for Hybrid Stores?<ul> <li>Support RT replay, otherwise it doesn't recognize producer\u2019s div state in RT.</li> </ul> </li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/","title":"Venice P2P Transfer Bootstrapping Architecture","text":"<p>High-Performance Peer-to-Peer Data Bootstrap</p> <p>Accelerating Venice Node Deployment Through Direct Peer-to-Peer RocksDB Snapshot Transfer</p>"},{"location":"operations/advanced/p2p-bootstrapping/#problem-solution","title":"\ud83d\udea8 Problem &amp; Solution","text":""},{"location":"operations/advanced/p2p-bootstrapping/#the-problem","title":"The Problem","text":"<p>Bootstrap Bottlenecks - Kafka-Only Recovery: All nodes bootstrap exclusively from Kafka brokers - Resource Intensive: Time-consuming process during cluster recovery, inefficient for consuming messages from the PubSub system under high-update workloads - Scalability Limits: Broker capacity becomes recovery bottleneck</p> <p>Real-World Impact - Extended MTTR (Mean Time to Restore or Repair) during outages - Cascading broker overload - Slower cluster expansion - Increased operational overhead</p>"},{"location":"operations/advanced/p2p-bootstrapping/#the-solution","title":"\ud83d\udca1 The Solution","text":"<p>Direct P2P Transfer - Peer-to-Peer: Direct node-to-node data transfer - RocksDB Snapshots: Consistent point-in-time data copies - Intelligent Fallback: Automatic Kafka Ingestion recovery on failure - Low Risk: Low Deployment Risk in DaVinci client</p> <p>Key Benefits - Faster node recovery and scaling - Reduced Kafka broker load</p>"},{"location":"operations/advanced/p2p-bootstrapping/#system-architecture-flow","title":"\ud83c\udfd7\ufe0f System Architecture Flow","text":""},{"location":"operations/advanced/p2p-bootstrapping/#venice-blob-transfer-complete-flow","title":"Venice Blob Transfer Complete Flow","text":"<pre><code>                           Venice Blob Transfer Complete Flow\n\nStep 1: Peer Discovery\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  Discovery Request   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  Query Helix/ZK     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Client Node \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502 Discovery   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Metadata    \u2502\n\u2502 (Needs Data)\u2502                      \u2502 Service     \u2502                     \u2502 Repository  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       ^                                     \u2502                                   \u2502\n       \u2502                                     \u2502 Return Host List                  \u2502\n       \u2502                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                            \u2502\n       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 Host List:  \u2502 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      \u2502 [host1,     \u2502\n                                      \u2502  host2,     \u2502\n                                      \u2502  host3...]  \u2502\n                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 2: Sequential Host Attempts\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  T1:Try Host 1  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n\u2502 Client Node \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Server A    \u2502 \n\u2502             \u2502 &lt;\u2500\u2500 FAIL \u2500\u2500\u2500\u2500\u2500\u2500 \u2502 (No Data/   \u2502                 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502  Busy)      \u2502                \n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \n                 T2:Try Host 2  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \n                \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Server B    \u2502 \n                &lt;\u2500\u2500 FAIL \u2500\u2500\u2500\u2500\u2500\u2500 \u2502 Table Format\u2502\n                                \u2502  not Match  \u2502              \n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \n\n                 T3:Try Host 3  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \n                \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Server C    \u2502 \n                                \u2502 (Not busy,  \u2502               \n                                \u2502Format Match)\u2502              \n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \n\nStep 3: Start Transfer                   \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                       \n\u2502 Client Node    \u2502 \n\u2502                \u2502\n\u2502  Receives:     \u2502 &lt;\u2500\u2500 1.1 File: file1.sst \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Files      \u2502 &lt;\u2500\u2500 1.2 File: file2.sst \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 Server C    \u2502\n\u2502                \u2502 &lt;\u2500\u2500 1.3 File: MANIFEST_00 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502             \u2502\n\u2502  2. Metadata   \u2502 &lt;\u2500\u2500 2. Metadata \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502  3. COMPLETE   \u2502 &lt;\u2500\u2500 3. COMPLETE Flag \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 4: Client Processing after recevie COMPLETE flag\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  Validate Files     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    Atomic Rename     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Client Node \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Temp Folder \u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Partition   \u2502\n\u2502             \u2502  (MD5)              \u2502/store/temp_ \u2502    (temp\u2192partition)  \u2502 Folder      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     | p1/         |                      \u2502 /store/p1/  \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 5: Kafka Ingestion Fallback (Always Happens)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  Resume/Fill Gap    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Client Node \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt; \u2502 Kafka       \u2502\n\u2502             \u2502  From snapshot      \u2502 Ingestion   \u2502\n\u2502             \u2502  offset to latest   \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Step 1: Peer Discovery - Venice Server: Query Helix CustomizedViewRepository for COMPLETED nodes - DaVinci Application: Query DaVinci push job report for ready-to-serve nodes</p> <p>Step 2: P2P Transfer in Client Side <pre><code>Connectivity Check \u2500\u2500\u2500\u2500&gt; Peer Selection \u2500\u2500\u2500\u2500&gt; Sequential Request\n      \u2193                         \u2193                      \u2193\nParallel connection       Filter &amp; shuffle      Try peers until\ncheck with caching      connectable hosts      success or exhaust\n</code></pre></p> <p>Step 3: Data Transfer <pre><code>Snapshot Creation \u2500\u2500\u2500\u2500&gt; File Streaming \u2500\u2500\u2500\u2500&gt;  Metadata Sync\n       \u2193                       \u2193                  \u2193\nServer creates         Chunked transfer      Offset + Version State\nRocksDB snapshot      with MD5 validation     after files complete\n</code></pre> Step 4: Completion - Success Path: After validating all files, atomically rename the temp directory to the partition directory, then initiate Kafka ingestion to synchronize any remaining offset gap. - Fallback Path: If any error occurs, clean up the temp directory and retry with the next peer; if all peers fail, back to Kafka bootstrap from the beginning.</p>"},{"location":"operations/advanced/p2p-bootstrapping/#key-components","title":"Key Components","text":"<ul> <li>DefaultIngestionBackend - Bootstrap orchestration entry point</li> <li>NettyP2PBlobTransferManager - P2P transfer coordinator</li> <li>BlobSnapshotManager - Server-side snapshot lifecycle</li> <li>NettyFileTransferClient - High-performance client</li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/#client-side-process","title":"\ud83d\udce5 Client-Side Process","text":""},{"location":"operations/advanced/p2p-bootstrapping/#process-flow","title":"Process Flow","text":"<pre><code>1. Discover Peers \u2192 2. Check Connectivity \u2192 3. Request Data \u2192 4. Fallback to Kafka\n</code></pre>"},{"location":"operations/advanced/p2p-bootstrapping/#step-1-peer-discovery","title":"\ud83d\udd0d Step 1: Peer Discovery","text":"<p>Venice Server: - Query Helix CustomizedViewRepository - Find COMPLETED nodes - Filter by store/version/partition</p> <p>DaVinci Application: - Query DaVinci push job report - Find ready-to-serve nodes - Extract available peer list</p>"},{"location":"operations/advanced/p2p-bootstrapping/#step-2-connectivity-checking","title":"\ud83d\udd17 Step 2: Connectivity Checking","text":"<p>Smart Caching Strategy due to large peer sets:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Purge Stale \u2502 \u2192 \u2502 Filter Bad  \u2502 \u2192 \u2502 Check Hosts in      \u2502 \u2192 \u2502 Update Cache\u2502\n\u2502 Records     \u2502   \u2502 Hosts       \u2502   \u2502Parallel Connectivity\u2502   \u2502 Results     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                          \n</code></pre> <ul> <li>Parallel Testing: Multiple host connections simultaneously</li> <li>Smart Caching: Remember good/bad hosts with timestamps</li> <li>Timeout Management: 1-minute connectivity check limit</li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/#step-3-sequential-data-request","title":"\ud83d\udce6 Step 3: Sequential Data Request","text":"<pre><code>For Each Peer (Shuffled List):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Send Request  \u2502 \u2192 \u2502  Receive Data   \u2502 \u2192 \u2502  Receive        \u2502 \u2192 \u2502  Receive        \u2502 \u2192 \u2502   Validate      \u2502 \u2192 \u2502  Rename Temp    \u2502\n\u2502    HTTP GET     \u2502   \u2502  File Chunks    \u2502   \u2502  Metadata       \u2502   \u2502 COMPLETE_FLAG   \u2502   \u2502      MD5        \u2502   \u2502 to Partition Dir\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"operations/advanced/p2p-bootstrapping/#error-handling-details-in-different-scenarios","title":"\ud83d\udea8 Error Handling Details in different Scenarios","text":"<p>Connection Failures (VenicePeersConnectionException) - Network timeout - SSL handshake failure - Host unreachable - Action: Try next peer</p> <p>File Not Found (VeniceBlobTransferFileNotFoundException) - Snapshot missing - Stale snapshot - Server too busy, reject with 429 errors - Action: Try next peer</p> <p>Transfer Errors (Data Integrity Issues) - Checksum mismatch - File size mismatch - Network interruption: Server initiates deployment or shuts down unexpectedly - Action: Cleanup + next peer</p>"},{"location":"operations/advanced/p2p-bootstrapping/#comprehensive-cleanup-process","title":"\ud83e\uddf9 Comprehensive Cleanup Process","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Force Flush \u2502 \u2192 \u2502 Close File  \u2502 \u2192 \u2502 Delete      \u2502 \u2192 \u2502 Reset       \u2502\n\u2502 File Channel\u2502   \u2502 Handles     \u2502   \u2502 Partial Dir \u2502   \u2502 Handler     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Zero Partial State: Complete cleanup ensures no corruption</li> <li>Handler Reset: Ready for next peer attempt</li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/#step-4-kafka-fallback-strategy","title":"\ud83d\udd04 Step 4: Kafka Fallback Strategy","text":"<p>Two-Phase Strategy: - Phase 1 - Blob Transfer: Rapid bulk data transfer via P2P - Phase 2 - Kafka Fill-Gap: Even if blob transfer fails, deployment continues with Kafka ingestion</p> <p>Zero-Risk Design: After a successful blob transfer, Kafka ingestion always follows to synchronize any data between the snapshot offset and the latest offset, guaranteeing full deployment.</p>"},{"location":"operations/advanced/p2p-bootstrapping/#server-side-process","title":"\ud83d\udce4 Server-Side Process","text":""},{"location":"operations/advanced/p2p-bootstrapping/#process-flow_1","title":"Process Flow","text":"<pre><code>1. Request Reception &amp; Validation \u2192 2. Prepare Snapshot &amp; Metadata \u2192 3. File Transfer &amp; Chunking \u2192 4. Metadata Transfer &amp; Completion\n</code></pre>"},{"location":"operations/advanced/p2p-bootstrapping/#step-1-request-reception-validation","title":"\u2611\ufe0f Step 1: Request Reception &amp; Validation","text":"<p>Incoming Request Pipeline: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 HTTP GET    \u2502    \u2502 Parse URI   \u2502    \u2502 Validate    \u2502    \u2502 Check       \u2502\n\u2502 Request     \u2502 \u2192  \u2502 Extract     \u2502 \u2192  \u2502 Table       \u2502 \u2192  \u2502 Concurrency \u2502\n\u2502             \u2502    \u2502 Parameters  \u2502    \u2502 Format      \u2502    \u2502 Limits      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <ul> <li>URI Parsing: /storeName/version/partition/tableFormat</li> <li>Format Check: PLAIN_TABLE vs BLOCK_BASED_TABLE match verification</li> <li>Concurrency Control: Global limit enforcement; reject with 429 if over limit</li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/#step-2-prepare-snapshot-metadata","title":"\ud83d\udcca Step 2: Prepare Snapshot &amp; Metadata","text":"<p>Snapshot Lifecycle: - Check staleness (Configable TTL) - Verify concurrent user limits - Create a new snapshot if the existing one is stale or does not exist - Prepare partition metadata</p> <p>Metadata Preparation: - Serialization of StoreVersionState (enables synchronization of hybrid store configuration parameters) - OffsetRecord encapsulation (captures the snapshot\u2019s offset for accurate state synchronization) - JSON metadata response</p>"},{"location":"operations/advanced/p2p-bootstrapping/#step-3-file-transfer-chunking-strategy-serverclient-single-file-transfer-process-details","title":"\ud83d\udce6 Step 3: File Transfer &amp; Chunking Strategy (Server/Client Single File Transfer Process Details)","text":"<p>Server-Side Adaptive Chunking Algorithm:</p> <pre><code>Step 1: File Preparation\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Open File for         \u2502   \u2502   Calculate File        \u2502   \u2502   Generate MD5          \u2502   \u2502   Prepare HTTP          \u2502   \u2502   Send Response         \u2502\n\u2502     Reading             \u2502   \u2502      Length             \u2502   \u2502     Checksum            \u2502   \u2502    Response             \u2502   \u2502      Headers            \u2502\n\u2502                         \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502\n\u2502 Open file in            \u2502   \u2502 Get file size for       \u2502   \u2502 Generate checksum       \u2502   \u2502 Set content headers     \u2502   \u2502 Send headers to         \u2502\n\u2502 read-only mode          \u2502   \u2502 response headers        \u2502   \u2502 for validation          \u2502   \u2502 and file metadata       \u2502   \u2502 client first            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 2: Adaptive Chunking Strategy\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \n\u2502   Calculate Optimal     \u2502   \u2502   Create Chunked        \u2502   \u2502   Wrap for HTTP         \u2502   \n\u2502     Chunk Size          \u2502   \u2502    File Handler         \u2502   \u2502     Streaming           \u2502  \n\u2502 16KB - 2MB              \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502 \n\u2502 Determine best chunk    \u2502   \u2502 Set up memory-efficient \u2502   \u2502 Prepare for HTTP        \u2502 \n\u2502 size based on file      \u2502   \u2502 streaming mechanism     \u2502   \u2502 chunked transfer        \u2502 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \n\nStep 3: Efficient File Streaming\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Stream File Data      \u2502   \u2502   Netty Chunked         \u2502   \u2502   Monitor Transfer      \u2502   \u2502   Complete Transfer     \u2502\n\u2502     in Chunks           \u2502   \u2502    Write Handler        \u2502   \u2502      Progress           \u2502   \u2502                         \u2502\n\u2502                         \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502 \u2192 \u2502                         \u2502\n\u2502 ctx.writeAndFlush()     \u2502   \u2502 Memory-efficient        \u2502   \u2502 Log success/failure     \u2502   \u2502 Ready for next file     \u2502\n\u2502 non-blocking write      \u2502   \u2502 file streaming          \u2502   \u2502 per file                \u2502   \u2502 or metadata             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Client-Side High-Performance Reception:</p> <p><pre><code>Step 1: File Setup\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Parse Headers         \u2502   \u2502   Create Temp Dir       \u2502   \u2502   Create Empty File     \u2502\n\u2502                         \u2502   \u2502                         \u2502   \u2502                         \u2502\n\u2502 \u2022 Extract filename      \u2502 \u2192 \u2502 \u2022 Make /store/temp_p0/  \u2502 \u2192 \u2502 \u2022 Create empty file     \u2502\n\u2502 \u2022 Extract file size     \u2502   \u2502 \u2022 Delete if exists      \u2502   \u2502 \u2022 Open FileChannel      \u2502\n\u2502 \u2022 Extract MD5 hash      \u2502   \u2502                         \u2502   \u2502   in WRITE mode         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nStep 2: Write Data Chunk for each chunk arrived repeatedly\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Receive Network       \u2502   \u2502   Convert to Channel    \u2502   \u2502   Write to File         \u2502\n\u2502      Data Chunk         \u2502   \u2502                         \u2502   \u2502                         \u2502\n\u2502                         \u2502 \u2192 \u2502 \u2022 ByteBuf \u2192 Stream      \u2502 \u2192 \u2502 \u2022 transferFrom() call   \u2502\n\u2502 \u2022 HttpContent arrives   \u2502   \u2502 \u2022 Stream \u2192 Channel      \u2502   \u2502                         \u2502\n\u2502 \u2022 Extract ByteBuf       \u2502   \u2502                         \u2502   \u2502 \u2022 Update file position  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nStep 3: Complete File\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Last Chunk Received   \u2502   \u2502   Flush and Close       \u2502   \u2502   Async Validation      \u2502\n\u2502                         \u2502   \u2502                         \u2502   \u2502                         \u2502\n\u2502 \u2022 Detect end of file    \u2502 \u2192 \u2502 \u2022 Force data to disk    \u2502 \u2192 \u2502 \u2022 Submit MD5 check      \u2502\n\u2502 \u2022 Validate file size    \u2502   \u2502 \u2022 Close FileChannel     \u2502   \u2502 \u2022 Reset handler state   \u2502\n\u2502                         \u2502   \u2502                         \u2502   \u2502 \u2022 Ready for next file   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>Data Flow Transformation at Convert to Channel\n\nNetwork \u2192 ByteBuf \u2192 ByteBufInputStream \u2192 ReadableByteChannel \u2192 FileChannel\n   \u2502         \u2502              \u2502                    \u2502               \u2502\n   \u2502         \u2502              \u2502                    \u2502               \u2514\u2500 Disk File\n   \u2502         \u2502              \u2502                    \u2514\u2500 NIO Channel (efficient)\n   \u2502         \u2502              \u2514\u2500 Java Stream (bridge)\n   \u2502         \u2514\u2500 Netty Buffer\n   \u2514\u2500 Raw network packets\n</code></pre></p>"},{"location":"operations/advanced/p2p-bootstrapping/#step-4-metadata-transfer-completion-protocol","title":"\ud83d\udcca Step 4: Metadata Transfer &amp; Completion Protocol","text":"<p>Critical Ordering for Data Consistency: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Prepare  \u2502   \u2502 2. Transfer \u2502   \u2502 3. Send     \u2502   \u2502 4. Send     \u2502\n\u2502 RocksDB     \u2502 \u2192 \u2502 All Files   \u2502 \u2192 \u2502 Metadata    \u2502\u2192  \u2502 COMPLETE    \u2502\n\u2502 Snapshot    \u2502   \u2502 (with MD5)  \u2502   \u2502 (Offset +   \u2502   \u2502 Signal      \u2502\n\u2502             \u2502   \u2502             \u2502   \u2502 SVS)        \u2502   \u2502             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>Why This Ordering is Critical:</p> <p>\u274c Wrong Order (Metadata \u2192 Files \u2192 Complete): - Client updates offset records immediately - If file transfer fails, offset state is corrupted - Need offset/SVS rollback mechanisms - Increased error handling complexity and risk</p> <p>\u2705 Correct Order (Files \u2192 Metadata \u2192 Complete): - Files transferred and validated first - Metadata only sent after successful file transfer - Atomic state update on COMPLETE signal - Less risk consistency guarantee</p> <p>Metadata Consistency Protocol: - Metadata: OffsetRecord + StoreVersionState captured before snapshot creation time - JSON Serialization: Structured metadata transfer with size validation</p>"},{"location":"operations/advanced/p2p-bootstrapping/#traffic-control","title":"\ud83c\udf10 Traffic Control","text":""},{"location":"operations/advanced/p2p-bootstrapping/#global-traffic-shaping","title":"\ud83d\udea5 Global Traffic Shaping","text":"<p>Shared Rate Limiting: Single GlobalChannelTrafficShapingHandler instance controls bandwidth across ALL blob transfer channels globally</p>"},{"location":"operations/advanced/p2p-bootstrapping/#traffic-management-strategy","title":"Traffic Management Strategy","text":"<pre><code>Global Control Architecture:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Single    \u2502   \u2502  Monitor    \u2502   \u2502  Enforce    \u2502\n\u2502 Controller  \u2502 \u2192 \u2502 All Channels\u2502 \u2192 \u2502 Bandwidth   \u2502\n\u2502  Instance   \u2502   \u2502 Globally    \u2502   \u2502  Limits     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ul> <li>Write Limit: Global bytes/sec across all channels</li> <li>Read Limit: Global bytes/sec across all channels</li> <li>Check Interval: 1 second monitoring cycle</li> </ul>"},{"location":"operations/advanced/p2p-bootstrapping/#adaptive-throttling-system","title":"Adaptive Throttling System","text":"<p>Dynamic Rate Adjustment: - Increment/Decrement: 20% - Range: 20% - 200% of base rate - Separate read/write throttlers - Idle threshold handling</p>"},{"location":"operations/advanced/p2p-bootstrapping/#configuration-operations","title":"\ud83c\udf9b\ufe0f Configuration &amp; Operations","text":""},{"location":"operations/advanced/p2p-bootstrapping/#configurations","title":"Configurations","text":"<p>\ud83d\udce5 Receiver/Client Feature Enablement Control</p> <p>Venice Server: - Store Level: <code>blobTransferInServerEnabled</code> - Application Level: <code>blob.transfer.receiver.server.policy</code></p> <p>DaVinci Application: - Store Level: <code>blobTransferEnabled</code></p> <p>\ud83d\udce4 Sender/Server Feature Enablement Control</p> <p>All Applications: - Application Level: <code>blob.transfer.manager.enabled</code></p>"},{"location":"operations/advanced/p2p-bootstrapping/#performance-tuning-parameters","title":"Performance Tuning Parameters","text":"<p>\ud83e\ude9c Thresholds - Offset Lag: Skip blob transfer if not lagged enough - Snapshot TTL: Maintain snapshot freshness - Snapshot Cleanup Interval: Maintain disk storage</p> <p>\ud83c\udfce\ufe0f Bandwidth Limits - Client Read: Client side read bytes per sec - Service Write: Server write bytes per sec - Adaptive Range: 20% - 200% of base rate - Max Concurrent Snapshot User: Control server concurrent serve requests load</p> <p>\u23f0 Timeouts - Transfer Max: Server side max timeout for transferring files - Receive Max: Client side max timeout for receiving files</p>"},{"location":"operations/advanced/p2p-bootstrapping/#summary","title":"\ud83d\udccb Summary","text":""},{"location":"operations/advanced/p2p-bootstrapping/#venice-blob-transfer-key-features-benefits","title":"Venice Blob Transfer: Key Features &amp; Benefits","text":"<p>\ud83d\ude80 Performance Excellence - Intelligent peer discovery and selection - Fast failover with comprehensive error handling - High-performance Netty streaming architecture - Efficient file operations and adaptive chunking</p> <p>\ud83d\udd12 Rock-Solid Reliability - Consistent RocksDB snapshots with point-in-time guarantees - Comprehensive error handling and automatic cleanup - Data integrity validation with MD5 and size checks - Automatic Kafka fallback for 100% data coverage</p> <p>\ud83d\udee1\ufe0f Security &amp; Control - End-to-end SSL/TLS encryption - Certificate-based ACL validation - Global traffic rate limiting and adaptive throttling - Comprehensive timeout and connection management</p> <p>\ud83d\udd27 Operational Excellence - Flexible multi-level configuration - Automated snapshot lifecycle management - Graceful degradation and low risk deployment</p> <p>Result: Faster node recovery, reduced infrastructure load, improved cluster scalability with enterprise-grade reliability</p>"},{"location":"operations/data-management/repush/","title":"Repush","text":"<p>In Venice, there are many things which happen under the hood as part of Full Pushes. Besides serving to refresh data, Full Pushes also serve to make certain configs take effect, and also more generally keep the system tidy and healthy.</p> <p>Many Venice users choose to periodically do Full Pushes for the sake of refreshing their data, in which case the auxiliary goals of Full Pushes end up being fulfilled organically.</p> <p>In other cases, users do Full Pushes either infrequently, or not at all. In these cases, the operator may want to  manually trigger a Repush, without involving the user.</p>"},{"location":"operations/data-management/repush/#use-cases","title":"Use Cases","text":"<p>The various motivations for operator-driven Repushes are described in more details below.</p>"},{"location":"operations/data-management/repush/#reconfiguration","title":"Reconfiguration","text":"<p>In Venice, some store configs can be changed live, while some others take effect when the next store-version is pushed. Configs which can be changed with a Repush (or a regular Full Push) include:</p> <ol> <li>Partition-related settings</li> <li>Partition count</li> <li>Amplification factor</li> <li>Custom partitioner implementation</li> <li>Replication settings</li> <li>Whether a store is hybrid</li> </ol> <p>Having some configs be immutable within the scope of a store-version makes the development and maintenance of the system easier to reason about. In the past, the Venice team has leveraged Repushes to execute large scale migrations as new modes were introduced (e.g., migrating from active-passive to active-active replication, or migrating from the  offline/bootstrap/online state model to the leader/follower state model). Repush enabled operators to roll out (and  occasionally roll back) these migrations in a way that was invisible to users of the platform. Although these migrations  are behind us, it is possible that future migrations may also be designed to be executed this way.</p>"},{"location":"operations/data-management/repush/#sorting","title":"Sorting","text":"<p>When doing a Full Push (and also when Repushing), the data gets sorted inside the Map Reduce job, prior to pushing. The  Start of Push control message is annotated with the fact that the incoming data is sorted, which triggers an alternative  code path within servers (and Da Vinci clients) in order to generate the storage files more efficiently.</p> <p>When doing a Stream Reprocessing (SR), on the other hand, data does not get sorted. More generally, if the last Full  Push happened a while ago, and lots of nearline writes happened since then, there could be a large portion of the data  which is unsorted, and the operator may find that rebalance performance degrades as a result.</p> <p>While it is not considered necessary to perform a Repush after every SR just for the sake of sorting, it can sometimes  be useful if the last SR or Full Push happened quite a while ago.</p>"},{"location":"operations/data-management/repush/#repair","title":"Repair","text":"<p>In some cases where a datacenter (DC) suffered an outage, it is possible that pushes succeeded in some DCs and failed in others. Alternatively, it's possible that correlated hardware failures resulted in some DC losing all replicas of some  partitions. In these cases, as long as one of the DCs is healthy, it can be used as the source of a repush to repair the data in other DCs.</p>"},{"location":"operations/data-management/repush/#time-to-live-ttl","title":"Time to Live (TTL)","text":"<p>It's usually required to evict stale entries from the store in order to achieve GDPR compliance or other business requirements. The repush with TTL functionality will replay and scan through the entries in the current version,  and use store-level rewind time as TTL time to evict stale records based on the write timestamp. See TTL for the comparison among various TTL options.</p>"},{"location":"operations/data-management/repush/#usage","title":"Usage","text":"<p>The following Venice Push Job config is used to enable Repush functionality: <pre><code>source.kafka = true\n</code></pre></p>"},{"location":"operations/data-management/repush/#optional-configs","title":"Optional Configs","text":"<p>The following Venice Push Job configs provide more control over the Repush functionality:</p> <p>To specify which Kafka topic (i.e. which store-version) to use as the source of the Repush. If unspecified, it will default to the highest available store-version. <pre><code>kafka.input.topic = store-name_v7\n</code></pre></p> <p>To specify which datacenter to pull from. If unspecified, it will default to one of the DCs which contains the highest available store-version. <pre><code>kafka.input.fabric = dc-name\n</code></pre></p> <p>To specify the address of the Kafka cluster to pull from. If unspecified, it will use the one returned by the controller of the datacenter chosen to pull from. <pre><code>kafka.input.broker.url = kafka-url:port\n</code></pre></p> <p>To specify the size of the offset range each mapper task will be responsible for. Each Kafka partition is fetched in parallel by multiple mappers, each taking care of distinct ranges. If unspecified, the default is 5M. <pre><code>kafka.input.max.records.per.mapper = 1000000\n</code></pre></p> <p>To specify whether to enable the Combiner of the Map Reduce job, in order to prune the shuffling traffic between mappers and reducers. This is an experimental functionality that has not been vetted for production usage yet. If unspecified, it defaults to false. <pre><code>kafka.input.combiner.enabled = true\n</code></pre></p> <p>To specify whether to enable TTL for the repush job. If unspecified, it defaults to false. <pre><code>repush.ttl.enable = true\n</code></pre> More details on how to configure TTL can be found in the TTL guide.</p>"},{"location":"operations/data-management/repush/#future-work","title":"Future Work","text":"<p>One potential enhancement would be to add support for changing the compression setting of a store, which currently requires a Full Push, and which Repush does not support yet.</p>"},{"location":"operations/data-management/system-stores/","title":"System Stores","text":"<p>Venice has several \"system stores\". These are used to store data and metadata needed by the system itself, in opposition  to \"user stores\" which store the data users are interested in. For the most part, the existence of these system stores  should not matter, but it can be useful for operators to know about them.</p> <p>This page explains the various types of system stores and their function.</p>"},{"location":"operations/data-management/system-stores/#system-store-types","title":"System Store Types","text":"<p>Below are a few distinct types of system stores:</p> Type Cardinality Content Schema system store 1 / deployment Metadata only Global system store 1 / deployment Data Cluster system store 1 / cluster Data Per-store system store 1 / user store Data"},{"location":"operations/data-management/system-stores/#schema-system-stores","title":"Schema System Stores","text":"<p>These are stores created in just one of the clusters of a deployment. They contain no data at all (and therefore no  store-versions) and thus cannot be queried. They only serve to store schemas. These schemas are those which the system needs for its own internal protocols.</p> <p>The schema system stores are used for the sake of protocol forward compatibility. When deploying Venice code, its jars  contain resource files for the current protocol versions as well as all previous protocol versions, but not future  protocol versions. Given that Venice is a distributed system, it is important to have the flexibility of deploying  different versions of the software across different nodes. When a node running a newer version of the software persists data written with a newer protocol version, and that data is then read by another node running an older version of the software which does not know about that newer protocol version, then it can query the schema of the appropriate system store and use this to decode the newer payload, and translate them into the version they understand (a process known as Schema Evolution).</p>"},{"location":"operations/data-management/system-stores/#global-system-stores","title":"Global System Stores","text":"<p>These are stores created in just one of the clusters of a deployment, but in opposition to schema system stores, they do contain data. Their global nature makes them convenient to store data about all clusters, but it also means that the clusters are not isolated from one another as far as this kind of system store is concerned. Therefore, it is best for this type of system store to serve only in \"best effort\" purposes, so that if for any reason they become unavailable, critical operations can still continue. An example of this would be to store heuristics or other non-critical data. These include:</p> <ul> <li>Push Job Details Store</li> <li>Batch Job Heartbeat Store</li> </ul>"},{"location":"operations/data-management/system-stores/#cluster-system-stores","title":"Cluster System Stores","text":"<p>These are stores created once per cluster, and which do contain data. It is appropriate to use such stores for critical operations, given that one cluster's availability issues would not bleed into other clusters. These include:</p> <ul> <li>Participant Store</li> </ul>"},{"location":"operations/data-management/system-stores/#per-store-system-stores","title":"Per-store System Stores","text":"<p>These are stores created once per user store, and which do contain data. These include:</p> <ul> <li>Da Vinci Push Status Store</li> <li>Meta Store</li> </ul>"},{"location":"operations/data-management/system-stores/#system-store-list","title":"System Store List","text":"<p>Below is a list of actual system stores, along with their type and function (excluding schema stores):</p> Name Type Function Push Job Details Store Global Stores historical status and characteristics about all Push Jobs that ever ran. Batch Job Heartbeat Store Global Stores heatbeats emitted from running push jobs, in order to detect if any push job terminated abruptly without cleaning itself up. Participant Store Cluster Intended as a signaling mechanism for commands emitted by controllers, to be executed by servers, including kill job commands. Da Vinci Push Status Store Per-store Stores per-instance consumption status of Da Vinci Clients. Written to by DVC, and read by controllers, in order to determine whether push jobs have succeeded. Meta Store Per-store Stores a copy of the store config (also found in Zookeeper). Written to by controllers, and read by DVC. Deprecated, will be removed eventually."},{"location":"resources/api-reference/","title":"API Reference","text":""},{"location":"resources/api-reference/#javadoc","title":"Javadoc","text":"<p>To browse the generated Java API reference locally:</p> <pre><code>./gradlew aggregateJavadoc\nmkdocs build\nopen site/javadoc/index.html\n</code></pre> <p>The published site also includes the latest Javadoc at <code>https://venicedb.org/javadoc/</code> once the GitHub Actions deployment completes.</p>"},{"location":"resources/api-reference/#code-examples","title":"Code Examples","text":"<p>See the quickstart guides for code examples.</p>"},{"location":"resources/community/","title":"Community","text":""},{"location":"resources/community/#get-help","title":"Get Help","text":"<ul> <li>Slack: slack.venicedb.org</li> <li>GitHub Issues: github.com/linkedin/venice/issues</li> <li>LinkedIn Group: Venice Community</li> </ul>"},{"location":"resources/community/#follow-updates","title":"Follow Updates","text":"<ul> <li>Blog: blog.venicedb.org</li> <li>YouTube: @venicedb</li> <li>X/Twitter: @VeniceDataBase</li> <li>Bluesky: venicedb.org</li> <li>LinkedIn: venicedb</li> </ul>"},{"location":"resources/learn-more/","title":"Learn More","text":""},{"location":"resources/learn-more/#learn-more","title":"Learn More","text":"<p>This page lists the Venice content published online. Please keep in mind that older content reflects an earlier phase of  the project and may not be entirely correct anymore.</p>"},{"location":"resources/learn-more/#posts","title":"Posts","text":"<p>The following blog posts have previously been published about Venice:</p> <ul> <li>2015: Prototyping Venice: Derived Data Platform</li> <li>2017: Building Venice with Apache Helix</li> <li>2017: Building Venice: A Production Software Case Study</li> <li>2017: Venice Hybrid: Doing Lambda Better</li> <li>2018: Venice Performance Optimization</li> <li>2021: Taming memory fragmentation in Venice with Jemalloc</li> <li>2022: Supporting large fanout use cases at scale in Venice</li> <li>2022: Open Sourcing Venice \u2013 LinkedIn\u2019s Derived Data Platform</li> <li>2023: Public CI</li> <li>2023: Stable Releases</li> <li>2024: VeniceCon 2024</li> <li>2024: Stable Release 0.4.263</li> </ul> <p>Follow us on HashNode or subscribe to the newsletter  to hear about it when new posts are published.</p>"},{"location":"resources/learn-more/#talks","title":"Talks","text":"<p>The following talks have been given about Venice:</p> <ul> <li>2019: People You May Know: Fast Recommendations over Massive Data</li> <li>2019: Enabling next generation models for PYMK Scale</li> <li>2022: Open Sourcing Venice</li> <li>Strange Loop 2022 (original)</li> <li>Big Data Montreal 65 (includes Q&amp;A)</li> <li>2023: What is Derived Data? (and Do You Already Have Any?)</li> <li>QCon London 2023 (original)</li> <li>University lecture at ETS (in French, includes Q&amp;A)</li> <li>2023: Partial Updates in Venice</li> <li>2023: When Only the Last Writer Wins We All Lose: Active-Active Geo-Replication in Venice</li> <li>2023: The Journey to a Million Ops / Sec / Node in Venice</li> <li>2024: Lessons Learned from Building LinkedIn\u2019s AI Data Platform</li> <li>QCon London 2024 (original)</li> <li>VeniceCon 2024: part 1, part 2</li> <li>University lecture at ETS (in French, includes Q&amp;A)</li> <li>2024: TLA+ @ LinkedIn: Ambry and Venice</li> <li>2024: What Venice Shipped in the Last Year</li> <li>2024: User Stories: Trust and Privacy Filtering using Venice</li> <li>2024: Fast Client: Venice Next-Gen Read Path</li> <li>2025: Da Vinci: Pumping Streams into RocksDB and DuckDB</li> </ul> <p>Follow us on YouTube to hear about it when new content is published on that  platform.</p>"},{"location":"resources/learn-more/#podcasts","title":"Podcasts","text":"<p>The following interviews have been given about Venice:</p> Year Channel Youtube Apple Spotify 2022 JUXT Cast 2022 SaaS Developer Community 2023 Monday Morning Data Chat 2023 Software Misadventures 2024 The Geek Narrator 2025 InfoQ"},{"location":"user-guide/","title":"User Guide","text":"<p>Documentation for Venice users.</p> <p></p>"},{"location":"user-guide/#sections","title":"Sections","text":"<ul> <li>Concepts - Core Venice concepts</li> <li>Write APIs - Batch, streaming, and online writes</li> <li>Read APIs - Thin, Fast, and Da Vinci clients</li> <li>Design Patterns - Common usage patterns</li> </ul>"},{"location":"user-guide/concepts/overview/","title":"Venice Concepts","text":"<p>Core architecture and design principles.</p> <p></p> <p>Venice straddles the offline, nearline and online worlds, providing:</p> <ul> <li>Batch ingestion from Hadoop</li> <li>Stream ingestion from Kafka/Samza</li> <li>Online writes via HTTP</li> <li>Distributed storage with active-active replication</li> <li>Flexible reads from stateless to stateful clients</li> </ul> <p>For more details, see the main documentation.</p>"},{"location":"user-guide/concepts/ttl/","title":"Time to Live (TTL)","text":""},{"location":"user-guide/concepts/ttl/#use-cases","title":"Use Cases","text":"<p>TTL can serve as a storage efficiency optimization for use cases where the data is transient in nature, and not useful beyond a certain age.</p> <p>TTL can also be useful in certain scenarios where the data should not be retained longer than prescribed for compliance reasons. Note that for these scenarios, it is important to understand (as explained above) that the rewind time is the  minimum, not maximum, TTL. It is also advisable to schedule the periodic purging such that there is a margin of safety  in case of infra delays or failures.</p>"},{"location":"user-guide/concepts/ttl/#usage","title":"Usage","text":"<p>There are two ways to achieve TTL compliance, via Repush with TTL or via empty push. The major differences between both approaches is the original data source and how real-time buffer is replayed.</p> Source Data for TTL Real-time buffer replay Empty push None Replay real-time buffer with <code>hybrid-rewind-seconds</code> config Repush with TTL Existing version topic Replay real-time buffer with <code>rewind.time.in.seconds.override</code> config"},{"location":"user-guide/concepts/ttl/#repush-with-ttl","title":"Repush with TTL","text":"<p>Repush with TTL runs as a VenicePushJob that reads the data in the existing version of the store,  and writes it back as a new version. The TTL is enforced by the VenicePushJob, which only writes records that are younger than the specified TTL. Repush with TTL can also be configured and scheduled periodically to enforce TTL.</p> <p>The store-level rewind time (in hybrid store configs) is used as the default TTL time. A custom TTL time can be specified by setting one of the following configs on the VenicePushJob:</p> Config TTL behavior <code>repush.ttl.seconds</code> Records older than the specified value will expire <code>repush.ttl.start.timestamp</code> Records that were written before the specified timestamp will expire <p>The <code>rewind.time.in.seconds.override</code> is a configurable value in push job (Default: 24 hours).</p> <p>This brings two major benefits: 1. The repush job de-dupes writes to the same key, so that the servers need to ingest fewer events. 2. The repush job sorts the data, thus allowing servers to ingest in a more optimized way.</p>"},{"location":"user-guide/concepts/ttl/#how-to-enable","title":"How to enable","text":"<p>In order to enable repush with ttl, a VenicePushJob needs to be configured with the following configs: <pre><code>source.kafka = true\nrepush.ttl.enable = true\n</code></pre> For more options related to repush, refer to the Repush guide.</p>"},{"location":"user-guide/concepts/ttl/#empty-push","title":"Empty push","text":"<p>For Venice stores that receive nearline writes, it is possible to set up a periodic purge of records that are older than some threshold.</p> <p>The way this works is by leveraging the real-time buffer replay mechanism of hybrid stores. As part of pushing a new store-version, after the push finishes, but before swapping the read traffic over to the new version, there is a buffer replay phase where the last N seconds of buffered real-time data is replayed. The replayed data gets overlaid on top of the data from the Full Push.</p> <p>This can be leveraged in order to enforce TTL on the dataset. By scheduling periodic empty pushes and configuring how far back to replay the real-time data (N), one can control the TTL parameters. The time to live is defined by N, which acts as a \"minimum TTL\", while the \"maximum TTL\" is N + delay between each empty push. For example, if you schedule a daily empty push, and N = 6 days, then the oldest data in your store will be at least 6 days old, and at most 7 days old.</p> <p>N can be set up by configuring the <code>hybrid-rewind-seconds</code> using venice-admin-tool.</p>"},{"location":"user-guide/concepts/ttl/#how-to-enable_1","title":"How to enable","text":"<p>At the moment, there are two ways to perform empty pushes: 1. Via the <code>empty-push</code> command in the admin tool. 2. Via the Venice Push Job, executed from a Hadoop grid, but with an input directory that contains a file with no    records in it.</p>"},{"location":"user-guide/design-patterns/","title":"Design Patterns","text":"<p>Venice is a flexible toolkit that can be used in a variety of ways. This section documents various design patterns that can be implemented by Venice users.</p> <ul> <li>Merging Batch and Real-Time Data</li> </ul>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/","title":"Merging Batch &amp; Real-Time Data","text":"<p>Venice being a derived data platform, an important category of use cases is to merge batch data sources and real-time data sources. This is a field where the industry has come up with multiple patterns over the past decade and a half. This page provides an overview of all these patterns, and how they can be implemented in Venice. The patterns are presented in the order they were published over the years.</p> <p>While it is useful to understand the history of how things were done in the past and how they have evolved over time, in practice, most Venice users choose the Hybrid Store design pattern. Impatient readers may choose to go directly to that section if they wish to skip the historical context.</p>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/#lambda-architecture","title":"Lambda Architecture","text":"<p>The Lambda Architecture was proposed by Nathan Marz in 2011, in a blog post oddly titled How to beat the CAP theorem. While the post describes what has come to be known as the \"lambda architecture\", it does not actually mention it by that name. Whether the presented ideas actually \"beat\" the CAP theorem, or merely side-step it in certain contexts is debatable, but it is ultimately unimportant to the subject we are interested in.</p> <p>In a nutshell, the idea is to have two parallel pipelines, one for batch, and one for data. Both of these pipelines are going to perform processing and serving of their respective data, each using specialized technology for the job. Only at the periphery are the two pipelines merged together, presumably within the user's own application.</p> <p></p>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/#implementing-the-lambda-architecture-in-venice","title":"Implementing the Lambda Architecture in Venice","text":"<p>The Lambda Architecture, exactly as it is proposed by Nathan Marz, can be implemented using two Venice stores: a  batch-only one, and a real-time-only one. The application can query both stores and implement whatever arbitrary  reconciliation logic they wish.</p>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/#kappa-architecture","title":"Kappa Architecture","text":"<p>The Kappa Architecture was proposed by Jay Kreps in 2014, in a blog post titled Questioning the Lambda Architecture.</p> <p>In a nutshell, the idea is that the Lambda Architecture's two parallel pipelines are cumbersome to maintain, especially  given that they would be implemented in different technologies (some batch processing framework and a stream processing one). The Kappa Architecture, on the other hand, proposes to use only a stream processor, but to configure it to run either in \"real-time processing\" or in \"historical reprocessing\" mode. Whenever data needs to be reprocessed, a new  pipeline can be instantiated, configured to start from the beginning of the historical input, output all of its  processed data into a new instance of a database, and then keep going to process real-time events after that. The  application can then switch over its read traffic to the new database instance.</p> <p></p>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/#implementing-the-lambda-architecture-in-venice_1","title":"Implementing the Lambda Architecture in Venice","text":"<p>The Kappa Architecture, exactly as it is proposed by Jay Kreps, can again be implemented using two Venice stores, except that both of them are real-time-only. The user does need to manually keep creating new stores and manually switch over  the reads to the new one when ready. It does achieve the purpose of maintaining only a stream processing stack, and it gives precise control over which version of the data to query, but it may be more tedious on an ongoing basis as  compared to the Hybrid Store (see below).</p>"},{"location":"user-guide/design-patterns/merging-batch-and-rt/#hybrid-store","title":"Hybrid Store","text":"<p>The Hybrid Store Design Pattern has been described both in blog and in video (1, 2)  formats.</p> <p>Venice supports this pattern out of the box, and it is likely the simplest way to merge batch and real-time data, as Venice handles the whole orchestration on behalf of the user.</p>"},{"location":"user-guide/read-apis/","title":"Read APIs","text":"<p>Venice provides multiple client types:</p> <ul> <li>Da Vinci Client - Local caching for ultra-low latency</li> </ul> <p>Additional clients (Thin Client, Fast Client) documentation coming soon.</p>"},{"location":"user-guide/read-apis/da-vinci-client/","title":"Da Vinci Client","text":"<p>This allows you to eagerly load some or all partitions of the dataset and perform queries against the resulting local  cache. Future updates to the data continue to be streamed in and applied to the local cache.</p>"},{"location":"user-guide/read-apis/da-vinci-client/#record-transformer-dvrt","title":"Record Transformer (DVRT)","text":"<p>The Record Transformer lets you hook into Da Vinci's data ingestion process to react to every record change in real-time.</p>"},{"location":"user-guide/read-apis/da-vinci-client/#what-does-it-do","title":"What Does It Do?","text":"<ul> <li>React to changes: Get notified when records are added, updated, or deleted</li> <li>Transform data: Modify records as they come in (add fields, filter data, etc.)</li> <li>Forward data: Send Venice data to other systems (databases, search indexes, analytics)</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"user-guide/read-apis/da-vinci-client/#step-1-implement-the-interface","title":"Step 1: Implement the Interface","text":"<p>Extend DaVinciRecordTransformer and implement:</p> <ul> <li><code>transform</code> - Transform data before local persistence, returns DaVinciRecordTransformerResult:</li> <li><code>UNCHANGED</code> - Keep original value</li> <li><code>TRANSFORMED</code> - Use new transformed value  </li> <li><code>SKIP</code> - Drop this record entirely</li> <li><code>processPut</code> - Handle record updates</li> <li><code>processDelete</code> - Handle deletions (optional)</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#step-2-configure-and-register","title":"Step 2: Configure and Register","text":"<p>Build a DaVinciRecordTransformerConfig and register it:</p> <pre><code>DaVinciRecordTransformerConfig config = new DaVinciRecordTransformerConfig.Builder()\n    .setRecordTransformerFunction(YourTransformer::new)\n    .build();\n\nDaVinciConfig daVinciConfig = new DaVinciConfig();\ndaVinciConfig.setRecordTransformerConfig(config);\n</code></pre> <p>For Custom Constructor Parameters: If you need to pass additional parameters to your transformer's constructor beyond the default ones provided:</p> <pre><code>// Your custom parameter\nString databasePath = \"/my/path\";\n\nDaVinciRecordTransformerFunctionalInterface transformerFunction = (\n    // Venice-provided parameters:\n    storeName, storeVersion, keySchema, inputValueSchema, outputValueSchema, config) -&gt; \n    new YourTransformer(\n        // Venice-provided parameters\n        storeName, storeVersion, keySchema, inputValueSchema, outputValueSchema, config,\n        // Your custom parameter\n        databasePath);\n\nDaVinciRecordTransformerConfig config = new DaVinciRecordTransformerConfig.Builder()\n    .setRecordTransformerFunction(transformerFunction)\n    .build();\n</code></pre>"},{"location":"user-guide/read-apis/da-vinci-client/#key-concepts","title":"Key Concepts","text":""},{"location":"user-guide/read-apis/da-vinci-client/#version-management","title":"Version Management","text":"<p>Venice stores have versions. When new data is pushed, Venice creates a future version while the current version continues serving traffic. Once ready, Venice atomically swaps the future version to become the new current version.</p>"},{"location":"user-guide/read-apis/da-vinci-client/#for-record-transformers","title":"For Record Transformers:","text":"<ul> <li>Each version gets its own transformer instance</li> <li>During a push, you'll have transformers for both current and future versions running in parallel </li> <li>Use <code>onStartVersionIngestion(partitionId, isCurrentVersion)</code> to initialize resources</li> <li>Use <code>onEndVersionIngestion(currentVersion)</code> to clean up when a version stops serving</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#best-practice-separate-data-by-version","title":"Best Practice: Separate Data by Version","text":"<p>When propagating Venice data to external systems (databases, search indexes, etc.), always separate data from different versions into independent storage locations. Think of it as maintaining one database table per Venice store version.</p>"},{"location":"user-guide/read-apis/da-vinci-client/#why-version-separation-matters","title":"Why Version Separation Matters:","text":"<ul> <li>Prevents data races: Multiple versions writing to the same table creates race conditions</li> <li>Avoids record leaks: Old version data won't pollute your current dataset</li> <li>Enables clean transitions: You can atomically switch to new data once ready</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#implementation-strategy","title":"Implementation Strategy:","text":"<ol> <li>Create version-specific storage (e.g., <code>user_profiles_v1</code>, <code>user_profiles_v2</code>)</li> <li>Maintain a pointer to current version (database views, atomic pointer, etc.)</li> <li>Switch pointer atomically when Venice promotes a new current version</li> <li>Clean up old versions once no longer needed</li> </ol> <p>See the example below:</p> <pre><code>AtomicBoolean setUpComplete = new AtomicBoolean();\nString tableName = getStoreName() + \"_v\" + getStoreVersion();\nString currentVersionViewName = getStoreName() + \"_current_version\";\n\n@Override\nsynchronized public void onStartVersionIngestion(int partitionId, boolean isCurrentVersion) {\n  if (setUpComplete.get()) {\n    return;\n  }\n\n  // Initialize resources for this version\n  if (!externalDB.containsTable(tableName)) {\n    externalDB.createTable(tableName);\n  }\n\n  // Maintain pointer to current version\n  if (isCurrentVersion) {\n    externalDB.createOrReplaceView(currentViewName, \"SELECT * FROM \" + tableName);\n  }\n  setUpComplete.set(true);\n}\n\n@Override\npublic void onEndVersionIngestion(int currentVersion) {\n  // Only clean up if this version is no longer serving\n  if (currentVersion != getStoreVersion()) {\n    String newCurrentTableName = getStoreName() + \"_v\" + currentVersion;\n\n    // Update view to point to new current version\n    externalDB.createOrReplaceView(currentViewName, \"SELECT * FROM \" + newCurrentTableName);\n\n    // Delete old version\n    externalDB.dropTable(tableName);\n  }\n}\n</code></pre>"},{"location":"user-guide/read-apis/da-vinci-client/#key-behaviors","title":"Key Behaviors","text":"<ul> <li>Lazy deserialization: Keys/values are deserialized lazily to avoid unnecessary CPU/memory overhead if you only need     to inspect some records or parameters</li> <li>Startup replay: Venice replays existing records from disk on startup to rebuild external state</li> <li>Compatibility checks: Implementation changes are automatically detected and local state is rebuilt to prevent stale data</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#featured-implementations","title":"Featured Implementations","text":"<ul> <li>VeniceChangelogConsumerDaVinciRecordTransformerImpl (source)<ul> <li>Powers the change data capture (CDC) client through the record transformer APIs.</li> </ul> </li> <li>DuckDBDaVinciRecordTransformer (source)<ul> <li>Streams Venice updates into DuckDB so teams can query feature data with SQL.</li> </ul> </li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#configuration-options","title":"Configuration Options","text":""},{"location":"user-guide/read-apis/da-vinci-client/#required","title":"Required","text":"<ul> <li><code>setRecordTransformerFunction</code>: Factory that creates transformer instances for each store version.</li> </ul>"},{"location":"user-guide/read-apis/da-vinci-client/#optional","title":"Optional","text":"<ul> <li><code>setKeyClass</code>: Deserialize keys into Avro <code>SpecificRecord</code> implementations.</li> <li><code>setOutputValueClass</code> and <code>setOutputValueSchema</code>: Provide both when changing value types or using Avro <code>SpecificRecord</code> values.</li> <li><code>setStoreRecordsInDaVinci</code> (default: <code>true</code>): Persist transformed data to Da Vinci\u2019s local storage.</li> <li><code>setAlwaysBootstrapFromVersionTopic</code> (default: <code>false</code>): Enable when <code>setStoreRecordsInDaVinci</code> is <code>false</code> and you rely on in-memory state.</li> <li><code>setRecordTransformationEnabled</code> (default: <code>true</code>): Disable to bypass <code>transform</code> while leaving the transformer registered.</li> <li><code>setRecordMetadataEnabled</code> (default: <code>false</code>): Include metadata via <code>DaVinciRecordTransformerRecordMetadata</code> for auditing.</li> </ul>"},{"location":"user-guide/write-apis/","title":"Write APIs","text":"<p>Venice supports multiple write patterns:</p> <ul> <li>Batch Push - Full dataset swaps from Hadoop</li> <li>Stream Processor - Real-time updates from Samza</li> <li>Online Producer - Direct writes from services</li> </ul>"},{"location":"user-guide/write-apis/batch-push/","title":"Push Job","text":"<p>The diagram below illustrates the high-level picture of the push job. (Link to diagram source)</p> <p></p> <p>The Push Job takes data from a Hadoop grid and writes it to Venice. </p>"},{"location":"user-guide/write-apis/batch-push/#use-cases","title":"Use Cases","text":"<p>There are two modes the Push Job can run in:</p> <ul> <li>Full Push (default)</li> <li>Incremental Push</li> </ul>"},{"location":"user-guide/write-apis/batch-push/#full-push","title":"Full Push","text":"<p>When performing a Full Push, the user takes advantage of the fact that Venice's datasets are versioned. A Full Push triggers the dynamic creation of a new dataset version, and then loads data into it in the background. The new dataset  version is called a \"future\" version as long as data is still loading, and during that time, no online read traffic will be served from it. When the loading is determined to have successfully completed in a given datacenter, the new dataset  version transitions from \"future\" to \"current\", whereas the old dataset version transitions from \"current\" to \"backup\".  When a dataset version becomes current, online read traffic gets routed to it.</p>"},{"location":"user-guide/write-apis/batch-push/#incremental-push","title":"Incremental Push","text":"<p>When performing an Incremental Push, no new dataset versions are created, and data gets added into all existing versions  of the dataset. This leverages the same mechanism as Streaming Writes, and requires that the store be configured as  Hybrid.</p>"},{"location":"user-guide/write-apis/batch-push/#targeted-region-push","title":"Targeted Region Push","text":"<p>Technically, targeted region push is an option of full push (hence not a new mode), but it allows writing data into a  subset of global regions/data centers, whereas full push writes globally at once.</p> <p>By default, it automatically pushes data to the rest of unspecified regions after the targeted region push is completed. We are working on to implement more validations in between to ensure the targeted regions are healthy after the first push.  Users may turn off this automation and perform validations and chain it with another full push/targeted region push to  achieve the same effect as full push in terms of data integrity, but the store versions across different regions might be  not the same depending on the exact setup.</p>"},{"location":"user-guide/write-apis/batch-push/#usage","title":"Usage","text":"<p>The Push Job is designed to require as few configs as possible. The following mandatory configs should be unique to each use case, and set by the user:</p> <ul> <li><code>venice.store.name</code>: The name of the Venice store to push into.</li> <li><code>input.path</code>: The HDFS path containing the data to be pushed, populated with one or many Avro files, where each file    contains a sequence of records, and where each record has a <code>key</code> field and a <code>value</code> field.</li> </ul> <p>In addition to use case-specific configs, there are also some necessary configs that would typically be the same for all  use cases in a given environment (e.g., one value for production, another value for staging, etc.). The following can  therefore be configured globally by the operator, in order to make the Push Job even easier to leverage by users:</p> <ul> <li><code>venice.discover.urls</code>: The URL of the Venice controller.</li> </ul>"},{"location":"user-guide/write-apis/batch-push/#optional-configs","title":"Optional Configs","text":"<p>The user may choose to specify the following configs:</p> <ul> <li><code>incremental.push</code>: Whether to run the job in incremental mode. Default: <code>false</code></li> <li><code>key.field</code>: The name of the key field within the input records. Default: <code>key</code></li> <li><code>value.field</code>: The name of the value field within the input records. Default: <code>value</code></li> <li><code>allow.duplicate.key</code>: Whether to let the Push Job proceed even if it detects that the input contains multiple records    having the same key but distinct values. If set to <code>true</code>, then the Push Job picks one of the values to be written in   a non-deterministic fashion. Default: <code>false</code></li> <li><code>extended.schema.validity.check.enabled</code>: Whether to perform extended schema validation on the input (equivalent to   the <code>STRICT</code> mode in avro-util's SchemaParseConfiguration).    If set to <code>false</code>, it becomes equivalent to avro-util's <code>LOOSE</code> mode. Default: <code>true</code></li> <li><code>targeted.region.push.enabled</code>: Whether to perform targeted region push. Default: <code>false</code></li> <li><code>targeted.region.push.list</code>: This config takes effect only when targeted region push flag is enabled.    Optionally specify a list of target region(s) to push data into. See full details at    TARGETED_REGION_PUSH_LIST.</li> <li><code>post.validation.consumption</code>: Whether to perform post validation consumption after targeted region push is finished.     Default: <code>true</code>. Set this to <code>false</code> if you want to achieve a true single colo push.</li> </ul> <p>The push job also supports using D2 URLs for automated controller service discovery. To use this, the user or operator must specify the following configs:</p> <ul> <li><code>multi.region</code>: <code>true</code> if the Venice cluster is deployed in a multi-region mode; <code>false</code> otherwise</li> <li><code>venice.discover.urls</code>: The D2 URL of the Venice controller. It must be of the form <code>d2://&lt;D2 service name of the controller&gt;</code></li> <li><code>d2.zk.hosts.&lt;region name&gt;</code>: The Zookeeper addresses where the components in the specified region are announcing themselves to D2</li> <li>If <code>multi.region</code> is <code>true</code>:</li> <li><code>venice.discover.urls</code> must use the D2 service name of the parent controller</li> <li><code>parent.controller.region.name</code> must denote the name of the datacenter where the parent controller is deployed</li> <li><code>d2.zk.hosts.&lt;parent controller region&gt;</code> is a mandatory config</li> <li>If <code>multi.region</code> is <code>false</code></li> <li><code>venice.discover.urls</code> must use the D2 service name of the child controller</li> <li><code>source.grid.fabric</code> must denote the name of the datacenter where Venice is deployed</li> <li><code>d2.zk.hosts.&lt;source grid fabric&gt;</code> is a mandatory config</li> </ul> <p>The user or operator may want to specify the following security-related configs:</p> <ul> <li><code>venice.ssl.enable</code>. Default: <code>false</code></li> <li><code>ssl.configurator.class</code>. Default: <code>com.linkedin.venice.hadoop.ssl.TempFileSSLConfigurator</code></li> <li><code>ssl.key.store.property.name</code></li> <li><code>ssl.trust.store.property.name</code></li> <li><code>ssl.key.store.password.property.name</code></li> <li><code>ssl.key.password.property.name</code></li> </ul>"},{"location":"user-guide/write-apis/online-producer/","title":"Online Producer","text":"<p>The Online Producer enables online applications to write data to a Venice store directly. All writes are still asynchronous, and data is only eventually consistent. However, the APIs guarantee durability of the data if the operation is successful.</p>"},{"location":"user-guide/write-apis/online-producer/#prerequisites","title":"Prerequisites","text":"<p>To use the Online Producer, the store must meet some prerequisites: 1. It must not have writes disabled 2. It must be a hybrid store 3. It must have a current version.</p> <p>In addition to the store-level prerequisites, the current version must meet the following prerequisites: 1. It must be configured as hybrid; aka capable of receiving near-line writes 2. It must specify either <code>ACTIVE_ACTIVE</code> or <code>NON_AGGREGATE</code> data-replication policies 3. It must specify a partitioner that the writer application knows how to use</p>"},{"location":"user-guide/write-apis/online-producer/#api","title":"API","text":"<p>Detailed Javadocs for the Online Producer API can be accessed here. All of these APIs have at least two versions - one that accepts a logical timestamp and one that doesn't. 1. Logical timestamps (in ms) are what Venice backend will use to resolve conflicts in case multiple writes modify the same record. An update to Venice could be triggered due to some trigger that can be attributed to a specific point in time. In such cases, it might be beneficial for applications to mark their updates to Venice with that timestamp and Venice will persist the record as if it had been received at that point in time - either by applying the update, dropping the update if a newer update has already been persisted, or applying an update partially only to fields that have not received an update with a newer timestamp yet. 2. In case the write requests are made without specifying the logical timestamp, then the time at which the message was produced is used as the logical timestamp during conflict resolution.</p>"},{"location":"user-guide/write-apis/online-producer/#usage","title":"Usage","text":"<p>To create an instance of the producer, <code>OnlineProducerFactory</code> should be used since the interface for <code>OnlineVeniceProducer</code> is not yet considered stable and can introduce backward incompatible changes. <pre><code>String storeName = \"&lt;store_name&gt;\";\nClientConfig clientConfig = ClientConfig.defaultGenericClientConfig(storeName)\n    .setVeniceURL(\"http://router.host:7777\")\n    ...;\nVeniceProperties producerConfig = VeniceProperties.empty();\nOnlineVeniceProducer producer = OnlineProducerFactory.createProducer(clientConfig, producerConfig, null);\n\nproducer.asyncPut(key, value).get();\nproducer.asyncDelete(key).get();\nproducer.asyncUpdate(key, builder -&gt; {\n    ((UpdateBuilder) updateBuilderObj)\n        .setNewFieldValue(FIELD_NUMBER, 10L);\n        .setNewFieldValue(FIELD_COMPANY, \"LinkedIn\")\n        ...;\n}).get();\n</code></pre></p>"},{"location":"user-guide/write-apis/online-producer/#optional-configs","title":"Optional Configs","text":"<p>The online Venice producer can be configured by specifying the following optional configs:</p> <p>To specify the number of threads dedicated for the online Venice producer. This also controls the number of concurrent write operations in each producer: <pre><code>client.producer.thread.num = 10\n</code></pre></p> <p>To specify the interval at which the client will refresh its schema cache: <pre><code>client.producer.schema.refresh.interval.seconds = 300\n</code></pre></p>"},{"location":"user-guide/write-apis/stream-processor/","title":"Stream Processor","text":"<p>Data can be produced to Venice in a nearline fashion, from stream processors. The best supported stream processor is Apache Samza though we intend to add first-class support for other stream processors in the future. The difference between using a stream processor library and the Online Producer library is that a stream  processor has well-defined semantics around when to ensure that produced data is flushed and a built-in mechanism to  checkpoint its progress relative to its consumption progress in upstream data sources, whereas the online producer  library is a lower-level building block which leaves these reliability details up to the user.</p> <p>For Apache Samza, the integration point is done at the level of the VeniceSystemProducer and VeniceSystemFactory.</p> <p>More details to come.</p>"}]}