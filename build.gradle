import com.github.spotbugs.snom.SpotBugsTask
import org.apache.tools.ant.taskdefs.condition.Os
import org.gradle.internal.logging.text.StyledTextOutput.Style
import org.gradle.internal.logging.text.StyledTextOutputFactory

import java.nio.file.Files


plugins {
  id 'idea'
  id 'java'
  id 'maven-publish'
  id 'com.diffplug.spotless' version '6.12.0'
  id 'com.dorongold.task-tree' version '2.1.0'
  id 'com.github.johnrengelman.shadow' version '6.1.0' apply false
  id 'com.github.spotbugs' version '4.8.0' apply false
  id 'org.gradle.test-retry' version '1.5.0' apply false
  id 'com.form.diff-coverage' version '0.9.5' apply false
}

apply from: "$rootDir/gradle/helper/git.gradle"
apply from: "$rootDir/gradle/helper/publishing.gradle"

/*
 * This snippet allows the Gradle environment to be overridden with custom
 * settings. This is useful in environments (such as a private company) where a
 * third party wishes to use custom repositories, or inject certain
 * functionality into the default Gradle build lifecycle.
 */
if (project.hasProperty('overrideBuildEnvironment')) {
  apply from: file(project.overrideBuildEnvironment)
  product {
    codeQuality {
      ignoreFailures = true
    }
  }
}

def avroVersion = '1.9.2'
def avroUtilVersion = '0.2.136'
def kafkaGroup = 'com.linkedin.kafka'
def kafkaVersion = '2.4.1.65'
def log4j2Version = '2.17.1'
def pegasusVersion = '29.31.0'
def jacksonVersion = '2.13.3'

ext.libraries = [
    avro: 'org.apache.avro:avro:' + avroVersion,
    avroCompiler: 'org.apache.avro:avro-compiler:' + avroVersion,
    avroMapred: 'org.apache.avro:avro-mapred:' + avroVersion,
    avroUtilBuilder: 'com.linkedin.avroutil1:builder:' + avroUtilVersion,
    avroUtilCompatHelper: 'com.linkedin.avroutil1:helper-all:' + avroUtilVersion,
    avroUtilFastserde: 'com.linkedin.avroutil1:avro-fastserde:' + avroUtilVersion,
    avroUtilSpotbugsPlugin: 'com.linkedin.avroutil1:spotbugs-plugin:0.2.69',
    bouncyCastle: 'org.bouncycastle:bcprov-jdk15on:1.55',
    caffeine: 'com.github.ben-manes.caffeine:caffeine:2.8.5',
    classgraph: 'io.github.classgraph:classgraph:4.8.60',
    commonsCodec: 'commons-codec:commons-codec:1.4',
    commonsConfiguration: 'commons-configuration:commons-configuration:1.9',
    commonsIo: 'commons-io:commons-io:2.11.0',
    commonsCli: 'commons-cli:commons-cli:1.5.0',
    commonsLang: 'commons-lang:commons-lang:2.6',
    conscrypt: 'org.conscrypt:conscrypt-openjdk-uber:2.4.0',
    d2: 'com.linkedin.pegasus:d2:' + pegasusVersion,
    failsafe: 'net.jodah:failsafe:2.4.0',
    fastUtil: 'it.unimi.dsi:fastutil:8.3.0',
    hadoopCommon: 'org.apache.hadoop:hadoop-common:2.3.0',
    helix: 'org.apache.helix:helix-core:1.0.4',
    httpAsyncClient: 'org.apache.httpcomponents:httpasyncclient:4.1.2',
    httpClient5: 'org.apache.httpcomponents.client5:httpclient5:5.2.1',
    httpCore5: 'org.apache.httpcomponents.core5:httpcore5:5.2.1',
    httpCore5H2: 'org.apache.httpcomponents.core5:httpcore5-h2:5.2.1',
    httpClient: 'org.apache.httpcomponents:httpclient:4.5.2',
    httpCore: 'org.apache.httpcomponents:httpcore:4.4.5',
    jacksonCore: 'com.fasterxml.jackson.core:jackson-core:' + jacksonVersion,
    jacksonAnnotations: 'com.fasterxml.jackson.core:jackson-annotations:' + jacksonVersion,
    jacksonDatabind: 'com.fasterxml.jackson.core:jackson-databind:' + jacksonVersion,
    javax: 'javax.servlet:javax.servlet-api:3.1.0',
    javaxActivation: 'com.sun.activation:javax.activation:1.2.0',
    jdom: 'org.jdom:jdom:1.1',
    jmhCore: 'org.openjdk.jmh:jmh-core:1.28',
    jmhGenerator: 'org.openjdk.jmh:jmh-generator-annprocess:1.28',
    jna: 'net.java.dev.jna:jna:4.5.1',
    jsr305: 'com.google.code.findbugs:jsr305:3.0.2',
    joptSimple: 'net.sf.jopt-simple:jopt-simple:3.2',
    kafka: kafkaGroup + ':kafka_2.12:' + kafkaVersion,
    kafkaClients: kafkaGroup + ':kafka-clients:' + kafkaVersion,
    kafkaClientsTest: kafkaGroup + ':kafka-clients:' + kafkaVersion + ':test',
    log4j2api: 'org.apache.logging.log4j:log4j-api:' + log4j2Version,
    log4j2core: 'org.apache.logging.log4j:log4j-core:' + log4j2Version,
    mail: 'javax.mail:mail:1.4.4',
    mapreduceClientCore: 'org.apache.hadoop:hadoop-mapreduce-client-core:2.3.0',
    mapreduceClientJobClient: 'org.apache.hadoop:hadoop-mapreduce-client-jobclient:2.3.0',
    mockito: 'org.mockito:mockito-core:3.3.3',
    netty: 'io.netty:netty-all:4.1.52.Final',
    oss: 'org.sonatype.oss:oss-parent:7',
    r2: 'com.linkedin.pegasus:r2:' + pegasusVersion,
    restliCommon: 'com.linkedin.pegasus:restli-common:' + pegasusVersion,
    rocksdbjni: 'org.rocksdb:rocksdbjni:7.0.3',
    samzaApi: 'org.apache.samza:samza-api:1.5.1',
    snappy: 'org.iq80.snappy:snappy:0.4',
    spark: 'com.sparkjava:spark-core:2.9.4',
    spotbugs: 'com.github.spotbugs:spotbugs:4.5.2',
    tehuti: 'io.tehuti:tehuti:0.9.2',
    testng: 'org.testng:testng:6.14.3',
    // Resolves java.lang.UnsupportedOperationException:  setXIncludeAware is not supported on this JAXP implementation or earlier: class org.apache.xerces.jaxp.DocumentBuilderFactoryImpl
    xalan: 'xalan:xalan:2.7.1',
    xerces: 'xerces:xercesImpl:2.9.1',
    zkclient: 'com.101tec:zkclient:0.7', // For Kafka AdminUtils
    zookeeper: 'org.apache.zookeeper:zookeeper:3.5.9',
    zstd: 'com.github.luben:zstd-jni:1.5.2-3'
]

group = 'com.linkedin.venice'

publishing {
  repositories {
    mavenLocal()
    maven {
      name 'LinkedInJFrog'
      url 'https://linkedin.jfrog.io/artifactory/venice'
      if (System.getenv('JFROG_USERNAME') != null && System.getenv('JFROG_API_KEY') != null) {
        credentials {
          username System.getenv('JFROG_USERNAME')
          password System.getenv('JFROG_API_KEY')
        }
      }
    }
  }
}

subprojects {
  //apply group and version to all submodules
  group = rootProject.group
  version = rootProject.version

  apply {
    plugin 'idea'
    plugin 'jacoco'
    plugin 'com.form.diff-coverage'
    plugin 'java-library'
    plugin 'com.github.spotbugs'
    plugin 'org.gradle.test-retry'
  }

  if (JavaVersion.current() >= JavaVersion.VERSION_1_9) {
    tasks.withType(JavaCompile) {
      options.release = 8
    }
  }

  java {
    withSourcesJar()
    // TODO: Enable after we have valid javadocs
    //withJavadocJar()
  }

  configurations {
    implementation {
      // These are global exclusions that will apply to the entire project
      exclude group: 'backport-util-concurrent'
      exclude group: 'com.intellij.annotations'
      exclude group: 'com.linkedin.avro-schemas'
      exclude group: 'com.linkedin.container'
      exclude group: 'com.linkedin.container-core'
      exclude group: 'com.linkedin.security'
      exclude group: 'com.linkedin.dds-storage-core'
      exclude group: 'com.linkedin.linkedin-kafka-clients'
      exclude group: 'com.linkedin.util', module: 'util-sql'
      exclude group: 'org.slf4j', module: 'slf4j-log4j12'
      exclude module: 'clojure'
      exclude module: 'kafka_2.10' // This ends up getting pulled in by a few dependencies, unfortunately :/ ...
      exclude module: 'kafka_2.11'

      // These various netty modules clash with netty-all, and we transitively pull a different version, which causes NoSuchMethodError
      exclude group: 'io.netty', module: 'netty-buffer'
      exclude group: 'io.netty', module: 'netty-codec'
      exclude group: 'io.netty', module: 'netty-common'
      exclude group: 'io.netty', module: 'netty-handler'
      exclude group: 'io.netty', module: 'netty-resolver'
      exclude group: 'io.netty', module: 'netty-transport'
      exclude group: 'io.netty', module: 'netty-transport-native-epoll'
      exclude group: 'io.netty', module: 'netty-transport-native-unix-common'
    }
    compileOnly {
      // These dependencies are transitively used at runtime, so we cannot exclude them further than compileOnly
      exclude group: 'com.google.guava'
      exclude group: 'com.typesafe.scala-logging'
      exclude group: 'log4j'
      exclude group: 'org.slf4j'
    }
    all {
      resolutionStrategy.force libraries.zookeeper
    }
    avroCompiler {
    }
  }

  dependencies {
    testImplementation libraries.log4j2api
    testImplementation libraries.mockito
    testImplementation libraries.testng
    // Test utils and framework for all unit tests and integration tests.
    testImplementation project(':internal:venice-test-common')

    spotbugs libraries.spotbugs
    spotbugsPlugins libraries.avroUtilSpotbugsPlugin

    avroCompiler libraries.avroCompiler
    avroCompiler libraries.avroUtilBuilder
    avroCompiler 'org.slf4j:slf4j-simple:1.7.32'
  }

  idea {
    module {
      downloadJavadoc = true
      downloadSources = true
    }
  }

  task compileAvro(type: SourceTask) {
    def sourceDir = file('src/main/resources/avro')
    def outputDir = file("$buildDir/generated/sources/avro/java/main")

    source sourceDir
    inputs.files(configurations.avroCompiler).withNormalizer(ClasspathNormalizer)
    outputs.dir(outputDir)
    outputs.cacheIf { true }

    doFirst {
      def versionOverrides = [
//        project(':internal:venice-common').file('src/main/resources/avro/StoreVersionState/v5', PathValidation.DIRECTORY)
      ]

      def schemaDirs = [sourceDir]
      sourceDir.eachDir { typeDir ->
        def parseVersionId = { dir ->
          (dir in versionOverrides) ? Integer.MAX_VALUE : dir?.name?.substring(1)?.toInteger()
        }
        def latestVersionDir = null
        typeDir.eachDirMatch(~/v-?\d+/) { versionDir ->
          if (parseVersionId(versionDir) > parseVersionId(latestVersionDir)) {
            latestVersionDir = versionDir
          }
        }
        if (latestVersionDir) {
          schemaDirs << latestVersionDir
        }
      }

      copy {
        from (schemaDirs) {
          include '*.avsc'
        }
        into temporaryDir
        duplicatesStrategy = DuplicatesStrategy.FAIL
        eachFile {
          println "Copying avro schema ${relativePath(it.file)} ${it.file.parentFile in versionOverrides ? '(OVERRIDE)' : ''}"
        }
      }

      javaexec {
        classpath = configurations.avroCompiler
        main = 'com.linkedin.avroutil1.builder.SchemaBuilder'
        args = [
            '--input', temporaryDir,
            '--output', outputDir
        ]
      }
    }
  }
  sourceSets.main.java.srcDir(compileAvro)

  tasks.withType(SpotBugsTask) {
    effort = 'max'
    reportLevel = 'low'
    includeFilter = file(
      project.hasProperty('spotallbugs') ?
        "$rootDir/gradle/spotbugs/include-all.xml" :
        "$rootDir/gradle/spotbugs/include.xml"
    )
    excludeFilter = file("$rootDir/gradle/spotbugs/exclude.xml")
    ignoreFailures = project.hasProperty('spotbugs.ignoreFailures')
    showStackTraces = false
    reports ({
      xml {
        enabled = project.hasProperty('spotbugs.xml')
      }
      html {
        enabled = !reports.getByName('XML').enabled
        stylesheet = 'fancy-hist.xsl'
      }
    })
    doFirst {
      sourceDirs += sourceSets.getByName(baseName).output.generatedSourcesDirs
      def generatedSources = sourceDirs.sum { dir ->
        dir.path =~ "^$buildDir/generated/sources/" ?
          fileTree(dir: dir, include: '**/*.java').collect { dir.relativePath(it) } : []
      }
      if (generatedSources) {
        def generatedClasses = generatedSources*.replaceFirst('.java$', '').sum {
          [ it + '.class', it + '\$*.class' ]
        }
        classes = classDirs.asFileTree.matching { exclude generatedClasses }
        auxClassPaths += classDirs.asFileTree.matching { include generatedClasses }.each {
          println "Excluding generated class ${project.relativePath(it)}"
        }
      }
    }
  }

  def ALPINI_TEST_FILTER = 'com.linkedin.alpini.*'
  def ALPINI_NETTY_TEST_FILTER = 'io.netty.*'
  def ALPINI_UNIT_TEST_TASK_NAME = 'alpiniUnitTest'
  def ALPINI_FUNCTIONAL_TEST_TASK_NAME = 'alpiniFunctionalTest'

  tasks.withType(Test) {
    mustRunAfter tasks.withType(SpotBugsTask)

    if (!project.ext.has("skipAlpnBoot") || !project.ext.skipAlpnBoot) {
      // The ALPN version should match the JVM version
      if (JavaVersion.current() < JavaVersion.VERSION_11) {
        if (Os.isFamily(Os.FAMILY_MAC)) {
          jvmArgs '-Xbootclasspath/p:/Library/Java/Boot/1_8_0_172/alpn-boot-8.1.12.v20180117.jar'
        } else if (Os.isFamily(Os.FAMILY_UNIX)) {
          jvmArgs '-Xbootclasspath/p:/export/apps/jdkboot/1_8_0_172/alpn-boot-8.1.12.v20180117.jar'
        } else {
          def osName = System.getProperty('os.name').toLowerCase(Locale.ENGLISH)
          throw new GradleException("$osName is not supported since ALPN only works for either Mac or Linux right now.")
        }
      }
    }

    forkEvery = Integer.valueOf(System.getProperty('forkEvery', '0'))
    maxParallelForks = Integer.valueOf(System.getProperty('maxParallelForks', '4'))
    minHeapSize = System.getProperty('minHeapSize', '1g')
    maxHeapSize = System.getProperty('maxHeapSize', '4g')
    System.getProperty('jvmArgs')?.eachMatch(/(?:[^\s'"]+|'[^']*'|"[^"]*")+/) { jvmArgs it }

    doFirst {
      println "forkEvery=$forkEvery"
      println "maxParallelForks=$maxParallelForks"
      println "jvmArgs=$allJvmArgs"
    }

    if (name != ALPINI_UNIT_TEST_TASK_NAME && name != ALPINI_FUNCTIONAL_TEST_TASK_NAME) {
      filter {
        excludeTestsMatching ALPINI_TEST_FILTER
        excludeTestsMatching ALPINI_NETTY_TEST_FILTER
        failOnNoMatchingTests = false
      }
    }

    useTestNG {
      excludeGroups 'flaky'
      listeners = ['com.linkedin.venice.testng.VeniceSuiteListener']
    }

    retry {
      maxRetries = 4 // 5 attempts in total
      maxFailures = 100
      failOnPassedAfterRetry = false
    }

    testLogging {
      events 'started', 'failed', 'skipped' // N.B. we override how 'passed' tests are formatted in afterTest
      showStandardStreams = false // to mute the DDS Router's noisy behavior...
      exceptionFormat = 'full'
    }

    afterTest { descriptor, result ->
      if (result.resultType == TestResult.ResultType.SUCCESS) {
        def totalTime = result.endTime - result.startTime
        def prettyTime = totalTime < 1000 ? "$totalTime ms" : "${totalTime / 1000} s"

        def out = services.get(StyledTextOutputFactory).create("an-ouput")
        out.style(Style.Normal).text("Gradle suite > Gradle test > $descriptor.className > $descriptor.displayName ")
            .style(Style.Identifier).text("PASSED ")
            .style(Style.Normal).println("($prettyTime)")
      }
    }
  }

  jacocoTestReport {
    dependsOn test // tests are required to run before generating the report
  }

  afterEvaluate {
    jacocoTestCoverageVerification {
      dependsOn jacocoTestReport

      violationRules {
        rule {
          def threshold = project.ext.has('jacocoCoverageThreshold') ?
              project.ext.jacocoCoverageThreshold : 0.6

          limit {
            counter = 'BRANCH'
            value = 'COVEREDRATIO'
            minimum = threshold
          }
        }
      }
    }

    diffCoverageReport {
      diffSource.file = createDiffFile()

      // Report locates at <module_name>/build/reports/jacoco/diffCoverage/html/index.html
      reports {
        html = true
      }

      violationRules {
        minBranches = project.ext.has('diffCoverageThreshold') ? project.ext.diffCoverageThreshold : 0.33
        failOnViolation = true
      }
    }
  }

  task flakyTest(type: Test) {
    useTestNG {
      includeGroups 'flaky'
    }
  }

  task "$ALPINI_UNIT_TEST_TASK_NAME"(type: Test) {
    useTestNG() {
      includeGroups 'unit'
    }
    filter {
      includeTestsMatching ALPINI_TEST_FILTER
      includeTestsMatching ALPINI_NETTY_TEST_FILTER
      failOnNoMatchingTests = false
    }
  }

  task "$ALPINI_FUNCTIONAL_TEST_TASK_NAME"(type: Test) {
    useTestNG() {
      includeGroups 'functional'
    }
    filter {
      includeTestsMatching ALPINI_TEST_FILTER
      includeTestsMatching ALPINI_NETTY_TEST_FILTER
      failOnNoMatchingTests = false
    }
  }

  tasks.withType(Jar) {
    zip64 = true
    duplicatesStrategy = DuplicatesStrategy.FAIL
    exclude('**/*.xml')
  }

  task testJar(type: Jar) {
    classifier 'tests'
    from sourceSets.test.output
  }

  // Only publish artifacts for projects that are at the leaf level
  if (project.childProjects.isEmpty()) {
    publishing.configureArtifactPublishing(project, testJar)
  }
}

spotless {
  ratchetFrom "${git.getUpstreamRemote()}/main"
  java {
    importOrder()
    removeUnusedImports()
    eclipse().configFile("$rootDir/gradle/spotless/eclipse-java-formatter.xml")
    target '**/*.java'
    targetExclude '**/generated/**'
  }
}

task setupWorkspace {
  println 'Setting up default git config'
  def gitConfig = [
      'core.hooksPath' : 'gradle/githooks',
      'blame.ignoreRevsFile' : '.git-blame-ignore-revs',
      'branch.autoSetupMerge' : 'true', // Only track remote branches
      'branch.autoSetupRebase' : 'always',
      'pull.rebase' : 'true',
  ]
  gitConfig.each(git.setConfig)
}

task spotbugs {
  dependsOn subprojects.tasks*.withType(SpotBugsTask)
}
check.dependsOn(spotbugs)

test {
  mustRunAfter spotbugs
  dependsOn subprojects.test
  afterTest { descriptor, result ->
    def totalTime = result.endTime - result.startTime
    println "Total time of $descriptor.name was $totalTime"
  }
}

assemble {
  dependsOn (
    setupWorkspace,
    testClasses,
    'internal:venice-test-common:jmhClasses',
    'internal:venice-test-common:integrationTestClasses'
  )
}

build {
  dependsOn (
    'services:venice-router:installDist',
    'services:venice-server:installDist',
    'services:venice-controller:installDist'
  )
}

idea.project.ipr {
  withXml { provider ->
    provider.node.component
            .find { it.@name == 'VcsDirectoryMappings' }
            .mapping.@vcs = 'Git'

    def inspectionProjectProfileManager = provider.node.component
        .find { it.@name == 'InspectionProjectProfileManager' }

    def danglingJavaDocInspectionProperties = [
      class: "DanglingJavadoc",
      enabled: "false",
      level: "WARNING",
      enabled_by_default: "false"
    ]

    if (inspectionProjectProfileManager == null) {
      inspectionProjectProfileManager = provider.node.appendNode(
        "component",
        [
          name: "InspectionProjectProfileManager"
        ]
      )

      def profile = inspectionProjectProfileManager.appendNode(
        "profile",
        [
          version: "1.0"
        ]
      )

      profile.appendNode(
        "option",
        [
          name: "myName",
          value: "Project Default"
        ]
      )

      profile.appendNode(
        "inspection_tool",
        danglingJavaDocInspectionProperties
      )

      inspectionProjectProfileManager.appendNode(
        "version",
        [
          value: "1.0"
        ]
      )
    } else {
      def danglingJavaDoc = inspectionProjectProfileManager.profile.inspection_tool
          .find { it.@class == 'DanglingJavadoc' }
      if (danglingJavaDoc == null) {
        inspectionProjectProfileManager.profile.get(0).appendNode(
          "inspection_tool",
          danglingJavaDocInspectionProperties
        )
      } else {
        danglingJavaDoc.@enabled = false
        danglingJavaDoc.@level = "WARNING"
        danglingJavaDoc.@enabled_by_default = false
      }
    }
  }
}

// Allow running diffCoverage against uncommitted files
// See https://github.com/form-com/diff-coverage-gradle/issues/73
ext.createDiffFile = { ->
  // Files that we don't plan to write unit tests for now. Will be worked in the future
  def exclusionFilter = [
        ':!services/venice-controller/src/main/java/com/linkedin/venice/controller/VeniceHelixAdmin.java',
        ':!services/venice-controller/src/main/java/com/linkedin/venice/controller/VeniceParentHelixAdmin.java',
        ':!internal/venice-test-common/*',
        ':!services/venice-server/src/main/java/com/linkedin/venice/server/VeniceServer.java',

        // Other files that have tests but are not executed in the regular unit test task
        ':!internal/alpini/*'
  ]
  def file = Files.createTempFile(URLEncoder.encode(project.name, 'UTF-8'), '.diff').toFile()
  def diffBase = "${git.getUpstreamRemote()}/main"
  def command = [
      'git', 'diff', diffBase, '--no-color', '--minimal', '--', '.'
  ]
  command.addAll(exclusionFilter)
  file.withOutputStream { out ->
    exec {
      commandLine command
      standardOutput = out
    }
  }

  return file
}
