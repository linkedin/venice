import groovy.io.FileType
import com.github.spotbugs.snom.SpotBugsTask
import org.apache.tools.ant.taskdefs.condition.Os

buildscript {
  dependencies {
    classpath 'com.github.spotbugs.snom:spotbugs-gradle-plugin:4.8.0'
    classpath 'com.gradle:gradle-enterprise-gradle-plugin:3.5.2'
    classpath 'gradle.plugin.com.dorongold.plugins:task-tree:1.5'
    classpath 'org.gradle:test-retry-gradle-plugin:1.3.1'
  }
}

plugins {
  id 'java'
  id 'idea'
}

try {
  apply plugin: 'com.gradle.build-scan'
  gradleEnterprise {
    server = System.getProperty('buildScanServer', 'Set systemProp.buildScanServer in ~/.gradle/gradle.properties')
    buildScan {
      def commitId = 'git rev-parse --verify HEAD'.execute().text.trim()
      value 'CommitId', commitId
    }
  }
} catch (e) {
  println e
}

try {
  apply plugin: 'com.dorongold.task-tree'
  taskTree {
    noRepeat = true
  }
} catch (e) {
  println e
}

if (project.hasProperty('overrideBuildEnvironment')) {
  if (project.overrideBuildEnvironment) {
    apply from: file(project.overrideBuildEnvironment)
    product {
      codeQuality {
        ignoreFailures = true
      }
    }
  }
}

def containerVersion = '38.4.415'
def ddsStorageCoreVersion = '13.0.16'
def internalKafkaGroup = 'com.linkedin.kafka'
def internalKafkaVersion = '2.3.0.32'
def log4j2Version = '2.17.1'
def pegasusVersion = '29.29.0'
def avroUtilVersion = '0.2.84'

ext.libraries = [
    avro: 'org.apache.avro:avro:1.9.2',
    avroMapred: 'org.apache.avro:avro-mapred:1.9.2',
    avroUtilCompatibilityHelper: 'com.linkedin.avroutil1:helper-all:' + avroUtilVersion,
    avroUtilFastserde: 'com.linkedin.avroutil1:avro-fastserde:' + avroUtilVersion,
    brooklinCommon: 'com.linkedin.brooklin-li-common:brooklin-li-common:80.0.2',
    brooklinKafka: 'com.github.datastream:datastream-kafka-connector:36.0.1',
    brooklinKafkaTransport: 'com.github.datastream:datastream-kafka:36.0.1',
    brooklinTest: 'com.linkedin.brooklin-li-common:brooklin-test-pub:80.0.2',
    bouncyCastle: 'org.bouncycastle:bcprov-jdk15on:1.55',
    caffeine: 'com.github.ben-manes.caffeine:caffeine:2.8.5',
    commonsCodec: 'commons-codec:commons-codec:1.4',
    commonsIo: 'commons-io:commons-io:2.11',
    fastUtil: 'it.unimi.dsi:fastutil:8.3.0',
    helix: 'org.apache.helix:helix-core:1.0.1.24',
    kafka: internalKafkaGroup + ':kafka_2.12:' + internalKafkaVersion,
    kafkaClients: internalKafkaGroup + ':kafka-clients:' + internalKafkaVersion,
    kafkaClientsTest: internalKafkaGroup + ':kafka-clients:' + internalKafkaVersion + ':test',
    mockito: 'org.mockito:mockito-core:3.3.3',
    netty: 'io.netty:netty-all:4.1.52.Final',
    zkclient: 'com.101tec:zkclient:0.7', // For Kafka AdminUtils
    d2: 'com.linkedin.container:pegasus-d2-server-factory:' + containerVersion,
    d2Client: 'com.linkedin.container:pegasus-d2-client-factory:' + containerVersion,
    r2Client: 'com.linkedin.container:pegasus-r2-client-factory:' + containerVersion,
    r2: "com.linkedin.pegasus:r2:" + pegasusVersion,
    routerApi: 'com.linkedin.dds-storage-core:dds-storage-router-api:' + ddsStorageCoreVersion,
    routerImpl: 'com.linkedin.dds-storage-core:dds-storage-router-impl:' + ddsStorageCoreVersion,
    ddsNettyBase: 'com.linkedin.dds-storage-core:dds-storage-netty4-base:' + ddsStorageCoreVersion,
    routerLnkd: 'com.linkedin.dds-storage-core:dds-storage-router-lnkd:' + ddsStorageCoreVersion,
    netty4Lnkd: 'com.linkedin.dds-storage-core:dds-storage-netty4-lnkd:' + ddsStorageCoreVersion,
    ddsStorageCommonBase: 'com.linkedin.dds-storage-core:dds-storage-common-base:' + ddsStorageCoreVersion,
    oss: 'org.sonatype.oss:oss-parent:7',
    asyncHttpClient: 'org.apache.httpcomponents:httpasyncclient:4.1.2',
    zookeeper: 'org.apache.zookeeper:zookeeper:3.4.6',
    zstd: 'com.github.luben:zstd-jni:1.4.4-7',

    // Hadoop to Venice Bridge dependencies
    mapreduceClientCore: 'org.apache.hadoop:hadoop-mapreduce-client-core:2.3.0',
    commonsConfiguration: 'commons-configuration:commons-configuration:1.9',
    commonsLang: 'commons-lang:commons-lang:2.6',
    hadoopCommon: 'org.apache.hadoop:hadoop-common:2.3.0',
    httpClient: 'org.apache.httpcomponents:httpclient:4.5.2',
    httpCore: 'org.apache.httpcomponents:httpcore:4.4.5',
    jacksonCore: 'org.codehaus.jackson:jackson-core-asl:1.9.6',
    jacksonMapper: 'org.codehaus.jackson:jackson-mapper-asl:1.9.6',
    javax: 'javax.servlet:javax.servlet-api:3.1.0',
    jdom: 'org.jdom:jdom:1.1',
    joptSimple: 'net.sf.jopt-simple:jopt-simple:3.2',
    log4j2api: 'org.apache.logging.log4j:log4j-api:' + log4j2Version,
    log4j2core: 'org.apache.logging.log4j:log4j-core:' + log4j2Version,
    slf4j: 'org.apache.logging.log4j:log4j-to-slf4j:' + log4j2Version,
    snappy: 'org.xerial.snappy:snappy-java:1.0.4.1',
    // TODO: move back to open source sparkjava once they fix Jetty SSL.
    // spark: 'com.sparkjava:spark-core:2.9.1',
    spark: 'com.linkedin.li-sparkjava:li-sparkjava-impl:0.0.2',
    testng: 'org.testng:testng:6.14.3',
    tehuti: 'io.tehuti:tehuti:0.8.8',
    // Resolves java.lang.UnsupportedOperationException:  setXIncludeAware is not supported on this JAXP implementation or earlier: class org.apache.xerces.jaxp.DocumentBuilderFactoryImpl
    xalan: 'xalan:xalan:2.7.1',
    xerces: 'xerces:xercesImpl:2.9.1',

    // Hadoop to Venice Bridge test dependencies
    mapreduceClientJobClient: 'org.apache.hadoop:hadoop-mapreduce-client-jobclient:2.3.0',

    // Samza dependencies
    samzaApi: 'com.linkedin.samza-li:samza-api:311.1050.0.66', // Equivalent to Samza-1.5

    // Added during dds-storage-core v2 => v3
    // Transitive dependency, to replace implicit version 11.0.17 which caused 4 test cases to fail:
    // 1. TestAdminSparkServer.controllerClientCanDeleteStore
    // 2. TestAdminSparkServer.controllerClientCanDeleteOldVersion
    // 3. TestAdminSparkServer.controllerClientCanDeleteAllVersion
    // 4. TestBrooklin.canReplicateKafkaWithBrooklinTopicReplicator
    restliDocgen: 'com.linkedin.pegasus:restli-docgen:' + pegasusVersion,

    // RocksDB
    rocksdbjni: 'org.rocksdb:rocksdbjni:6.20.3',

    // Flatbuffers lib, currently for benchmark only
    flatbuffersJava: 'com.google.flatbuffers:flatbuffers-java:1.11.0',

    // open source conscrypt from google
    conscrypt: 'org.conscrypt:conscrypt-openjdk-uber:2.4.0',
    classgraph: 'io.github.classgraph:classgraph:4.8.60',

    jmhCore: 'org.openjdk.jmh:jmh-core:1.28',
    jmhGenerator: 'org.openjdk.jmh:jmh-generator-annprocess:1.28',
    failsafe: 'net.jodah:failsafe:2.4.0',

    spotbugs: 'com.github.spotbugs:spotbugs:4.5.2',
    spotbugsAvroPlugin: 'com.linkedin.avroutil1:spotbugs-plugin:0.2.69',

    // TODO: switch to avro-util once it's able to generate version agnostic code.
    avroSchemasBuilder: 'com.linkedin.avro-schemas:avro-schemas-builder:63.1.428',

    "httpClient5": "org.apache.httpcomponents.client5:httpclient5:5.1.2"
]

subprojects {
  apply {
    plugin 'java-library'
    plugin 'idea'
    plugin 'jacoco'
    plugin 'com.github.spotbugs'
    plugin 'org.gradle.test-retry'
  }

  sourceCompatibility = 1.8
  if (!project.hasProperty('overrideBuildEnvironment') || !project.overrideBuildEnvironment) {
     version = '0.1'
  }

  repositories {
    ivy { // TODO: remove this before opensourcing
      url 'http://artifactory.corp.linkedin.com:8081/artifactory/release/'
      patternLayout {
        ivy '[organisation]/[module]/[revision]/[module]-[revision].ivy'
        artifact '[organisation]/[module]/[revision]/[artifact]-[revision](-[classifier]).[ext]'
        m2compatible = true
      }
    }
    mavenCentral() // when build fails, move this to first and then build, then move it back and build should work
  }

  configurations {
    implementation {
      // These are global exclusions that will apply to the entire project
      exclude group: 'backport-util-concurrent'
      exclude group: 'com.intellij.annotations'
      exclude group: 'com.linkedin.avro-schemas'
      // exclude group: 'com.linkedin.container'
      exclude group: 'com.linkedin.container-core'
      exclude group: 'com.linkedin.linkedin-kafka-clients'
      exclude group: 'com.linkedin.util', module: 'util-sql'
      exclude module: 'kafka_2.10' // This ends up getting pulled in by a few dependencies, unfortunately :/ ...
      exclude module: 'kafka_2.11'
      // exclude group: 'org.eclipse.jetty'
      exclude module: 'clojure'
    }
    compileOnly {
      // These dependencies are transitively used at runtime, so we cannot exclude them further than compileOnly
      exclude group: 'com.google.guava', module: 'guava' // Used by Brooklin at runtime
      exclude group: 'log4j', module: 'log4j'
      exclude group: 'com.typesafe.scala-logging'
    }
    avroCompiler.extendsFrom compileOnly
  }

  dependencies {
    implementation 'javax.validation:validation-api:2.0.1.Final'

    testImplementation libraries.testng
    testImplementation libraries.mockito
    testImplementation libraries.log4j2api
    // Test utils and framework for all unit tests and integration tests.
    testImplementation project(':venice-test-common')

    spotbugs libraries.spotbugs
    spotbugsPlugins libraries.spotbugsAvroPlugin

    avroCompiler libraries.avroSchemasBuilder
    avroCompiler 'org.slf4j:slf4j-simple:1.7.32'
  }

  idea {
    module {
      downloadJavadoc = true
      downloadSources = true
    }
  }

  task sourceSets {
    doLast {
      sourceSets.each {
        println "[$it.name]"
        println "Sources: $it.allJava.srcDirs"
        println "Output Sources: $it.output.generatedSourcesDirs.files"
        println "Output Classes: $it.output.classesDirs.files"
        println ""
      }
    }
  }

  task compileAvro(type: JavaExec) {
    def inputDir = file('src/main/resources/avro')
    def outputDir = file("$buildDir/generated/sources/avro/java/main")

    def versionOverrides = [
//      'venice-common/src/main/resources/avro/StoreVersionState/v5'
    ]

    onlyIf { inputDir.exists() }
    inputs.files(configurations.avroCompiler).withNormalizer(ClasspathNormalizer)
    inputs.dir(inputDir)
    outputs.dir(outputDir)

    doFirst {
      def schemas = [:]
      def copySchemas = { dir ->
        println "Copying avro schemas from ${project.relativePath(dir)}"
        dir.eachFileMatch(FileType.FILES, ~/.*\.avsc/) { schema ->
          def original = schemas.put(schema.name, schema)
          if (original) {
            throw new GradleException("Duplicate avro schemas: \n $original\n $schema")
          }
          copy { from schema into temporaryDir }
        }
      }

      // Copy not versioned schemas from the avro root
      copySchemas(inputDir)

      // Copy latest versioned schemas from sub-folders
      inputDir.eachDir { typeDir ->
        def parseVersionId = { dir ->
          if (!dir) {
            null
          } else if (rootProject.relativePath(dir) in versionOverrides) {
            Integer.MAX_VALUE
          } else {
            dir.name.substring(1).toInteger()
          }
        }
        def latestVersionDir = null
        typeDir.eachDirMatch(~/v-?\d+/) { versionDir ->
          if (parseVersionId(versionDir) > parseVersionId(latestVersionDir)) {
            latestVersionDir = versionDir
          }
        }
        if (latestVersionDir) {
          copySchemas(latestVersionDir)
        }
      }

      println "Compiling avro schemas ${temporaryDir.list().toSorted()}"
      classpath = configurations.avroCompiler
      main = 'com.linkedin.avro.Builder'
      args = [
        '--input', temporaryDir,
        '--output', outputDir,
        '--includeClasspath', 'true'
      ]
    }
  }
  sourceSets.main.java.srcDir(compileAvro)

  tasks.withType(SpotBugsTask) {
    effort = 'max'
    reportLevel = 'low'
    includeFilter = file(
      project.hasProperty('spotallbugs') ?
        "$rootDir/gradle/spotbugs/include-all.xml" :
        "$rootDir/gradle/spotbugs/include.xml"
    )
    excludeFilter = file("$rootDir/gradle/spotbugs/exclude.xml")
    ignoreFailures = project.hasProperty('spotbugs.ignoreFailures')
    showStackTraces = false
    reports ({
      xml {
        enabled = project.hasProperty('spotbugs.xml')
      }
      html {
        enabled = !reports.getByName('XML').enabled
        stylesheet = 'fancy-hist.xsl'
      }
    })
    doFirst {
      sourceDirs += sourceSets.getByName(baseName).output.generatedSourcesDirs
      def generatedSources = sourceDirs.sum { dir ->
        dir.path =~ "^$buildDir/generated/sources/" ?
          fileTree(dir: dir, include: '**/*.java').collect { dir.relativePath(it) } : []
      }
      if (generatedSources) {
        def generatedClasses = generatedSources*.replaceFirst('.java$', '').sum {
          [ it + '.class', it + '\$*.class' ]
        }
        classes = classDirs.asFileTree.matching { exclude generatedClasses }
        auxClassPaths += classDirs.asFileTree.matching { include generatedClasses }.each {
          println "Excluding generated class ${project.relativePath(it)}"
        }
      }
    }
  }

  tasks.withType(Test) {
    mustRunAfter tasks.withType(SpotBugsTask)

    // The ALPN version should match the JVM version
    if (JavaVersion.current() < JavaVersion.VERSION_11) {
      if (Os.isFamily(Os.FAMILY_MAC)) {
        jvmArgs = ['-Xbootclasspath/p:/Library/Java/Boot/1_8_0_172/alpn-boot-8.1.12.v20180117.jar']
      } else if (Os.isFamily(Os.FAMILY_UNIX)) {
        jvmArgs = ['-Xbootclasspath/p:/export/apps/jdkboot/1_8_0_172/alpn-boot-8.1.12.v20180117.jar']
      } else {
        String osName = System.getProperty('os.name').toLowerCase(Locale.ENGLISH);
        throw new GradleException("$osName is not supported since ALPN only works for either Mac or Linux right now.")
      }
    }

    forkEvery = Integer.valueOf(System.getProperty('forkEvery', '0'))
    maxParallelForks = Integer.valueOf(System.getProperty('maxParallelForks', '4'))
    minHeapSize = System.getProperty('minHeapSize', '1g')
    maxHeapSize = System.getProperty('maxHeapSize', '4g')

    doFirst {
      println "forkEvery=$forkEvery"
      println "maxParallelForks=$maxParallelForks"
      println "minHeapSize=$minHeapSize"
      println "maxHeapSize=$maxHeapSize"
    }

    useTestNG() {
      excludeGroups 'flaky'
      Set<String> suiteListeners = ['com.linkedin.venice.testng.VeniceSuiteListener']
      setListeners(suiteListeners)
    }

    retry {
      maxRetries = 4 // 5 attempts in total
      maxFailures = 100
      failOnPassedAfterRetry = false
    }

    testLogging {
      events 'started', 'passed', 'failed', 'skipped'
      showStandardStreams = false // to mute the DDS Router's noisy behavior...
      exceptionFormat = 'full'
    }
  }

  task flakyTest(type: Test) {
    useTestNG() {
      includeGroups 'flaky'
    }
  }
}

task spotbugs {
  dependsOn subprojects.tasks*.withType(SpotBugsTask)
}
check.dependsOn(spotbugs)

test {
  mustRunAfter spotbugs
  dependsOn subprojects.test
}

assemble {
  dependsOn (
    testClasses,
    'venice-test-common:jmhClasses',
    'venice-test-common:integrationTestClasses'
  )
}

build {
  dependsOn (
    'venice-router:installDist',
    'venice-server:installDist',
    'venice-controller:installDist'
  )
}
