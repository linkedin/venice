package com.linkedin.davinci.replication.merge;

import com.linkedin.avroutil1.compatibility.AvroCompatibilityHelper;
import com.linkedin.avroutil1.compatibility.AvroVersion;
import com.linkedin.venice.schema.merge.UpdateResultStatus;
import com.linkedin.venice.schema.merge.MergeRecordHelper;
import com.linkedin.venice.exceptions.VeniceException;
import com.linkedin.venice.schema.merge.ValueAndReplicationMetadata;
import com.linkedin.venice.schema.rmd.v1.ReplicationMetadataSchemaGeneratorV1;
import com.linkedin.venice.schema.writecompute.WriteComputeProcessor;
import com.linkedin.venice.utils.Lazy;
import java.util.List;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericRecord;
import org.apache.commons.lang.Validate;

import static com.linkedin.venice.VeniceConstants.*;


/**
 * Implementations of the API defined in {@link Merge} based on V1 metadata timestamp Schema generated by
 * {@link ReplicationMetadataSchemaGeneratorV1}.
 * All the implementations assume replication metadata format is union record type [long, record] where record
 * is top-level fieldName:timestamp format.
 * 1. Currently collection merging is not supported as replication metadata does not support it yet.
 * 2. schema evolution is not supported, so it assumes incoming and old schema are same else else throws VeniceException
 * 3. Assumes new value to be GenericRecord type, does not support non-record values.
 */
class MergeGenericRecord implements Merge<GenericRecord> {
  private static final AvroVersion RUNTIME_AVRO_VERSION = AvroCompatibilityHelper.getRuntimeAvroVersion();
  private final WriteComputeProcessor writeComputeProcessor;
  private final MergeRecordHelper mergeRecordHelper;

  MergeGenericRecord(WriteComputeProcessor writeComputeProcessor, MergeRecordHelper mergeRecordHelper) {
    Validate.notNull(writeComputeProcessor);
    Validate.notNull(mergeRecordHelper);
    this.writeComputeProcessor = writeComputeProcessor;
    this.mergeRecordHelper = mergeRecordHelper;
  }

  @Override
  public ValueAndReplicationMetadata<GenericRecord> put(
      ValueAndReplicationMetadata<GenericRecord> oldValueAndReplicationMetadata, GenericRecord newValue,
      long putOperationTimestamp, int putOperationColoID, long sourceOffsetOfNewValue, int sourceBrokerIDOfNewValue) {
    final GenericRecord oldReplicationMetadata = oldValueAndReplicationMetadata.getReplicationMetadata();
    final GenericRecord oldValue = oldValueAndReplicationMetadata.getValue();

    // TODO support schema evolution and caching the result of schema validation.
    if (oldValue != null && !oldValue.getSchema().equals(newValue.getSchema())) {
      throw new VeniceException("Incoming schema " + newValue.getSchema() + " is not same as existing schema" + oldValue.getSchema());
    }

    final Object tsObject = oldReplicationMetadata.get(TIMESTAMP_FIELD_NAME);
    ReplicationMetadataType replicationMetadataType = MergeUtils.getReplicationMetadataType(tsObject);

    switch (replicationMetadataType) {
      case ROOT_LEVEL_TIMESTAMP:
        long oldTimeStamp = (long) tsObject;
        if (oldTimeStamp < putOperationTimestamp) {
          // New value wins
          oldValueAndReplicationMetadata.setValue(newValue);
          oldReplicationMetadata.put(TIMESTAMP_FIELD_NAME, putOperationTimestamp);
          updateReplicationCheckpointVector(oldReplicationMetadata, sourceOffsetOfNewValue, sourceBrokerIDOfNewValue);

        } else if (oldTimeStamp == putOperationTimestamp) {
          // for timestamp tie compare decide which one to store.
          newValue = (GenericRecord) MergeUtils.compareAndReturn(oldValue, newValue);
          if (newValue == oldValue) {
            oldValueAndReplicationMetadata.setUpdateIgnored(true);
          } else {
            oldValueAndReplicationMetadata.setValue(newValue);
            updateReplicationCheckpointVector(oldReplicationMetadata, sourceOffsetOfNewValue, sourceBrokerIDOfNewValue);
          }

        } else {
          oldValueAndReplicationMetadata.setValue(oldValue);
          oldValueAndReplicationMetadata.setUpdateIgnored(true);
        }
        return oldValueAndReplicationMetadata;

      case PER_FIELD_TIMESTAMP:
        GenericRecord timestampRecordForOldValue = (GenericRecord) tsObject;

        // TODO: Support schema evolution, as the following assumes old/new schema are same.
        updateReplicationCheckpointVector(oldReplicationMetadata, sourceOffsetOfNewValue, sourceBrokerIDOfNewValue);

        // update the field values based on replication metadata
        List<Schema.Field> oldTimestampRecordFields = timestampRecordForOldValue.getSchema().getFields();
        boolean allFieldsNew = true;
        boolean noFieldUpdated = true;
        for (Schema.Field oldTimestampRecordField : oldTimestampRecordFields) {
          final String fieldName = oldTimestampRecordField.name();
          UpdateResultStatus fieldUpdateResult = mergeRecordHelper.putOnField(
              oldValue,
              timestampRecordForOldValue,
              fieldName,
              newValue.get(fieldName),
              putOperationTimestamp,
              putOperationColoID
          );

          allFieldsNew &= (fieldUpdateResult == UpdateResultStatus.COMPLETELY_UPDATED);
          noFieldUpdated &= (fieldUpdateResult == UpdateResultStatus.NOT_UPDATE);
        }
        if (allFieldsNew) {
          oldReplicationMetadata.put(TIMESTAMP_FIELD_NAME, putOperationTimestamp);
        }
        if (noFieldUpdated) {
          oldValueAndReplicationMetadata.setUpdateIgnored(true);
        }
        return oldValueAndReplicationMetadata;

      default:
        throw new VeniceException("Invalid replication metadata type"  + replicationMetadataType);
    }
  }

  @Override
  public ValueAndReplicationMetadata<GenericRecord> delete(
      ValueAndReplicationMetadata<GenericRecord> oldValueAndReplicationMetadata,
      long deleteOperationTimestamp, int deleteOperationColoID, long sourceOffsetOfNewValue, int sourceBrokerIDOfNewValue) {
    if (RUNTIME_AVRO_VERSION.earlierThan(AvroVersion.AVRO_1_7)) {
      throw new VeniceException("'delete' operation won't work properly with Avro version before 1.7 and"
          + " the runtime Avro version is: " + RUNTIME_AVRO_VERSION);
    }

    final GenericRecord oldReplicationMetadata = oldValueAndReplicationMetadata.getReplicationMetadata();
    final Object tsObject = oldReplicationMetadata.get(TIMESTAMP_FIELD_NAME);
    ReplicationMetadataType replicationMetadataType = MergeUtils.getReplicationMetadataType(tsObject);

    // Always update the vector field
    updateReplicationCheckpointVector(oldReplicationMetadata, sourceOffsetOfNewValue, sourceBrokerIDOfNewValue);

    switch (replicationMetadataType) {
      case ROOT_LEVEL_TIMESTAMP:
        long oldTimeStamp = (long) tsObject;
        // Delete wins when old and new write operation timestamps are equal.
        if (oldTimeStamp <= deleteOperationTimestamp) {
          oldValueAndReplicationMetadata.setValue(null);
          // Still need to track the delete timestamp in order to reject future PUT record with lower replication timestamp
          oldReplicationMetadata.put(TIMESTAMP_FIELD_NAME, deleteOperationTimestamp);
        } else {
          oldValueAndReplicationMetadata.setUpdateIgnored(true);
        }
        return oldValueAndReplicationMetadata;

      case PER_FIELD_TIMESTAMP:
        UpdateResultStatus recordDeleteResultStatus = mergeRecordHelper.deleteRecord(
            oldValueAndReplicationMetadata.getValue(),
            (GenericRecord) tsObject,
            deleteOperationTimestamp,
            deleteOperationColoID
        );

        if (recordDeleteResultStatus == UpdateResultStatus.COMPLETELY_UPDATED) {
          // Full delete
          oldValueAndReplicationMetadata.setValue(null);
          oldReplicationMetadata.put(TIMESTAMP_FIELD_NAME, deleteOperationTimestamp);
        } else if (recordDeleteResultStatus == UpdateResultStatus.NOT_UPDATE) {
          oldValueAndReplicationMetadata.setUpdateIgnored(true);
        }
        return oldValueAndReplicationMetadata;

      default:
        throw new VeniceException("Invalid replication metadata type type"  + replicationMetadataType);
    }
  }

  @Override
  public ValueAndReplicationMetadata<GenericRecord> update(
      ValueAndReplicationMetadata<GenericRecord> oldValueAndReplicationMetadata,
      Lazy<GenericRecord> writeComputeRecord,
      Schema currValueSchema, // Schema of the current value that is to-be-updated here.
      Schema writeComputeSchema,
      long updateOperationTimestamp,
      int updateOperationColoID,
      long sourceOffsetOfNewValue,
      int sourceBrokerIDOfNewValue
  ) {
    updateReplicationCheckpointVector(oldValueAndReplicationMetadata.getReplicationMetadata(), sourceOffsetOfNewValue, sourceBrokerIDOfNewValue);
    return writeComputeProcessor.updateRecordWithRmd(
        currValueSchema,
        writeComputeSchema,
        oldValueAndReplicationMetadata,
        writeComputeRecord.get(),
        updateOperationTimestamp,
        updateOperationColoID
    );
  }

  private void updateReplicationCheckpointVector(GenericRecord oldReplicationMetadata, long sourceOffsetOfNewValue, int sourceBrokerIDOfNewValue) {
    oldReplicationMetadata.put(
        REPLICATION_CHECKPOINT_VECTOR_FIELD,
        MergeUtils.mergeOffsetVectors(
            (List<Long>) oldReplicationMetadata.get(REPLICATION_CHECKPOINT_VECTOR_FIELD),
            sourceOffsetOfNewValue,
            sourceBrokerIDOfNewValue
        )
    );
  }
}
