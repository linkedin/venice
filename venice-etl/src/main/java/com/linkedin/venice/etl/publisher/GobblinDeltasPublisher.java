package com.linkedin.venice.etl.publisher;

import azkaban.jobExecutor.AbstractJob;
import com.linkedin.venice.controllerapi.ControllerClient;
import com.linkedin.venice.controllerapi.StoreResponse;
import com.linkedin.venice.etl.client.VeniceKafkaConsumerClient;
import com.linkedin.venice.exceptions.UndefinedPropertyException;
import com.linkedin.venice.meta.StoreInfo;
import com.linkedin.venice.security.SSLFactory;
import com.linkedin.venice.utils.SslUtils;
import com.linkedin.venice.utils.VeniceProperties;
import java.io.BufferedWriter;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Optional;
import java.util.PriorityQueue;
import java.util.Properties;
import java.util.Set;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;

import static com.linkedin.venice.CommonConfigKeys.*;
import static com.linkedin.venice.ConfigKeys.*;
import static com.linkedin.venice.etl.client.VeniceKafkaConsumerClient.*;
import static com.linkedin.venice.etl.publisher.DataPublisherUtils.*;
import static com.linkedin.venice.etl.source.VeniceKafkaSource.*;


/**
 * Gobblin Deltas Publisher is an Azkaban workflow that will publish the latest gobblin delta file
 * for Venice stores to a specified HDFS path.
 * Only stores which enabled on-demand ETL need to run through this job.
 */
public class GobblinDeltasPublisher extends AbstractJob {
  private static final Logger logger = Logger.getLogger(GobblinDeltasPublisher.class);
  /**
   * indicating a version number for a store is the current version
   */
  private static final String CURRENT_VERSION_SUFFIX = "_current";

  /**
   * indicating a version number for a store is future version
   */
  private static final String FUTURE_VERSION_SUFFIX = "_future";

  // Immutable state
  private final VeniceProperties props;
  private final String fabricName;
  private final String veniceControllerUrls;
  private final int maxDailyDeltasToKeep;
  private final String gobblinDeltasSourceDir;
  private final String gobblinDbChanges = "/dbchanges";

  private String destinationDirPrefix;
  private FileSystem fs;
  private Map<String, String> ETLStoreToUserName;
  private Set<String> onDemandETLEnabledStores;
  private Optional<SSLFactory> sslFactory;
  private Map<String, ExecutorService> storeNameToExecutors;

  public GobblinDeltasPublisher(String jobId, Properties vanillaProps) throws Exception {
    super(jobId, logger);
    this.props = new VeniceProperties(vanillaProps);
    logger.info("Constructing " + GobblinDeltasPublisher.class.getSimpleName() + ": " + props.toString(true));
    this.fabricName = props.getString(FABRIC_NAME);
    this.veniceControllerUrls = props.getString(VENICE_CONTROLLER_URLS);
    this.maxDailyDeltasToKeep = props.getInt(GOBBLIN_MAX_DAILY_DELTAS_TO_KEEP, DEFAULT_MAX_FILES_TO_KEEP);
    // the directory that contains the deltas generated by Gobblin workflow, which are the deltas in version level
    this.gobblinDeltasSourceDir = props.getString(GOBBLIN_DELTAS_SOURCE_DIR);
    // the prefix directory where we publish the Gobblin deltas
    this.destinationDirPrefix = props.getString(ETL_DESTINATION_DIR_PREFIX);
    // builds the list of stores which enabled on-demand ETL, and thus need to run this job
    this.onDemandETLEnabledStores = new HashSet<>();
    setUpOnDemandETLStores(props, onDemandETLEnabledStores);
    // builds the map from etl store name to venice etl user name
    this.ETLStoreToUserName = new HashMap<>();
    setUpETLStoreToUserName(props, ETLStoreToUserName);
    // set up ssl factory to talk to controller
    this.sslFactory = Optional.of(SslUtils.getSSLFactory(setUpSSLProperties(props), props.getString(SSL_FACTORY_CLASS_NAME)));
    // gets configured HDFS filesystem for read and write
    try {
      // without the two configs, fs might write to local FS
      // https://hadoop.apache.org/docs/r1.2.1/cluster_setup.html
      Configuration config = new Configuration();
      config.addResource(new Path("/HADOOP_HOME/conf/core-site.xml"));
      config.addResource(new Path("/HADOOP_HOME/conf/hdfs-site.xml"));
      this.fs = FileSystem.get(config);
    } catch (IOException e) {
      logger.error("Get configured FileSystem implementation failed: ", e);
      throw e;
    }
    this.storeNameToExecutors = new HashMap<>();
  }

  @Override
  public void run() throws Exception {
    // Get all latest Gobblin deltas from the source directory
    Map<String, StoreFilesInfo> storeToDeltasPath = getDeltasPath(gobblinDeltasSourceDir, fs, FILES_COMPARATOR, onDemandETLEnabledStores);

    // Build ControllerClient, which will be used to determine the current version and potentially future version of each store
    Map<String, ControllerClient> storeToControllerClient = VeniceKafkaConsumerClient.getControllerClients(
        storeToDeltasPath.keySet(), this.veniceControllerUrls, sslFactory);

    for (Map.Entry<String, StoreFilesInfo> entry : storeToDeltasPath.entrySet()) {
      String storeName = entry.getKey();
      /**
       * Each store will have a single worker thread to process publishing hourly deltas. It needs to be single thread
       * as for each store the hourly deltas need to sequential. If anytime this publisher fails, it is self-healing
       * next time without missing any previous hourly deltas.
       *
       * Each batch jobs for two different stores could run in parallel.
       */
      this.storeNameToExecutors.put(storeName, Executors.newSingleThreadExecutor());
      if (ETLStoreToUserName.containsKey(storeName)) {
        destinationDirPrefix += ETLStoreToUserName.get(storeName) + gobblinDbChanges;
      } else {
        destinationDirPrefix += props.getString(ETL_STORE_TO_ACCOUNT_NAME_DEFAULT) + gobblinDbChanges;
      }
      fs.mkdirs(new Path(destinationDirPrefix));
      StoreFilesInfo deltaInfo = entry.getValue();
      StoreResponse storeResponse = storeToControllerClient.get(storeName).getStore(storeName);
      StoreInfo storeInfo = storeResponse.getStore();
      String destinationStorePath = destinationDirPrefix + "/" + storeName;
      logger.info("Begin publishing Gobblin deltas for store " + storeName);
      writeMetadata(storeInfo, destinationStorePath, deltaInfo);
      publishVersionDeltas(storeName, deltaInfo, destinationStorePath);
    }
    checkAllExecutorsFinish(storeNameToExecutors);
    cleanOldDeltasInSource(storeToDeltasPath);
  }

  private void publishVersionDeltas(String storeName, StoreFilesInfo deltasPath, String storePath) throws Exception {
    try {
      Set<Integer> storeVersions = deltasPath.getVersions();
      for (int storeVersion : storeVersions) {
        String destination = createHourlyDeltaDir(storeVersion, storePath);
        PriorityQueue<Path> sourcePathsQueue = new PriorityQueue<>(deltasPath.getFilesByVersion(storeVersion));
        /**
         * The paths are popped from oldest to later
         */
        for (Path deltaSourcePath : sourcePathsQueue) {
          Path destinationDir = createDeltaDestinationDir(deltaSourcePath, destination);
          String destinationDirName = destinationDir.toString();
          /**
           * examine if the to-be-published hour directory is already in destination
           * TODO: If we run Gobblin workflow more than once per hour, this will be a problem.
           *       We need to compare each avro file to see if the to-be-published file already exists.
           */
          if (dirAlreadyExisted(destinationDir, deltaSourcePath)) {
            continue;
          }
          VeniceGobblinDistcp distcp = new VeniceGobblinDistcp(deltaSourcePath, destinationDir);
          // Add customized configs to the distcp job
          setupConfigs(distcp, this.props);
          storeNameToExecutors.get(storeName).execute(distcp);
          logger.info("Added distcp for store " + storeName + " deltas to " + destinationDirName + " to queue.");
        }
      }
    } catch (Exception e) {
      logger.error("Failed on file operations: ", e);
      throw e;
    }
  }

  /**
   * Write current version and future version info for a store to the metadata file.
   * During each publish the metadata will be overridden.
   */
  private void writeMetadata(StoreInfo storeInfo, String storePath, StoreFilesInfo deltaInfo) throws IOException {
    fs.mkdirs(new Path(storePath));
    int currentVersion = storeInfo.getColoToCurrentVersions().get(fabricName);
    Path metadataFile = new Path(storePath + "/metadata.txt");
    OutputStream os = fs.create(metadataFile);
    BufferedWriter br = new BufferedWriter(new OutputStreamWriter(os, "UTF-8"));
    br.write("current version : " + currentVersion);
    int futureVersion = deltaInfo.getLargestVersionNumber();
    if (futureVersion > currentVersion) {
      br.write("future version : " + futureVersion);
    }
    br.flush();
    br.close();
  }

  private void cleanOldDeltasInSource(Map<String, StoreFilesInfo> storeToDeltasPath) throws IOException {
    for (Map.Entry<String, StoreFilesInfo> entry : storeToDeltasPath.entrySet()) {
      StoreFilesInfo deltasPath = entry.getValue();
      for (int storeVersion : deltasPath.getVersions()) {
        PriorityQueue<Path> dailySourceDeltaPaths = new PriorityQueue<>(new Comparator<Path>() {
          @Override
          public int compare(Path o1, Path o2) {
            return -1 * o1.compareTo(o2);
          }
        });
        Set<Path> dailyDeltasPathSet = new HashSet<>();
        PriorityQueue<Path> sourcePathsQueue = deltasPath.getFilesByVersion(storeVersion);
        for (Path hourlyDeltaPath : sourcePathsQueue) {
          Path curr = hourlyDeltaPath.getParent();
          if (!dailyDeltasPathSet.contains(curr)) {
            dailySourceDeltaPaths.add(curr);
            dailyDeltasPathSet.add(curr);
          }
        }
        retireOldFiles(dailySourceDeltaPaths, fs, maxDailyDeltasToKeep);
        dailyDeltasPathSet.clear();
      }
    }
  }

  private String createHourlyDeltaDir(int storeVersion, String destination) throws IOException {
    destination += "/v" + storeVersion;
    fs.mkdirs(new Path(destination));
    destination += "/hourly";
    fs.mkdirs(new Path(destination));
    return destination;
  }

  /**
   * Creates the destination directory is the directory does not exist. Also cleans old daily folders under
   * month directory.
   * @param deltaPath path from source directory. e.g.
   *                  /jobs/voldteei/ETL/VeniceETL/dbchanges/KAFKA/venice_h2v_test_100_v3308/hourly/2019/10/22/13/
   * @param destination part of destionation. e.g. /jobs/veniceetlxxx/dbchanges/venice_h2v_test_100/v3308/hourly/
   * @return the path to put hour directory. e.g. /jobs/veniceetlxxx/dbchanges/venice_h2v_test_100/v3308/hourly/2019/10/22/
   * @throws IOException
   */
  private Path createDeltaDestinationDir(Path deltaPath, String destination) throws IOException {
    Path day = deltaPath.getParent();
    Path month = day.getParent();
    Path year = month.getParent();
    destination += "/" + year.getName();
    fs.mkdirs(new Path(destination));
    destination += "/" + month.getName();
    fs.mkdirs(new Path(destination));
    // Delete old daily folders under month directory in user destination dir
    retireDestinationOldFiles(new Path(destination), fs);
    Path destinationDir = new Path(destination + "/" + day.getName());
    fs.mkdirs(destinationDir);
    return destinationDir;
  }

  private boolean dirAlreadyExisted(Path destinationDir, Path deltaSourcePath) throws IOException {
    FileStatus[] hourDirs = fs.listStatus(destinationDir, PATH_FILTER);
    for (FileStatus file : hourDirs) {
      if (file.getPath().getName().equals(deltaSourcePath.getName())) {
        return true;
      }
    }
    return false;
  }

  private static void setUpOnDemandETLStores(VeniceProperties props, Set<String> onDemandETLEnabledStore) {
    /**
     * In the format of:
     * "{storeName1},{storeName2}"
     */
    String onDemandETLEnabledStores;
    try {
      onDemandETLEnabledStores = props.getString(ON_DEMAND_ETL_ENABLED_STORES);
    } catch (UndefinedPropertyException e) {
      logger.warn("The config for future-etl-enabled-stores doesn't exist.");
      return;
    }
    String[] tokens = onDemandETLEnabledStores.split(",");
    for (String token : tokens) {
      onDemandETLEnabledStore.add(token.trim());
    }
  }

  /**
   * Hadoop Path comparator which puts smaller path in the beginning, so the PriorityQueue will be a min heap
   * and the first element will be the oldest file. It is also the default comparator.
   */
  protected static final Comparator<Path> FILES_COMPARATOR = new Comparator<Path>() {
    @Override
    public int compare(Path o1, Path o2) {
      // smaller path comes first
      return o1.compareTo(o2);
    }
  };
}
