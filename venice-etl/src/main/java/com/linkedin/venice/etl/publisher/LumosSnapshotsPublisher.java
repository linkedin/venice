package com.linkedin.venice.etl.publisher;

import azkaban.jobExecutor.AbstractJob;
import com.linkedin.venice.security.SSLFactory;
import com.linkedin.venice.utils.SslUtils;
import java.util.Comparator;
import java.util.Optional;
import com.linkedin.venice.controllerapi.ControllerClient;
import com.linkedin.venice.controllerapi.StoreResponse;
import com.linkedin.venice.etl.client.VeniceKafkaConsumerClient;
import com.linkedin.venice.exceptions.UndefinedPropertyException;
import com.linkedin.venice.meta.StoreInfo;
import com.linkedin.venice.utils.Pair;
import com.linkedin.venice.utils.VeniceProperties;
import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.PriorityQueue;
import java.util.Properties;
import java.util.Set;
import org.apache.gobblin.runtime.api.JobExecutionDriver;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.log4j.Logger;

import static com.linkedin.venice.CommonConfigKeys.*;
import static com.linkedin.venice.ConfigKeys.*;
import static com.linkedin.venice.etl.client.VeniceKafkaConsumerClient.*;
import static com.linkedin.venice.etl.publisher.DataPublisherUtils.*;
import static com.linkedin.venice.etl.source.VeniceKafkaSource.*;


/**
 * Lumos Snapshots Publisher is an Azkaban workflow that will publish the latest ETL snapshot
 * for Venice stores to a specified HDFS path.
 *
 * In the snapshot source directory, Venice ETL will dump snapshots for different store
 * versions; different versions will end up in different directories. Publisher will talk
 * to Venice controller to figure out the current version of the stores and copy the latest
 * snapshot of the current version to the specified destination path.
 *
 * Publisher workflow can run asynchronously with other ETL workflow; publisher will skip
 * doing duplicate work if the latest snapshot is published already.
 */
public class LumosSnapshotsPublisher extends AbstractJob {
  private static final Logger logger = Logger.getLogger(LumosSnapshotsPublisher.class);
  /**
   * Any stores which enables future etl will have this suffix appended to job path.
   */
  private static final String FUTURE_ETL_SUFFIX = "_future";

  // Immutable state
  private final VeniceProperties props;
  private final String fabricName;
  private final String veniceControllerUrls;
  private final int maxDailySnapshotsToKeep;
  private final String snapshotSourceDir;
  private final String destinationDirPrefix;

  private FileSystem fs;
  private Optional<SSLFactory> sslFactory;
  private Map<Pair<String, String>, JobExecutionDriver> distcpJobFutures;
  private Map<String, String> ETLStoreToUserName;
  private Set<String> futureETLEnabledStores;

  public LumosSnapshotsPublisher(String jobId, Properties vanillaProps) throws Exception{
    super(jobId, logger);
    this.props = new VeniceProperties(vanillaProps);
    logger.info("Constructing " + LumosSnapshotsPublisher.class.getSimpleName() + ": " + props.toString(true));
    this.fabricName = props.getString(FABRIC_NAME);
    this.veniceControllerUrls = props.getString(VENICE_CONTROLLER_URLS);
    this.maxDailySnapshotsToKeep = props.getInt(ETL_MAX_SNAPSHOTS_TO_KEEP, DEFAULT_MAX_FILES_TO_KEEP);
    // the directory that contains the snapshots generated by Gobblin/Lumos pipeline,
    // which are the snapshots in version level
    this.snapshotSourceDir = props.getString(ETL_SNAPSHOT_SOURCE_DIR);
    // the directory where we publish the latest snapshot, and in the destination, we only specify store name in the path.
    this.destinationDirPrefix = props.getString(ETL_DESTINATION_DIR_PREFIX);
    // set up ssl factory to talk to controller
    this.sslFactory = Optional.of(SslUtils.getSSLFactory(setUpSSLProperties(props), props.getString(SSL_FACTORY_CLASS_NAME)));
    // gets configured HDFS filesystem for read and write
    try {
      this.fs = FileSystem.get(new Configuration());
    } catch (IOException e) {
      logger.error("Get configured FileSystem implementation failed: ", e);
      throw e;
    }

    // records all distcp job futures
    this.distcpJobFutures = new HashMap<>();

    // builds the map from etl store name to venice etl user name
    this.ETLStoreToUserName = new HashMap<>();
    setUpETLStoreToUserName(props, ETLStoreToUserName);

    // builds the set for future version etl enabled stores
    this.futureETLEnabledStores = new HashSet<>();
    setUpFutureETLStores(props, futureETLEnabledStores);
  }

  @Override
  public void run() throws Exception {
    // Get all latest ETL snapshots from the source directory
    Map<String, StoreFilesInfo> storeToSnapshotPath = getSnapshotsPath(snapshotSourceDir, fs, FILES_COMPARATOR);

    // Build ControllerClient, which will be used to determine the current version and potentially future version of each store
    Map<String, ControllerClient> storeToControllerClient = VeniceKafkaConsumerClient.getControllerClients(
        storeToSnapshotPath.keySet(), this.veniceControllerUrls, sslFactory);

    for (Map.Entry<String, StoreFilesInfo> entry : storeToSnapshotPath.entrySet()) {
      String storeName = entry.getKey();
      StoreFilesInfo snapshotInfo = entry.getValue();
      StoreResponse storeResponse = storeToControllerClient.get(storeName).getStore(storeName);
      StoreInfo storeInfo = storeResponse.getStore();

      // Get current version of the store and publish current version snapshots
      int currentVersion = storeInfo.getColoToCurrentVersions().get(fabricName);
      logger.info("Current version for store: " + storeName + " is " + currentVersion);
      publishVersionSnapshots(storeName, snapshotInfo, currentVersion, false);

      // Get future version and publish future version snapshots
      if (futureETLEnabledStores.contains(storeName)) {
        int futureVersion = snapshotInfo.getLargestVersionNumber();
        if (futureVersion > currentVersion) {
          logger.info("Future version for store: " + storeName + " is " + futureVersion);
          publishVersionSnapshots(storeName, snapshotInfo, futureVersion, true);
        } else {
          logger.info("Store " + storeName + " doesn't have a future version running yet. Skipped publishing.");
        }
      }
    }
    waitForDistcpJobs(this.distcpJobFutures);
  }

  private void publishVersionSnapshots(String storeNamePath, StoreFilesInfo snapshotInfo, int storeVersion, boolean isFutureVersion) throws Exception {
    try {
      // build destination path
      String destination = destinationDirPrefix;
      if (ETLStoreToUserName.containsKey(storeNamePath)) {
        destination += ETLStoreToUserName.get(storeNamePath);
      } else {
        destination += props.getString(ETL_STORE_TO_ACCOUNT_NAME_DEFAULT);
      }
      if (isFutureVersion) {
        storeNamePath += FUTURE_ETL_SUFFIX;
      }
      destination += "/" + storeNamePath;
      logger.info("Publishing ETL snapshot for store: " + storeNamePath + " to dir: " + destination);

      // Get snapshot list for the store version
      PriorityQueue<Path> snapshotQueue = snapshotInfo.getFilesByVersion(storeVersion);
      if (null == snapshotQueue || snapshotQueue.size() == 0) {
        logger.info("No snapshot for store: " + storeNamePath);
        return;
      }

      // Get the latest snapshot from the sorted snapshot list
      Path snapshotSourcePath = snapshotQueue.peek();
      String snapshotName = snapshotSourcePath.getName();

      // Create the directory for the store in destination if it doesn't exist before
      Path storePath = new Path(destination);
      if (!fs.exists(storePath)) {
        fs.mkdirs(storePath);
      }

      // check whether the last snapshot already exists in the publish directory
      destination += "/" + snapshotName + "_v" + storeVersion;
      Path snapshotDestinationPath = new Path(destination);
      if (fs.exists(snapshotDestinationPath)) {
        logger.info(
            "Skip copying snapshot " + snapshotName + " for store " + storeNamePath + ", because it already exists in " + destination);
        return;
      }

      /**
       * Start a Gobblin distributed cp job asynchronously to release snapshot;
       * Distcp for different stores will run in parallel;
       * Gobblin distcp is a MapReduce job which can speed up the data movement process a lot.
       */
      logger.info("Starting distcp for store " + storeNamePath);
      VeniceGobblinDistcp distcp = new VeniceGobblinDistcp(snapshotSourcePath, snapshotDestinationPath);
      // Add customized configs to the distcp job
      setupConfigs(distcp, this.props);
      JobExecutionDriver jobFuture = distcp.runAsync();
      distcpJobFutures.put(Pair.create(storeNamePath, destination), jobFuture);
      logger.info("Started distcp for store " + storeNamePath + " snapshot " + snapshotName);

      // clean up old snapshots in source
      retireOldFiles(snapshotQueue, fs, maxDailySnapshotsToKeep);
      // clean up old snapshots in destination
      retireDestinationOldFiles(storePath, fs);
    } catch (Exception e) {
      logger.error("Failed on file operations: ", e);
      throw e;
    }
  }

  private static void setUpFutureETLStores(VeniceProperties props, Set<String> futureETLStores) {
    /**
     * In the format of:
     * "{storeName1},{storeName2}"
     */
    String futureETLEnabledStores;
    try {
      futureETLEnabledStores = props.getString(FUTURE_ETL_ENABLED_STORES);
    } catch (UndefinedPropertyException e) {
      logger.warn("The config for future-etl-enabled-stores doesn't exist.");
      return;
    }
    String[] tokens = futureETLEnabledStores.split(",");
    for (String token : tokens) {
      futureETLStores.add(token.trim());
    }
  }

  /**
   * Hadoop Path comparator which puts bigger path in the beginning, so the PriorityQueue will be a max heap
   * and the first element will be the latest file.
   */
  protected static final Comparator<Path> FILES_COMPARATOR = new Comparator<Path>() {
    @Override
    public int compare(Path o1, Path o2) {
      // Bigger path comes first
      return -1 * o1.compareTo(o2);
    }
  };
}
