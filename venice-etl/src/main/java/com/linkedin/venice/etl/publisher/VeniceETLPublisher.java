package com.linkedin.venice.etl.publisher;

import azkaban.jobExecutor.AbstractJob;
import com.linkedin.venice.controllerapi.ControllerClient;
import com.linkedin.venice.controllerapi.StoreResponse;
import com.linkedin.venice.etl.client.VeniceKafkaConsumerClient;
import com.linkedin.venice.meta.StoreInfo;
import com.linkedin.venice.meta.Version;
import com.linkedin.venice.utils.VeniceProperties;
import java.io.IOException;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Map;
import java.util.PriorityQueue;
import java.util.Properties;
import java.util.TreeMap;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FileUtil;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.fs.PathFilter;
import org.apache.log4j.Logger;

import static com.linkedin.venice.ConfigKeys.*;
import static com.linkedin.venice.etl.source.VeniceKafkaSource.*;


/**
 * Venice ETL publisher is an Azkaban workflow that will publish the latest ETL snapshot
 * for Venice stores to a specified HDFS path.
 *
 * In the snapshot source directory, Venice ETL will dump snapshots for different store
 * versions; different versions will end up in different directories. Publisher will talk
 * to Venice controller to figure out the current version of the stores and copy the latest
 * snapshot of the current version to the specified destination path.
 *
 * Publisher workflow can run asynchronously with other ETL workflow; publisher will skip
 * doing duplicate work if the latest snapshot is published already.
 */
public class VeniceETLPublisher extends AbstractJob {
  private static final Logger logger = Logger.getLogger(VeniceETLPublisher.class);

  /**
   * Daily ETL snapshot format is "timestamp-PT-recordNumber"
   */
  public static final String SNAPSHOT_SEPARATOR = "-PT-";
  /**
   * The regex is "\A[0-9]+-PT-[0-9]+\z", which matches the strings like:
   * 1570183273436-PT-558022380
   * 1570269674164-PT-558132495
   */
  public static final Pattern SNAPSHOT_PATTERN = Pattern.compile("\\A[0-9]+" + SNAPSHOT_SEPARATOR + "[0-9]+\\z");

  /**
   * Hadoop Path comparator which puts bigger path in the beginning, so the PriorityQueue will be a max heap
   * and the first element will be the latest snapshot.
   */
  private static final Comparator<Path> SNAPSHOT_COMPARATOR = new Comparator<Path>() {
    @Override
    public int compare(Path o1, Path o2) {
      // Bigger path comes first
      return -1 * o1.compareTo(o2);
    }
  };
  private static int INITIAL_QUEUE_SIZE = 10;

  // Immutable state
  private final VeniceProperties props;
  private final String veniceControllerUrls;
  private final String fabricName;
  private final String jobId;

  public VeniceETLPublisher(String jobId, Properties vanillaProps) {
    super(jobId, logger);
    this.jobId = jobId;
    this.props = new VeniceProperties(vanillaProps);
    logger.info("Constructing " + VeniceETLPublisher.class.getSimpleName() + ": " + props.toString(true));
    this.veniceControllerUrls = props.getString(VENICE_CONTROLLER_URLS);
    this.fabricName = props.getString(FABRIC_NAME);
  }

  @Override
  public void run() throws Exception {
    /**
     * Get the ETL snapshot publish source path and destination path from job properties.
     *
     * Source -- the directory that contains the snapshots generated by Gobblin/Lumos pipeline, which are the
     *           snapshots in version level;
     * Destination -- the directory where we publish the latest snapshot, and in the destination, we only specify
     *                store name in the path.
     */
    String snapshotSourceDir = props.getString(ETL_SNAPSHOT_SOURCE_DIR);
    String snapshotDestinationDir = props.getString(ETL_SNAPSHOT_DESTINATION_DIR);
    if (!snapshotDestinationDir.endsWith("/")) {
      snapshotDestinationDir = snapshotDestinationDir + "/";
    }

    try {
      Configuration conf = new Configuration();
      FileSystem fs = FileSystem.get(conf);
      // Get all latest ETL snapshots from the source directory
      Map<String, StoreETLSnapshotInfo> storeToSnapshotPath = getSnapshotPath(snapshotSourceDir, fs);

      // Build ControllerClient, which will be used to determine the current version of each store
      Map<String, ControllerClient> storeToControllerClient = VeniceKafkaConsumerClient.getControllerClients(
          storeToSnapshotPath.keySet().toArray(new String[storeToSnapshotPath.size()]), this.veniceControllerUrls);
      for (Map.Entry<String, StoreETLSnapshotInfo> entry : storeToSnapshotPath.entrySet()) {
        String storeName = entry.getKey();
        logger.info("Publishing ETL snapshot for store: " + storeName + " to dir: " + snapshotDestinationDir + storeName);
        StoreETLSnapshotInfo snapshotInfo = entry.getValue();

        // Get current version of the store
        StoreResponse storeResponse = storeToControllerClient.get(storeName).getStore(storeName);
        StoreInfo storeInfo = storeResponse.getStore();
        Map<String, Integer> coloToCurrentVersions = storeInfo.getColoToCurrentVersions();
        int currentVersion = coloToCurrentVersions.get(fabricName);
        logger.info("Current version for store: " + storeName + " is " + currentVersion);

        // Get snapshot list for the current version
        PriorityQueue<Path> snapshotQueue = snapshotInfo.getSnapshotsByVersion(currentVersion);
        if (null == snapshotQueue || snapshotQueue.size() == 0) {
          logger.info("No snapshot for store: " + storeName);
          continue;
        }

        // Get the latest snapshot from the sorted snapshot list
        Path snapshotSourcePath = snapshotQueue.poll();
        String snapshotName = snapshotSourcePath.getName();

        // Create the directory for the store in destination if it doesn't exist before
        String destination = snapshotDestinationDir + storeName;
        Path storePath = new Path(destination);
        if (!fs.exists(storePath)) {
          fs.mkdirs(storePath);
        }

        // check whether the last snapshot already exists in the publish directory
        destination = destination + "/" + snapshotName;
        Path snapshotDestinationPath = new Path(destination);
        if (fs.exists(snapshotDestinationPath)) {
          logger.info("Skip copying snapshot " + snapshotName + " for store " + storeName + ", because it already exists in " + destination);
          continue;
        }

        // Publish the latest snapshot to destination
        FileUtil.copy(fs, snapshotSourcePath, fs, snapshotDestinationPath, false, conf);
        logger.info("Successfully publish the latest snapshot " + snapshotName + " for store " + storeName + " in " + destination);
      }
    } catch (Exception e) {
      logger.error("Failed on file operations: ", e);
      throw e;
    }
  }

  /**
   * The file structure in the snapshot source directory:
   * /path/to/source/storeName_version/timestamp-PT-recordNumber/ListOfAvroFiles
   *
   * The input snapshotsPath points to "/path/to/source/" in the above example,
   * this helper function will parse the snapshot path and return the state in
   * structured format:
   *              | version_10 -> list of snapshot paths sorted by timestamp
   * storeName -> | version_11 -> list of snapshot paths sorted by timestamp
   *              | ....
   */
  private Map<String, StoreETLSnapshotInfo> getSnapshotPath(String snapshotsPath, FileSystem fs)
        throws IOException {
    Map<String, StoreETLSnapshotInfo> snapshotInfoMap = new HashMap<>();
    Path path = new Path(snapshotsPath);
    FileStatus[] fileStatuses = fs.listStatus(path, PATH_FILTER);
    for (FileStatus fileStatus : fileStatuses) {
      if (!fileStatus.isDirectory()) {
        logger.info("Skipping " + fileStatus.getPath().getName());
        continue;
      }

      Path filePath = fileStatus.getPath();
      String storeVersion = filePath.getName();
      if (!Version.topicIsValidStoreVersion(storeVersion)) {
        logger.info("Skipping file path " + storeVersion + " because the table name doesn't contain version suffix");
        continue;
      }

      // build snapshot info collection for this store version
      String storeName = Version.parseStoreFromKafkaTopicName(storeVersion);
      int version = Version.parseVersionFromKafkaTopicName(storeVersion);
      StoreETLSnapshotInfo snapshotInfo = snapshotInfoMap.get(storeName);
      if (null == snapshotInfo) {
        snapshotInfo = new StoreETLSnapshotInfo(storeName);
        snapshotInfoMap.put(storeName, snapshotInfo);
      }

      // add all snapshots of this version to the list
      FileStatus[] snapshotStatuses = fs.listStatus(filePath, PATH_FILTER);
      for (FileStatus snapshotStatus : snapshotStatuses) {
        if (!snapshotStatus.isDirectory()) {
          logger.info("Skipping " + snapshotStatus.toString() + " because it's not a snapshot directory");
          continue;
        }

        Path snapshot = snapshotStatus.getPath();
        if (isValidSnapshotPath(snapshot.getName())) {
          snapshotInfo.addSnapshot(version, snapshot);
        }
      }
    }
    return snapshotInfoMap;
  }

  /**
   * A helper function to check whether the input string follow such
   * @param path
   * @return
   */
  public static boolean isValidSnapshotPath(String path) {
    Matcher m = SNAPSHOT_PATTERN.matcher(path);
    return m.matches();
  }

  /**
   * A helper class that stores the version to snapshot list mapping.
   */
  private static class StoreETLSnapshotInfo {
    String storeName;
    Map<Integer, PriorityQueue<Path>> versionToSnapshotPath;

    StoreETLSnapshotInfo(String storeName) {
      this.storeName = storeName;
      // Version are sorted in TreeMap, which is for future version ETL support (TODO)
      this.versionToSnapshotPath = new TreeMap<>();
    }

    /**
     * Snapshot is added to list in sorted order for two reasons:
     * 1. Return the latest snapshot easily;
     * 2. Keep a list of snapshots instead of a single reference to latest snapshot for automatic clean up (TODO)
     * @param version
     * @param snapshot
     */
    protected void addSnapshot(int version, Path snapshot) {
      PriorityQueue<Path> snapshotQueue = this.versionToSnapshotPath.get(version);
      if (null == snapshotQueue) {
        snapshotQueue = new PriorityQueue<>(INITIAL_QUEUE_SIZE, SNAPSHOT_COMPARATOR);
        this.versionToSnapshotPath.put(version, snapshotQueue);
      }
      snapshotQueue.offer(snapshot);
    }

    protected PriorityQueue<Path> getSnapshotsByVersion(int version) {
      return versionToSnapshotPath.get(version);
    }
  }

  /**
   * ignore hdfs files with prefix "_" and "."
   */
  public static final PathFilter PATH_FILTER = p -> !p.getName().startsWith("_") && !p.getName().startsWith(".");
}
