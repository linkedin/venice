{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b784a202-e278-4da1-8070-8f1b3aaf4bd5",
   "metadata": {},
   "source": [
    "![Alt text](venice-logo-lion.png \"Venice Logo\")\n",
    "\n",
    "# Welcome To Venice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52225a84-96f1-4093-9de7-435d6b7a3d43",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "This notebook is meant to serve as an interactive demo of Venice and some of it's concepts.  In this workbook we'll demo:\n",
    "\n",
    "* Preparing and Processing a dataset with Spark\n",
    "* Preparing a Venice store\n",
    "* Pushing the dataset to Venice and querying it\n",
    "\n",
    "To this end, we're going to demo a simple workflow that an AI engineer may go through when publishing processing and Publishing data to Venice.\n",
    "First, we'll start by getting a dataset from Hugging Face.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546e686d-63bf-4d76-ba96-ceff46e9628b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing and Processing a dataset with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43106d-d33e-4617-8756-978e706d3df4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "For this demo to be interesting we'll need an interesting dataset, and the means to prepare it.  We'll use spark and a dataset from [Hugging Face](https://huggingface.co/)\n",
    "\n",
    "### What is Spark?\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) is an open source data processing engine that is quite popular.  It enables users to set up data processing jobs in a distributed manner on a cluster (if you're familiar with Hadoop you probably understand some of the core concepts).  Venice supports writing data to a Venice store via Spark, and we will show case that capability here.  In the below cell, we'll use python and pyspark as our means to programmatically interact with a spark cluster that is running as part of this demo.\n",
    "\n",
    "### What is our dataset?\n",
    "\n",
    "We're going to grab a dataset from Hugging Face and upload it for processing to our spark cluster.  This dataset is a table of wine reviews!  Yay!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729ffdd-580a-4851-916b-981da2126793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "dataset = \"alfredodeza/wine-ratings\"\n",
    "\n",
    "# Initialize our Spark Session\n",
    "spark = SparkSession.builder.appName(\"VeniceWineReviewsDemo\").getOrCreate()\n",
    "\n",
    "# Query HuggingFace for the appropriate request paths for our dataset\n",
    "HUGGING_FACE_PARQUET_API = \"https://huggingface.co/api/datasets/{dataset}/parquet\"\n",
    "r = requests.get(HUGGING_FACE_PARQUET_API.format(dataset=dataset))\n",
    "train_parquet_files = r.json()['default']['train']\n",
    "\n",
    "# Download the datasets to our spark cluster\n",
    "for url in train_parquet_files:\n",
    "  spark.sparkContext.addFile(url)\n",
    "\n",
    "# Process the parquet files on the spark cluster so we can query them\n",
    "df = spark.read.parquet(SparkFiles.getRootDirectory() + \"/*.parquet\")\n",
    "\n",
    "# Displaying first 10 rows of our dataset\n",
    "df.show(n=10)\n",
    "\n",
    "# It's normally good practice to stop your spark session at the end, however, we're\n",
    "# going to reuse some of this state in the following cells, so we'll leave this commented\n",
    "# out for now.  If for your own purposes you want to try and play with other datasets,\n",
    "# be sure to run with spark.close between runs so as to clear out state and avoid errors.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7929a1-9e2a-48d9-85a3-23ca143153ae",
   "metadata": {},
   "source": [
    "### Using Spark To Prepare Our Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3344245-5a77-4ec6-83fa-3f9f301ecca4",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "\n",
    "Now that we have our dataset, we need to manipulate it a bit. \n",
    "\n",
    "Venice is a key/value data store which uses [Avro](https://avro.apache.org/) as it's serialization format.  To that end, we'll need to process and transform our dataset to have a key/value structure and serialize it to Avro.\n",
    "\n",
    "The key to our dataset in Venice needs to be both non nullable and unique.  So we'll need to make sure those constraints are applied to our dataset before we can push it.  Looking at our dataset, we want to select a field for our key.  'name' seems a likely choice (and who wouldn't want a database where you can look up a wine rating by it's name!).  So we'll have spark process this data to form key/value pairs by grouping the columns in our dataset, remove null entries and duplicates, and finally serialize the dataset to avro.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b03ff5-0c50-47d2-b507-fae24306c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Create a struct type for the value column and group together the columns\n",
    "# which will make up our field value in the Venice store.\n",
    "df_transformed = df.withColumn(\"value\", struct(\n",
    "    col(\"region\").alias(\"region\"),\n",
    "    col(\"variety\").alias(\"variety\"),\n",
    "    col(\"rating\").alias(\"rating\"),\n",
    "    col(\"notes\").alias(\"notes\")\n",
    ")).select(\n",
    "    col(\"name\").alias(\"key\"),\n",
    "    col(\"value\")\n",
    ").dropDuplicates([\"key\"])\n",
    "\n",
    "# Define the new schema with key as non-nullable\n",
    "updated_schema = StructType([\n",
    "    StructField(\"key\", StringType(), nullable=False),\n",
    "    StructField(\"value\", StructType([\n",
    "        StructField(\"region\", StringType(), nullable=True),\n",
    "        StructField(\"variety\", StringType(), nullable=True),\n",
    "        StructField(\"rating\", FloatType(), nullable=True),\n",
    "        StructField(\"notes\", StringType(), nullable=True)\n",
    "    ]), nullable=False)\n",
    "])\n",
    "\n",
    "# Create new DataFrame with updated schema\n",
    "df_non_nullable_key = spark.createDataFrame(df_transformed.rdd, schema=updated_schema)\n",
    "\n",
    "# Show the resulting DataFrame schema\n",
    "df_non_nullable_key.printSchema()\n",
    "\n",
    "df_non_nullable_key.show(n=3)\n",
    "df_non_nullable_key.write.format(\"avro\").save(\"transformed_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba56d77-efa7-4173-a1f9-0bb63e09b3f3",
   "metadata": {},
   "source": [
    "## Preparing a Venice store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f7cc8-dc55-498b-baf9-9f8a02247ea5",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "Looking good!  So we've got our data set.  Now we need to prepare a Venice store.  We've set up some endpoints in this demo that will enable you to interact with a venice cluster called \"venice-cluster0\".  We'll create a store called \"wine-ratings-store\" and we'll initialize the store with a schema that matches our dataset.  It's possible to generate the schema from the parqet file in the spark session, but to simplify this demo we've already generated the schema files.  We'll create our venice store with the below code:\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e1bb2-9be4-493a-84a7-98ed0496a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Utility function for printing command output\n",
    "def log_subprocess_output(pipe):\n",
    "    for line in iter(pipe.readline, b''):\n",
    "        print(line)\n",
    "\n",
    "# Arguments for Venice store creation\n",
    "create_store_args = [\n",
    "    \"./create-store.sh\",\n",
    "    \"http://venice-controller:5555\", \n",
    "    \"venice-cluster0\",\n",
    "    \"wine-ratings-store\", # Name of our store\n",
    "    \"~/wineKeySchema.avsc\",\n",
    "    \"~/wineValueSchema.avsc\"\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(create_store_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dcdbfa-3a42-4e5d-983f-90cb673d7b1c",
   "metadata": {},
   "source": [
    "## Pushing the dataset to Venice and querying it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b012e-0fd4-4e49-aafa-34c4a519d5c3",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "And now the moment we've been waiting for!  We've prepared our dataset and we've created our venice store.  Now let's transmit the data to Venice via spark!\n",
    "    \n",
    "With this demo we've provided the push job as a java jar which we'll submit to our spark cluster.  We've also prepared a properties file with the push job which contains some of the mandatory configurations needed for the push (things like the name of the store, the cluster's endpoint, and the path to the data that we'll need to transmit).  You can take a look at these configurations in batch-push-job.properties in the home directory of this demo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff9b8b-7afd-4e1d-b460-55eb7ae0acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the push job jar Java JAR file\n",
    "jar_path = \"bin/venice-push-job-all.jar\"\n",
    "\n",
    "# Arguments for spark-submit\n",
    "spark_submit_args = [\n",
    "    \"spark-submit\",\n",
    "    \"--class\", \"com.linkedin.venice.hadoop.VenicePushJob\",  # Push job main class\n",
    "    jar_path,\n",
    "    \"batch-push-job.properties\"  # Configurations\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(spark_submit_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeaaee5-85f3-4efc-87ea-b89d9c4ace25",
   "metadata": {},
   "source": [
    "### Query the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100be71-2273-4f56-b448-98325be6a948",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588);\">\n",
    "And now finally, let's query this dataset from Venice!!\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b021f-d447-4b61-9888-4d703d8dc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the store for the data\n",
    "query_store_args = [\n",
    "    \"./fetch.sh\",\n",
    "    \"http://venice-router:7777\", \n",
    "    \"wine-ratings-store\",\n",
    "    \"A. Mano Puglia Primitivo 2006\",\n",
    "]\n",
    "\n",
    "# Submit the job\n",
    "process = Popen(query_store_args, stdout=PIPE, stderr=STDOUT)\n",
    "with process.stdout:\n",
    "    log_subprocess_output(process.stdout)\n",
    "exitcode = process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb39a7-66fe-43cc-944a-d0ac9aa235e5",
   "metadata": {},
   "source": [
    "__Saluti!!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c214620-c1a5-42c9-bf3c-c1ecad0b4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to clean up your spark session afterward\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
