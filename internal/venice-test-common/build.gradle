import java.nio.file.Files
import java.nio.file.Path
import java.nio.file.Paths
import java.nio.file.StandardCopyOption
import java.nio.file.StandardOpenOption


apply {
  plugin 'me.champeau.jmh'
}

jmh {
  fork = 3
  warmupForks = 1
  iterations = 10
  warmupIterations = 5
  timeUnit = 'ns'
  resultFormat = 'json'
  failOnError = true
  includeTests = false
  profilers = ['gc']
  benchmarkMode = ['sample']
  includes = ['DaVinciClientBenchmark']
  jvmArgs = ['-Xms4G', '-Xmx4G', '-Djmh.shutdownTimeout=0', '-Djmh.shutdownTimeout.step=0']
}

jmhJar {
  zip64 = true
}

configurations {
  all {
    resolutionStrategy {
      force libraries.kafka
      force libraries.javax
    }
  }
  implementation {
    exclude group: 'org.apache.kafka'
    exclude group: 'org.mortbay.jetty', module: 'servlet-api'
  }
  integrationTestImplementation.extendsFrom testImplementation
  integrationTestUtils
}

sourceSets {
  integrationTest {
    // 'src/integrationTest/java' is in srcDir by default. Just add the proto directory
    java.srcDir 'src/integrationTest/proto'
    resources
  }
  jmh {
    // 'src/jmh/java' is in srcDir by default. Just add the proto directory
    java.srcDir 'src/jmh/proto'
    resources
  }
}

dependencies {
  implementation (libraries.d2) {
    exclude group: 'com.oracle', module: 'ojdbc14' // unused transitive dependencies, doesn't exist in repo
    // this will introduce another different mockito-all version
    exclude group: 'org.mockito', module: 'mockito-all'
  }
  implementation project(':clients:da-vinci-client')
  implementation project(':clients:venice-client')
  implementation project(':clients:venice-push-job')
  implementation project(':internal:venice-common')
  implementation project(':services:venice-controller')
  implementation project(':services:venice-router')
  implementation project(':clients:venice-samza')
  implementation project(':clients:venice-producer')
  implementation project(':internal:venice-client-common')
  implementation project(':services:venice-server')
  implementation project(':clients:venice-thin-client')
  implementation project(':internal:alpini:netty4:alpini-netty4-base')
  implementation project(':internal:alpini:router:alpini-router-api')
  implementation project(':internal:alpini:router:alpini-router-base')

  implementation libraries.avroUtilCompatHelper
  implementation libraries.avroUtilFastserde
  implementation libraries.commonsCli
  implementation libraries.conscrypt
  implementation libraries.fastUtil
  implementation (libraries.hadoopCommon) {
    exclude group: 'javax.servlet'
  }
  implementation libraries.helix
  implementation libraries.httpAsyncClient
  implementation libraries.javax
  implementation libraries.kafka
  implementation libraries.kafkaClients
  implementation libraries.kafkaClientsTest
  implementation libraries.mockito
  implementation libraries.rocksdbjni
  implementation libraries.samzaApi
  implementation libraries.spark
  implementation libraries.testng

  implementation (libraries.mapreduceClientJobClient) {
    exclude group: 'org.apache.avro'
    exclude group: 'javax.servlet'
  }
  testImplementation project(':clients:venice-admin-tool')
  testImplementation project(':internal:alpini:common:alpini-common-base')
  testImplementation project(':internal:venice-common').sourceSets.test.output
  testImplementation libraries.log4j2core
  testImplementation libraries.log4j2api

  jmhAnnotationProcessor 'org.openjdk.jmh:jmh-generator-annprocess:' + jmh.jmhVersion.get()
  jmhImplementation project(path: ':internal:venice-test-common', configuration: 'integrationTestUtils')
}

def integrationTestConfigs = {
  mustRunAfter test
  classpath = sourceSets.integrationTest.runtimeClasspath
  testClassesDirs = sourceSets.integrationTest.output.classesDirs
  forkEvery = Integer.parseInt(project.properties.get('integrationTest.forkEvery', "$forkEvery"))
  maxParallelForks = Integer.parseInt(project.properties.get('integrationTest.maxParallelForks', "$maxParallelForks"))
}

def integrationTestBuckets = [
  "A": [
      "com.linkedin.davinci.*",
      "com.linkedin.venice.endToEnd.DaVinciClient*"],
  "B": [
      "com.linkedin.venice.endToEnd.DaVinciCluster*",
      "com.linkedin.venice.endToEnd.DaVinciCompute*",
      "com.linkedin.venice.endToEnd.DaVinciLive*"],
  "C": [
      "com.linkedin.venice.endToEnd.ActiveActive*"],
  "D": [
      "com.linkedin.venice.endToEnd.TestActiveActive*"],
  "E": [
      "com.linkedin.venice.helix.*",
      "com.linkedin.venice.helixrebalance.*"],
  "F": [
      "com.linkedin.venice.server.*",
      "com.linkedin.venice.storagenode.*",
      "com.linkedin.venice.restart.*"],
  "G": [
      "com.linkedin.venice.router.*",
      "com.linkedin.venice.fastclient.AvroStoreClientGzipEndToEndTest",
      "com.linkedin.venice.fastclient.AvroStoreClientZstdEndToEndTest"],
  "H": [
      "com.linkedin.venice.fastclient.BatchGet*",
      "com.linkedin.venice.fastclient.schema.*",
      "com.linkedin.venice.fastclient.meta.*",
      "com.linkedin.venice.endToEnd.TestEmptyPush"],
  "I": [
      "com.linkedin.venice.fastclient.AvroStoreClientEndToEndTest"],
  "J": [
      "com.linkedin.venice.hadoop.*",
      "com.linkedin.venice.endToEnd.TestVson*",
      "com.linkedin.venice.endToEnd.Push*"],
  "K": [
      "com.linkedin.venice.endToEnd.TestPushJob*"],
  "L": [
      "com.linkedin.venice.endToEnd.TestBatch*"],
  "M": [
      "com.linkedin.venice.kafka.*",
      "com.linkedin.venice.samza.*",
      "com.linkedin.venice.writer.*",
      "com.linkedin.venice.endToEnd.PartialUpdateTest",
      "com.linkedin.venice.endToEnd.TestWritePathComputation",
      "com.linkedin.venice.endToEnd.WriteComputeWithActiveActiveReplicationTest"],
  "N": [
      "com.linkedin.venice.endToEnd.StoragePersona*",
      "com.linkedin.venice.endToEnd.TestStoreUpdateStoragePersona",
      "com.linkedin.venice.persona.*"],
  "O": [
      "com.linkedin.venice.endToEnd.TestHybrid*"],
  "P": [
      "com.linkedin.venice.controller.server.*",
      "com.linkedin.venice.controller.kafka.consumer.*",
      "com.linkedin.venice.controller.migration.*",
      "com.linkedin.venice.controller.AdminTool*",
      "com.linkedin.venice.controller.VeniceParentHelixAdminTest"],
  "Q": [
      "com.linkedin.venice.controller.Test*"]
]

integrationTestBuckets.each { name, patterns ->
  task "integrationTest${name}" (type: Test) {
    filter {
      patterns.each { pattern ->
        includeTestsMatching pattern
      }
    }
    configure integrationTestConfigs
    useTestNG {
      excludeGroups 'flaky'
      listeners = ['com.linkedin.venice.testng.VeniceSuiteListener', 'com.linkedin.venice.testng.VeniceTestListener']
    }
  }
}

task integrationTestZ(type: Test) {
  filter {
    integrationTestBuckets.each { name, patterns ->
      patterns.each { pattern ->
        excludeTestsMatching pattern
      }
    }
  }

  configure integrationTestConfigs
  useTestNG {
    excludeGroups 'flaky'
    listeners = ['com.linkedin.venice.testng.VeniceSuiteListener', 'com.linkedin.venice.testng.VeniceTestListener']
  }
}

task generateGHCI() {
  def targetDir = rootDir.getPath() + "/.github/rawWorkflows/"
  def targetFile = new File(targetDir, "gh-ci.yml")
  def targetFilePath = Paths.get(targetFile.getPath())

  def validateGradleWrapperFile = new File(targetDir, "gh-ci-validate-gradle-wrapper.txt")
  def validateGradleWrapperFilePath = Paths.get(validateGradleWrapperFile.getPath())
  def validateGradleWrapperFileContent = new String(Files.readAllBytes(validateGradleWrapperFilePath))

  def paramFile = new File(targetDir, "gh-ci-parameterized-flow.txt")
  def paramFilePath = Paths.get(paramFile.getPath())
  def paramFileContent = new String(Files.readAllBytes(paramFilePath))

  def alpiniParamFile = new File(targetDir, "gh-ci-alpini-parametrized-flow.txt")
  def alpiniParamFilePath = Paths.get(alpiniParamFile.getPath())
  def alpiniParamFileContent = new String(Files.readAllBytes(alpiniParamFilePath))

  targetFile.delete()
  targetFile.createNewFile()

  append(targetFilePath, "# Auto-generated file. Do not edit manually!\n#\n")
  append(targetFilePath, "# To alter these flows, edit:\n#\n#     internal/venice-test-common/build.gradle\n#\n")
  append(targetFilePath, "# To regenerate, run:\n#\n#     ./gradlew generateGHCI\n\n")

  append(targetFilePath, "name: VeniceCI\n\n")
  append(targetFilePath, "on: [push, pull_request, workflow_dispatch]\n\n")
  append(targetFilePath, "jobs:\n")

  def jobs = []

  def validateGradleWrapper = "ValidateGradleWrapper"
  appendToGHCI(validateGradleWrapperFileContent, targetFilePath, validateGradleWrapper, 5, "")
  jobs << validateGradleWrapper

  def common = "--stacktrace --continue --no-daemon "
  def staticAnalysisFlowGradleArgs = common + "clean check --parallel -Pspotallbugs -x test -x integrationTest -x jacocoTestCoverageVerification"

  def staticAnalysis = "StaticAnalysis"
  appendToGHCI(paramFileContent, targetFilePath, staticAnalysis, 20, staticAnalysisFlowGradleArgs)
  jobs << staticAnalysis

  def unitTestsAndCodeCoverage = "UnitTestsAndCodeCoverage"
  def unitTestsFlowGradleArgs = "-x :internal:venice-avro-compatibility-test:test jacocoTestCoverageVerification diffCoverage --continue"
  appendToGHCI(paramFileContent, targetFilePath, unitTestsAndCodeCoverage, 60, unitTestsFlowGradleArgs)
  jobs << unitTestsAndCodeCoverage

  def avroCompatibilityTests = "AvroCompatibilityTests"
  def avroCompatibilityFlowGradleArgs = "-DmaxParallelForks=2 --parallel :internal:venice-avro-compatibility-test:test --continue"
  appendToGHCI(paramFileContent, targetFilePath, avroCompatibilityTests, 60, avroCompatibilityFlowGradleArgs)
  jobs << avroCompatibilityTests

  def integTestGradleArgs = common + "-DforkEvery=1 -DmaxParallelForks=1 integrationTest"
  integrationTestBuckets.each { name, patterns ->
    def flowName = "IntegrationTests" + name
    jobs << flowName
    appendToGHCI(paramFileContent, targetFilePath, flowName, 120, integTestGradleArgs + name)
  }
  def otherTest = "IntegrationTestsZ"
  appendToGHCI(paramFileContent, targetFilePath, otherTest, 120, integTestGradleArgs + "Z")
  jobs << otherTest

  def alpiniUnitTests = "AlpiniUnitTests"
  appendToGHCI(alpiniParamFileContent, targetFilePath, alpiniUnitTests, 120, common + "-DmaxParallelForks=1 alpiniUnitTest")
  jobs << alpiniUnitTests

  def alpiniFunctionalTests = "AlpiniFunctionalTests"
  appendToGHCI(alpiniParamFileContent, targetFilePath, alpiniFunctionalTests, 120, common + "-DmaxParallelForks=1 alpiniFunctionalTest")
  jobs << alpiniFunctionalTests

  // define a job that depends others to manage the status check
  appendToGHCI(paramFileContent, targetFilePath, "DummyStep", 120, "clean", jobs)

  def finalDestinationPath = Paths.get(rootDir.getPath() + "/.github/workflows/gh-ci.yml")
  Files.move(targetFilePath, finalDestinationPath, StandardCopyOption.REPLACE_EXISTING)
}

def appendToGHCI(String paramFileContent, Path targetFilePath, String flowName, int timeOut, String gradleArguments, ArrayList dependency=null, String conditional=null) {
  String postProcessing = paramFileContent
      .replace("\$FlowName", flowName)
      .replace("\$TimeOut", Integer.toString(timeOut))
      .replace("\$GradleArguments", gradleArguments)

  if (dependency == null) {
    postProcessing = postProcessing.replace("    needs: \$Dependency\n", "")
  } else {
    postProcessing = postProcessing.replace("\$Dependency", dependency.toString())
  }

  if (conditional == null) {
    postProcessing = postProcessing.replace("    if: \$Conditional\n", "")
  } else {
    postProcessing = postProcessing.replace("\$Conditional", conditional.toString())
  }

  append(targetFilePath, postProcessing)
  append(targetFilePath, "\n")
}

def append(Path targetFilePath, String content) {
  Files.write(targetFilePath, content.getBytes(), StandardOpenOption.APPEND)
}

task integrationTest(type: Test) {
  configure integrationTestConfigs
}
check.dependsOn(integrationTest)

flakyTest {
  classpath += sourceSets.integrationTest.runtimeClasspath
  testClassesDirs += sourceSets.integrationTest.output.classesDirs
}

idea {
  module {
    testSourceDirs += project.sourceSets.integrationTest.java.srcDirs
    testSourceDirs += project.sourceSets.jmh.java.srcDirs

    testResourceDirs += project.sourceSets.integrationTest.resources.srcDirs
  }
}

task integrationTestJar(type: Jar) {
  classifier 'integrationTest'
  from sourceSets.integrationTest.output
}

artifacts {
  integrationTestUtils integrationTestJar
}

ext {
  jacocoCoverageThreshold = 0.00
  diffCoverageThreshold = 0.00
}

publishing {
  publications {
    "${project.name}" (MavenPublication) {
      artifact integrationTestJar
    }
  }
}
