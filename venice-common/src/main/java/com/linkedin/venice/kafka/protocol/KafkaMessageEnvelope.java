/**
 * Autogenerated by Avro
 * 
 * DO NOT EDIT DIRECTLY
 */
package com.linkedin.venice.kafka.protocol;

@SuppressWarnings("all")
public class KafkaMessageEnvelope extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse("{\"type\":\"record\",\"name\":\"KafkaMessageEnvelope\",\"namespace\":\"com.linkedin.venice.kafka.protocol\",\"fields\":[{\"name\":\"messageType\",\"type\":\"int\",\"doc\":\"Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The mapping is the following: 0 => Put, 1 => Delete, 2 => ControlMessage.\"},{\"name\":\"producerMetadata\",\"type\":{\"type\":\"record\",\"name\":\"ProducerMetadata\",\"fields\":[{\"name\":\"producerGUID\",\"type\":{\"type\":\"fixed\",\"name\":\"GUID\",\"size\":16},\"doc\":\"A unique identifier for this producer.\"},{\"name\":\"segmentNumber\",\"type\":\"int\",\"doc\":\"A number used to disambiguate between sequential segments sent into a given partition by a given producer. An incremented SegmentNumber should only be sent following an EndOfSegment control message. For finite streams (such as those bulk-loaded from Hadoop), it can be acceptable to have a single SegmentNumber per producer/partition combination, though that is not something that the downstream consumer should assume. For infinite streams, segments should be terminated and begun anew periodically. This number begins at 0.\"},{\"name\":\"messageSequenceNumber\",\"type\":\"int\",\"doc\":\"A monotonically increasing number with no gaps used to distinguish unique messages produced in this segment (i.e.: by this producer into a given partition). This number begins at 0 (with a StartOfSegment ControlMessage) and subsequent messages (such as Put) will have a SequenceNumber of 1 and so forth.\"},{\"name\":\"messageTimestamp\",\"type\":\"long\",\"doc\":\"The time of the producer's local system clock, at the time the message was submitted for production. This is the number of milliseconds from the unix epoch, 1 January 1970 00:00:00.000 UTC.\"},{\"name\":\"upstreamOffset\",\"type\":\"long\",\"doc\":\"this field is deprecated and superseded by the field inside the LeaderMetadata. TODO: Remove this field in the future.\",\"default\":-1},{\"name\":\"logicalTimestamp\",\"type\":\"long\",\"doc\":\"This timestamp may be specified by the user. Sentinel value of -1 => apps are not using latest lib, -2 => apps have not specified the time. In case of negative values messageTimestamp field will be used for replication metadata.\",\"default\":-1}]},\"doc\":\"ProducerMetadata contains information that the consumer can use to identify an upstream producer. This is common for all MessageType.\"},{\"name\":\"targetVersion\",\"type\":\"int\",\"doc\":\"This field is used in incremental pushes only - the current version of the target store when the inc push starts.\",\"default\":-1},{\"name\":\"payloadUnion\",\"type\":[{\"type\":\"record\",\"name\":\"Put\",\"fields\":[{\"name\":\"putValue\",\"type\":\"bytes\",\"doc\":\"The record's value to be persisted in the storage engine.\"},{\"name\":\"schemaId\",\"type\":\"int\",\"doc\":\"An identifier used to determine how the PutValue can be deserialized. Also used, in conjunction with the replicationMetadataVersionId, to deserialize the replicationMetadataPayload.\"},{\"name\":\"replicationMetadataVersionId\",\"type\":\"int\",\"doc\":\"The A/A replication metadata schema version ID that will be used to deserialize replicationMetadataPayload.\",\"default\":-1},{\"name\":\"replicationMetadataPayload\",\"type\":\"bytes\",\"doc\":\"The serialized value of the replication metadata schema.\",\"default\":\"\"}]},{\"type\":\"record\",\"name\":\"Update\",\"fields\":[{\"name\":\"schemaId\",\"type\":\"int\",\"doc\":\"The original schema ID.\"},{\"name\":\"updateSchemaId\",\"type\":\"int\",\"doc\":\"The derived schema ID that will be used to deserialize updateValue.\"},{\"name\":\"updateValue\",\"type\":\"bytes\",\"doc\":\"New value(s) for parts of the record that need to be updated.\"}]},{\"type\":\"record\",\"name\":\"Delete\",\"fields\":[{\"name\":\"schemaId\",\"type\":\"int\",\"doc\":\"An identifier used, in conjunction with the replicationMetadataVersionId, to deserialize the replicationMetadataPayload.\",\"default\":-1},{\"name\":\"replicationMetadataVersionId\",\"type\":\"int\",\"doc\":\"The A/A replication metadata schema version ID that will be used to deserialize replicationMetadataPayload.\",\"default\":-1},{\"name\":\"replicationMetadataPayload\",\"type\":\"bytes\",\"doc\":\"The serialized value of the replication metadata schema.\",\"default\":\"\"}]},{\"type\":\"record\",\"name\":\"ControlMessage\",\"fields\":[{\"name\":\"controlMessageType\",\"type\":\"int\",\"doc\":\"Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The mapping is the following: 0 => StartOfPush, 1 => EndOfPush, 2 => StartOfSegment, 3 => EndOfSegment, 4 => StartOfBufferReplay, 5=> StartOfIncrementalPush, 6=> EndOfIncrementalPush\"},{\"name\":\"debugInfo\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"doc\":\"This metadata is for logging and traceability purposes. It can be used to propagate information about the producer, the environment it runs in, or the source of data being produced into Venice. There should be no assumptions that any of this data will be used (or even looked at) by the downstream consumer in any particular way.\"},{\"name\":\"controlMessageUnion\",\"type\":[{\"type\":\"record\",\"name\":\"StartOfPush\",\"fields\":[{\"name\":\"sorted\",\"type\":\"boolean\",\"doc\":\"Whether the messages inside current topic partition between 'StartOfPush' control message and 'EndOfPush' control message is lexicographically sorted by key bytes\",\"default\":false},{\"name\":\"chunked\",\"type\":\"boolean\",\"doc\":\"Whether the messages inside the current push are encoded with chunking support. If true, this means keys will be prefixed with ChunkId, and values may contain a ChunkedValueManifest (if schema is defined as -20).\",\"default\":false},{\"name\":\"compressionStrategy\",\"type\":\"int\",\"doc\":\"What type of compression strategy the current push uses. Using int because Avro Enums are not evolvable. The mapping is the following: 0 => NO_OP, 1 => GZIP, 2 => ZSTD, 3 => ZSTD_WITH_DICT\",\"default\":0},{\"name\":\"compressionDictionary\",\"type\":[\"null\",\"bytes\"],\"doc\":\"The raw bytes of dictionary used to compress/decompress records.\",\"default\":null},{\"name\":\"timestampPolicy\",\"type\":\"int\",\"doc\":\"The policy to determine timestamps of batch push records. 0 => no per record timestamp metadata is stored, hybrid writes always win over batch, 1 => no per record timestamp metadata is stored, Start-Of-Push Control message's logicalTimestamp is treated as last update timestamp for all batch record, and hybrid writes wins only when their own logicalTimestamp are higher, 2 => per record timestamp metadata is provided by the push job and stored for each key, enabling full conflict resolution granularity on a per field basis, just like when merging concurrent update operations.\",\"default\":0}]},{\"type\":\"record\",\"name\":\"EndOfPush\",\"fields\":[]},{\"type\":\"record\",\"name\":\"StartOfSegment\",\"fields\":[{\"name\":\"checksumType\",\"type\":\"int\",\"doc\":\"Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The downstream consumer is expected to compute this checksum and use it to validate the incoming stream of data. The current mapping is the following: 0 => None, 1 => MD5, 2 => Adler32, 3 => CRC32.\"},{\"name\":\"upcomingAggregates\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"doc\":\"An array of names of aggregate computation strategies for which there will be a value percolated in the corresponding EndOfSegment ControlMessage. The downstream consumer may choose to compute these aggregates on its own and use them as additional validation safeguards, or it may choose to merely log them, or even ignore them altogether.\"}]},{\"type\":\"record\",\"name\":\"EndOfSegment\",\"fields\":[{\"name\":\"checksumValue\",\"type\":\"bytes\",\"doc\":\"The value of the checksum computed since the last StartOfSegment ControlMessage.\"},{\"name\":\"computedAggregates\",\"type\":{\"type\":\"array\",\"items\":\"long\"},\"doc\":\"A map containing the results of the aggregate computation strategies that were promised in the previous StartOfSegment ControlMessage. The downstream consumer may choose to compare the value of these aggregates against those that it computed on its own ir oder to use them as additional validation safeguards, or it may choose to merely log them, or even ignore them altogether.\"},{\"name\":\"finalSegment\",\"type\":\"boolean\",\"doc\":\"This field is set to true when the producer knows that there is no more data coming from its data source after this EndOfSegment. This happens at the time the producer is closed.\"}]},{\"type\":\"record\",\"name\":\"StartOfBufferReplay\",\"fields\":[{\"name\":\"sourceOffsets\",\"type\":{\"type\":\"array\",\"items\":\"long\"},\"doc\":\"Array of offsets from the real-time buffer topic at which the Buffer Replay Service started replaying data. The index position of the array corresponds to the partition number in the real-time buffer.\"},{\"name\":\"sourceKafkaCluster\",\"type\":\"string\",\"doc\":\"Kafka bootstrap servers URL of the cluster where the source buffer exists.\"},{\"name\":\"sourceTopicName\",\"type\":\"string\",\"doc\":\"Name of the source buffer topic.\"}]},{\"type\":\"record\",\"name\":\"StartOfIncrementalPush\",\"fields\":[{\"name\":\"version\",\"type\":\"string\",\"doc\":\"The version of current incremental push. Each incremental push is associated with a version. Both 'StartOfIncrementalPush' control message and 'EndOfIncrementalPush' contain version info so they can be paired to each other.\"}]},{\"type\":\"record\",\"name\":\"EndOfIncrementalPush\",\"fields\":[{\"name\":\"version\",\"type\":\"string\",\"doc\":\"The version of current incremental push. Each incremental push is associated with a version. Both 'StartOfIncrementalPush' control message and 'EndOfIncrementalPush' contain version info so they can be paired to each other.\"}]},{\"type\":\"record\",\"name\":\"TopicSwitch\",\"fields\":[{\"name\":\"sourceKafkaServers\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"doc\":\"A list of Kafka bootstrap servers URLs where the new source topic exists; currently there will be only one URL in the list, but the list opens up the possibility for leader to consume from different fabrics in active-active replication mode.\"},{\"name\":\"sourceTopicName\",\"type\":\"string\",\"doc\":\"Name of new the source topic.\"},{\"name\":\"rewindStartTimestamp\",\"type\":\"long\",\"doc\":\"The creation time of this control message in parent controller minus the rewind time of the corresponding store; leaders in different fabrics will get the offset of the source topic by the same start timestamp and start consuming from there; if timestamp is 0, leader will start consuming from the beginning of the source topic.\"}]}],\"doc\":\"This contains the ControlMessage data which is specific to each type of ControlMessage. Which branch of the union is present is based on the previously-defined MessageType field.\"}]}],\"doc\":\"This contains the main payload of the message. Which branch of the union is present is based on the previously-defined MessageType field.\"},{\"name\":\"leaderMetadataFooter\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"LeaderMetadata\",\"fields\":[{\"name\":\"hostName\",\"type\":\"string\",\"doc\":\"The identifier of the host which sends the message.This helps detect the 'split brain' scenario in leader SN. Notice that it is different from GUID. GUID represents the one who produces the message. In 'pass-through' mode, the relaying producer will reuse the same GUID from the upstream message.\"},{\"name\":\"upstreamOffset\",\"type\":\"long\",\"doc\":\"Where this message is located in RT/GF/remote VT topic. This value will be determined and modified by leader SN at runtime.\",\"default\":-1},{\"name\":\"upstreamKafkaClusterId\",\"type\":\"int\",\"doc\":\"Kafka bootstrap server URL of the cluster where RT/GF/remote VT topic exists, represented by an integer to reduce the overhead. This value will be determined and modified by leader SN at runtime.\",\"default\":-1}]}],\"doc\":\"A optional footer that leader SN can use to give extra L/F related mete data\",\"default\":null}]}");
  /** Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The mapping is the following: 0 => Put, 1 => Delete, 2 => ControlMessage. */
  public int messageType;
  /** ProducerMetadata contains information that the consumer can use to identify an upstream producer. This is common for all MessageType. */
  public com.linkedin.venice.kafka.protocol.ProducerMetadata producerMetadata;
  /** This field is used in incremental pushes only - the current version of the target store when the inc push starts. */
  public int targetVersion;
  /** This contains the main payload of the message. Which branch of the union is present is based on the previously-defined MessageType field. */
  public java.lang.Object payloadUnion;
  /** A optional footer that leader SN can use to give extra L/F related mete data */
  public com.linkedin.venice.kafka.protocol.LeaderMetadata leaderMetadataFooter;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return messageType;
    case 1: return producerMetadata;
    case 2: return targetVersion;
    case 3: return payloadUnion;
    case 4: return leaderMetadataFooter;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
  // Used by DatumReader.  Applications should not call. 
  @SuppressWarnings(value="unchecked")
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: messageType = (java.lang.Integer)value$; break;
    case 1: producerMetadata = (com.linkedin.venice.kafka.protocol.ProducerMetadata)value$; break;
    case 2: targetVersion = (java.lang.Integer)value$; break;
    case 3: payloadUnion = (java.lang.Object)value$; break;
    case 4: leaderMetadataFooter = (com.linkedin.venice.kafka.protocol.LeaderMetadata)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
}
