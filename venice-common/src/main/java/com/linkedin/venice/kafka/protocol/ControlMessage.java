/**
 * Autogenerated by Avro
 * 
 * DO NOT EDIT DIRECTLY
 */
package com.linkedin.venice.kafka.protocol;

@SuppressWarnings("all")
/** ControlMessage payloads contain metadata about the stream of data, for validation and debuggability purposes. */
public class ControlMessage extends org.apache.avro.specific.SpecificRecordBase implements org.apache.avro.specific.SpecificRecord {
  public static final org.apache.avro.Schema SCHEMA$ = org.apache.avro.Schema.parse("{\"type\":\"record\",\"name\":\"ControlMessage\",\"namespace\":\"com.linkedin.venice.kafka.protocol\",\"fields\":[{\"name\":\"controlMessageType\",\"type\":\"int\",\"doc\":\"Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The mapping is the following: 0 => StartOfPush, 1 => EndOfPush, 2 => StartOfSegment, 3 => EndOfSegment, 4 => StartOfBufferReplay, 5=> StartOfIncrementalPush, 6=> EndOfIncrementalPush\"},{\"name\":\"debugInfo\",\"type\":{\"type\":\"map\",\"values\":\"string\"},\"doc\":\"This metadata is for logging and traceability purposes. It can be used to propagate information about the producer, the environment it runs in, or the source of data being produced into Venice. There should be no assumptions that any of this data will be used (or even looked at) by the downstream consumer in any particular way.\"},{\"name\":\"controlMessageUnion\",\"type\":[{\"type\":\"record\",\"name\":\"StartOfPush\",\"fields\":[{\"name\":\"sorted\",\"type\":\"boolean\",\"doc\":\"Whether the messages inside current topic partition between 'StartOfPush' control message and 'EndOfPush' control message is lexicographically sorted by key bytes\",\"default\":false},{\"name\":\"chunked\",\"type\":\"boolean\",\"doc\":\"Whether the messages inside the current push are encoded with chunking support. If true, this means keys will be prefixed with ChunkId, and values may contain a ChunkedValueManifest (if schema is defined as -20).\",\"default\":false},{\"name\":\"compressionStrategy\",\"type\":\"int\",\"doc\":\"What type of compression strategy the current push uses. Using int because Avro Enums are not evolvable. The mapping is the following: 0 => NO_OP, 1 => GZIP, 2 => ZSTD, 3 => ZSTD_WITH_DICT\",\"default\":0},{\"name\":\"compressionDictionary\",\"type\":[\"null\",\"bytes\"],\"doc\":\"The raw bytes of dictionary used to compress/decompress records.\",\"default\":null},{\"name\":\"timestampPolicy\",\"type\":\"int\",\"doc\":\"The policy to determine timestamps of batch push records. 0 => no per record replication metadata is stored, hybrid writes always win over batch, 1 => no per record timestamp metadata is stored, Start-Of-Push Control message's logicalTimestamp is treated as last update timestamp for all batch record, and hybrid writes wins only when their own logicalTimestamp are higher, 2 => per record timestamp metadata is provided by the push job and stored for each key, enabling full conflict resolution granularity on a per field basis, just like when merging concurrent update operations.\",\"default\":0}]},{\"type\":\"record\",\"name\":\"EndOfPush\",\"fields\":[]},{\"type\":\"record\",\"name\":\"StartOfSegment\",\"fields\":[{\"name\":\"checksumType\",\"type\":\"int\",\"doc\":\"Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The downstream consumer is expected to compute this checksum and use it to validate the incoming stream of data. The current mapping is the following: 0 => None, 1 => MD5, 2 => Adler32, 3 => CRC32.\"},{\"name\":\"upcomingAggregates\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"doc\":\"An array of names of aggregate computation strategies for which there will be a value percolated in the corresponding EndOfSegment ControlMessage. The downstream consumer may choose to compute these aggregates on its own and use them as additional validation safeguards, or it may choose to merely log them, or even ignore them altogether.\"}]},{\"type\":\"record\",\"name\":\"EndOfSegment\",\"fields\":[{\"name\":\"checksumValue\",\"type\":\"bytes\",\"doc\":\"The value of the checksum computed since the last StartOfSegment ControlMessage.\"},{\"name\":\"computedAggregates\",\"type\":{\"type\":\"array\",\"items\":\"long\"},\"doc\":\"A map containing the results of the aggregate computation strategies that were promised in the previous StartOfSegment ControlMessage. The downstream consumer may choose to compare the value of these aggregates against those that it computed on its own ir oder to use them as additional validation safeguards, or it may choose to merely log them, or even ignore them altogether.\"},{\"name\":\"finalSegment\",\"type\":\"boolean\",\"doc\":\"This field is set to true when the producer knows that there is no more data coming from its data source after this EndOfSegment. This happens at the time the producer is closed.\"}]},{\"type\":\"record\",\"name\":\"StartOfBufferReplay\",\"fields\":[{\"name\":\"sourceOffsets\",\"type\":{\"type\":\"array\",\"items\":\"long\"},\"doc\":\"Array of offsets from the real-time buffer topic at which the Buffer Replay Service started replaying data. The index position of the array corresponds to the partition number in the real-time buffer.\"},{\"name\":\"sourceKafkaCluster\",\"type\":\"string\",\"doc\":\"Kafka bootstrap servers URL of the cluster where the source buffer exists.\"},{\"name\":\"sourceTopicName\",\"type\":\"string\",\"doc\":\"Name of the source buffer topic.\"}]},{\"type\":\"record\",\"name\":\"StartOfIncrementalPush\",\"fields\":[{\"name\":\"version\",\"type\":\"string\",\"doc\":\"The version of current incremental push. Each incremental push is associated with a version. Both 'StartOfIncrementalPush' control message and 'EndOfIncrementalPush' contain version info so they can be paired to each other.\"}]},{\"type\":\"record\",\"name\":\"EndOfIncrementalPush\",\"fields\":[{\"name\":\"version\",\"type\":\"string\",\"doc\":\"The version of current incremental push. Each incremental push is associated with a version. Both 'StartOfIncrementalPush' control message and 'EndOfIncrementalPush' contain version info so they can be paired to each other.\"}]},{\"type\":\"record\",\"name\":\"TopicSwitch\",\"fields\":[{\"name\":\"sourceKafkaServers\",\"type\":{\"type\":\"array\",\"items\":\"string\"},\"doc\":\"A list of Kafka bootstrap servers URLs where the new source topic exists; currently there will be only one URL in the list, but the list opens up the possibility for leader to consume from different fabrics in active-active replication mode.\"},{\"name\":\"sourceTopicName\",\"type\":\"string\",\"doc\":\"Name of new the source topic.\"},{\"name\":\"rewindStartTimestamp\",\"type\":\"long\",\"doc\":\"The creation time of this control message in parent controller minus the rewind time of the corresponding store; leaders in different fabrics will get the offset of the source topic by the same start timestamp and start consuming from there; if timestamp is 0, leader will start consuming from the beginning of the source topic.\"}]}],\"doc\":\"This contains the ControlMessage data which is specific to each type of ControlMessage. Which branch of the union is present is based on the previously-defined MessageType field.\"}]}");
  /** Using int because Avro Enums are not evolvable. Readers should always handle the 'unknown' value edge case, to account for future evolutions of this protocol. The mapping is the following: 0 => StartOfPush, 1 => EndOfPush, 2 => StartOfSegment, 3 => EndOfSegment, 4 => StartOfBufferReplay, 5=> StartOfIncrementalPush, 6=> EndOfIncrementalPush */
  public int controlMessageType;
  /** This metadata is for logging and traceability purposes. It can be used to propagate information about the producer, the environment it runs in, or the source of data being produced into Venice. There should be no assumptions that any of this data will be used (or even looked at) by the downstream consumer in any particular way. */
  public java.util.Map<java.lang.CharSequence,java.lang.CharSequence> debugInfo;
  /** This contains the ControlMessage data which is specific to each type of ControlMessage. Which branch of the union is present is based on the previously-defined MessageType field. */
  public java.lang.Object controlMessageUnion;
  public org.apache.avro.Schema getSchema() { return SCHEMA$; }
  // Used by DatumWriter.  Applications should not call. 
  public java.lang.Object get(int field$) {
    switch (field$) {
    case 0: return controlMessageType;
    case 1: return debugInfo;
    case 2: return controlMessageUnion;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
  // Used by DatumReader.  Applications should not call. 
  @SuppressWarnings(value="unchecked")
  public void put(int field$, java.lang.Object value$) {
    switch (field$) {
    case 0: controlMessageType = (java.lang.Integer)value$; break;
    case 1: debugInfo = (java.util.Map<java.lang.CharSequence,java.lang.CharSequence>)value$; break;
    case 2: controlMessageUnion = (java.lang.Object)value$; break;
    default: throw new org.apache.avro.AvroRuntimeException("Bad index");
    }
  }
}
