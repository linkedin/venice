How to run Hadoop To Venice Bridge Locally

1) Download the Hadoop binary version hadoop-1.2.1 ( I tried various versions of Hadoop 2, they either failed with GenericRecord class missing or Avro MapRed incompatible APIs). https://www.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz
2) Download the data required for running the hadoop locally. I downloaded the Avro data from internal Holdem cluster
You can use dali to download the data by running the command
dali fs -copyToLocal dali://holdem/user/voldtest/ha_store_data_v1

3) Unpack the hadoop binary ( tar -zxf hadoop-1.2.1.tar.gz)
4) Run the Hadoop jar locally by the command

bin/hadoop jar ~/projects/aruns-venice/hadoop-to-venice-bridge/build/libs/hadoop-to-venice-bridge-0.1.jar  com.linkedin.venice.hadoop.KafkaPushJob --venice-url http://localhost:54333 --cluster-name test-cluster --store-name myCountry --input-path file:///home/athirupa/projects/hadoop/hadoop-1.2.1/ha_store_data_v1/ --key-field memberId --value-field dedupId --store-owners <your_linkedin_email>

Note:  Either use the venice-admin-tool to create your store first, or add the --create-store flag to have H2V auto create your store when it runs the first time.

You can query the router with curl.  (-i means show headers, this assumes router running on localhost with port 54333)

curl -i http://localhost:54333/storage/myCountry/6J_eTg==?f=b64
